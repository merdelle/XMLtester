<meetings>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Intermission</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  9:15AM</starts_at>
    <ends_at>Feb 14 2020  9:25AM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>NA</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68181-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T09:15:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Intermission</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  9:15AM</starts_at>
    <ends_at>Feb 15 2020  9:25AM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>NA</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68187-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T09:15:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>IP1 Parallel Tomographic Reconstruction - Where Combinatorics Meets Geometry</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP1</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  5:15PM</starts_at>
    <ends_at>Feb 12 2020  6:00PM</ends_at>
    <EventFilter>PP20|Invited Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67801</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>67801-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T17:15:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Lunch Break</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 12:35PM</starts_at>
    <ends_at>Feb 14 2020  2:05PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Attendees on their own</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68185-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T12:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Lunch Break</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 12:20PM</starts_at>
    <ends_at>Feb 15 2020  1:50PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Attendees on their own</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68189-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T12:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS1 Industrial Mathematical Software</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS1</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  2:40PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67824</EventHandoutURL>
    <blurb>Mathematical software, which is essential to all areas of science, engineering, and machine learning, is continually being developed and improved to attack new applications and to keep up with breakthroughs in hardware technologies.  In this minisymposium we highlight recent advances in industrial software, covering both CPUs and GPUs, as well as low-level optimizations through high-level software packages.</blurb>
    <EventParentName></EventParentName>
    <external_id>67824-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS10 Advances in Algorithms Exploiting Low Precision Floating-Point Arithmetic - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS10</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  4:50PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67712</EventHandoutURL>
    <blurb>In the June 2019 Top 500 list, 133 systems are
accelerator-based, more than half of which support half precision
floating-point arithmetic.  Since low precision floating-point formats are
increasingly being supported by hardware vendors, developing algorithms and
software that can exploit these formats is of increasing importance. This
minisymposium will give a broad overview of recent contributions in
exploiting low precision arithmetic in scientific computing, and details of
applications in which it can successfully be used.</blurb>
    <EventParentName></EventParentName>
    <external_id>67712-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS27 Challenges in Parallel Adaptive Mesh Refinement - Part I of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS27</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67770</EventHandoutURL>
    <blurb>Parallel adaptive mesh refinement (AMR) is a key technique when simulations are required to capture time-dependent and/or multiscale features. Frequent re-adaptation and repartitioning of the mesh during the simulation can impose significant overhead, particularly in largescale parallel environments. Further challenges arise due to the availability of accelerated or special-purpose hardware, and the trend toward hierarchical and hybrid compute architectures. Our minisymposium addresses algorithms, scalability, and software issues of parallel AMR.</blurb>
    <EventParentName></EventParentName>
    <external_id>67770-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Scalable FFT-Krylov Subspace Method for Scattering Problem</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP11</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:45PM</starts_at>
    <ends_at>Feb 14 2020  4:05PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yun Teck Lee</EventSpeakers>
    <EventSpeakerUniqueID>781837</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102026</EventHandoutURL>
    <blurb>This talk is to present an efficient parallel implementation of the direct compact numerical solver for 3D Helmholtz equation for forward scattering problem on multicore computers. Forward scattering problems have many applications such as predicting the scattered electromagnetic field given the geometry and find many applications in computer-aided design. Hence a fast and efficient way to solve the forward scattering problem will impact areas, like high-speed circuits, integrated optics, antenna analysis, remote sensing, geophysical sensing, and inverse scattering.  

Our parallel algorithm will be used to computationally simulate data for the solution of the inverse problem of imaging mine-like targets. In our setting, land mines are modeled as small abnormalities embedded in an otherwise uniform media with an air-ground interface. The main challenge in this setting is the requirement of solving the Helmholtz equation for high frequencies which are time-consuming using standard direct solution techniques. In our approach, this system is solved by a combination of Krylov subspace-type method with a direct parallel FFT-type preconditioner. The resulting numerical method allows a natural and efficient implementation on parallel computers. Numerical results for realistic ranges of parameters in soil and mine-like targets confirm the high efficiency of the proposed parallel iterative algorithm.
</blurb>
    <EventParentName>CP11 Applications - Part III of III</EventParentName>
    <external_id>68198-102026</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Communication Performance Modeling of LAP3D and its Application in Performance Optimization</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP12</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 11:00AM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hong Guo</EventSpeakers>
    <EventSpeakerUniqueID>747384</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102019</EventHandoutURL>
    <blurb>LAP3D(Laser And Plasma 3 Dimension code) is a three-dimensional laser and plasma code for explorering the influence of various instabilities in LPI processes. In LAP3D, several physical processes are coupled together. Fine numerical simulation requires the grid to have a certain size, which leads to a large amount of computation and long calculation time. These become challenges that LAP3D faces. To address such challenges, the following work is carried out. Firstly, LAP3D is analyzed to get the key characteristics which determine the performance of the code. Secondly, a communication performance model is set up based on the key characteristics. Lastly, the model is applied to predict the LAP3D communication costs, to look for the shortest communication cost and to predict the parallel efficiency. In this talk, the communication performance model is tested. From the test results, the communication performance model effectively predicts the optimal communication time and the parallel efficiency of the code, supports it run efficiently on tens of thousands of cores and provides theoretical guidance for the code optimization.

	</blurb>
    <EventParentName>CP12 Communication Performance</EventParentName>
    <external_id>68199-102019</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Toward a Predictive Model to Monitor the Balance Between Discretization and Rounding Errors in Hydrodynamic Simulations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP1</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  1:20PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>William Weens</EventSpeakers>
    <EventSpeakerUniqueID>790097</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102006</EventHandoutURL>
    <blurb>In order to take advantage of HPC and exaflopic machines, a natural tendency is to scale up the simulations by increasing mesh sizes, and to rely blindly on the double floating-point precision. With more points in the mesh, the discretization error decreases, and simulations should deliver more precise results.

However, increasing too much the number of points has negative consequences on the result quality. Numerically, the time step is constrained by the number of points: with more points in the mesh, simulations require more iterations, hence more operations. With more operations, the rounding errors accumulate and perturb the computation, preventing the simulation from converging to its solution. This competition between discretization and rounding errors shows that there is an optimal mesh size, where the cumulated (and unavoidable) rounding errors due to floating-point arithmetic are still less significant (in the mantissa) than the discretization error.

The present study investigates hydrodynamic computation in 1, 2 and 3D and aims at building a predictive model that can determine this optimal size of the mesh. This predictive model is based on analysis tools on floating-point operations that must be inserted in simulation codes. We shall discuss how its results still depend on cases and schemes. These tools, the simulation and the results that helped to design the model are detailed in this work.</blurb>
    <EventParentName>CP1 UQ and Stochastic Processes</EventParentName>
    <external_id>68191-102006</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallel Algorithms for Nonlinear Time-Space Fractional PDEs</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP2</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  1:20PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Toheeb Biala</EventSpeakers>
    <EventSpeakerUniqueID>781225</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101494</EventHandoutURL>
    <blurb>In this talk, we present an algorithm for the numerical integration of nonlinear time-space fractional PDEs. Due to the non-local nature of the fractional operators and memory dependencies of the proposed scheme, we develop parallel algorithms (MPI, OpenMP, and hybrid) for efficiently integrating the class of PDEs

	</blurb>
    <EventParentName>CP2 Efficient Methods for PDEs and IDEs</EventParentName>
    <external_id>68192-101494</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>An Efficient Method for Solving the Fractional Fredholm Integrodifferential Equation</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP2</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  2:15PM</starts_at>
    <ends_at>Feb 12 2020  2:35PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Muhammed Syam</EventSpeakers>
    <EventSpeakerUniqueID>789985</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101622</EventHandoutURL>
    <blurb>Fredholm integrodifferential equation (FFIDE) plays an essential role in several applications in biology, physics, and engineering. More attention was given to FFIDE recently.  In this article, we present a method to approximate the solution of FFIDE based on the ABFD in Caputo sense. We use the fractional operational matrix of the fractional derivative to investigate this problem. We present two examples to show that the proposed method is accurate. In the two examples, we get the exact solution. Form the numerical results; we see that the proposed method give accurate results. Some theoretical results are presented.
	</blurb>
    <EventParentName>CP2 Efficient Methods for PDEs and IDEs</EventParentName>
    <external_id>68192-101622</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Fast Image Reconstruction at a Synchrotron Laboratory</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP4</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:45AM</starts_at>
    <ends_at>Feb 13 2020 12:05PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Eduardo Miqueles</EventSpeakers>
    <EventSpeakerUniqueID>791898</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104194</EventHandoutURL>
    <blurb>In this work we present a fast GPU implementation for tomographic reconstruction of large datasets using data obtained at the brazilian synchrotron light source. The algorithm is distributed in a server with 4 GPUs through a fast pipeline implemented in C/CUDA programming language.  Our algorithm is theoretically based on a recently discovered low complexity formula, computing the total volume within O(N3 logN) floating point operations; much less than traditional algorithms that operates within O(N4) FLOPS over an input data of size O(N3). The results obtained with real data indicate that a reconstruction can be achieved within orders of second, provided the data is transferred completely to the memory.</blurb>
    <EventParentName>CP4 Proceedings Papers - Part I of III</EventParentName>
    <external_id>68613-104194</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Virtual Environment for Sensor Performance Assessment (vespa) Geometry Engine</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP5</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:35PM</starts_at>
    <ends_at>Feb 13 2020  4:55PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Barry White</EventSpeakers>
    <EventSpeakerUniqueID>731334</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101980</EventHandoutURL>
    <blurb>For realistic synthetic imagery, radiative transfer methods coupled with large mesh geometry provide the most scientifically accurate way to model a scene.  Radiative models typically use ray-tracing techniques to determine where radiative energy is coming from or moving to.  This work presents an approach to making a ray query geometry engine that actively stores large scale terabyte geometry in out-of-core memory on parallel general purpose processors.  Procedures for geometry distribution, structures for efficient ray-tracing, and the ray query API are discussed.  Geometry distribution uses Morton codes and parallel sorting routines to create geometry scene-chunks that are distributed among processing nodes.  Each scene chunk is then broken down using a bounding volume hierarchy (BVH) using axis-aligned bounding boxes (AABB).  The BVH allows for efficient ray tracing of the geometry.  The ray query API allows client-side programs, such as sensor models and radiative transfer models, which exist on the same high performance computer to efficiently identify intersected geometry given directed rays.  The geometry has key values that can uniquely identify data from solver programs.  Scalability, partition timing, and ray timing results will be presented.</blurb>
    <EventParentName>CP5 Application - Part II of III</EventParentName>
    <external_id>68194-101980</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Graph Based Algebraic Multigrid Method</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP6</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:45PM</starts_at>
    <ends_at>Feb 13 2020  4:05PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Manan Shah</EventSpeakers>
    <EventSpeakerUniqueID>790103</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101993</EventHandoutURL>
    <blurb>Algebraic Multigrid (AMG) method is widely used to solve large sparse linear systems which arise in various computational simulations. They are also used as preconditioners with Krylov subspace solvers like conjugate gradient method. With evolving multicore architectures, we face new challenges to achieve linear complexity for AMG. In this work, we analyze computational complexity of aggregation-based AMG on multicore architectures. We demonstrate that linear complexity of AMG can be achieved with graph based approaches. Our Primitive Graph based approach helps to achieve good strong scaling on multicore architectures till it achieves the bandwidth bound of multicore architecture. We also evaluate complexity of the coarse grid construction phase of AMG with graph based approaches for larger system sizes.

	</blurb>
    <EventParentName>CP6 Multigrid and Preconditioning</EventParentName>
    <external_id>68195-101993</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>SMILU: a Staggered-Grid, Multi-Level ILU Preconditioner for Steady Fluid-Transport Problems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP8</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:20AM</starts_at>
    <ends_at>Feb 14 2020 11:40AM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jonas Thies</EventSpeakers>
    <EventSpeakerUniqueID>767488</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102021</EventHandoutURL>
    <blurb>When computing steady state solutions of the incompressible Navier-Stokes equations of computational fluid dynamics,
a common approach is to use "pseudo time stepping", which allows to control the conditioning of the arising linear 
systems, or even resort to explicit schemes, by using a small time step.

An arguably more effective approach is to directly solve for steady states and step through parameter space instead 
using a homotopy or numerical continuation technique. This, however, leads to non-symmetric linear systems of saddle 
point type, for which it is difficult to find an appropriate preconditioner, especially if the Reynolds Number becomes
large.

In this talk we exploit structure-preserving properties of a two-level ILU method geared towards a particular 
discretization method (the Arakawa-C grid finite volume method) [Wubs \&amp; Thies, SIMAX (32), 2011].
We extend the scheme to a parallel fully coupled multi-level incomplete factorization for the 3D stationary 
incompressible Navier-Stokes equations by observing that the structure-preserving properties can be fully retained 
in a recursion. The main idea is to define an appropriate geometric domain decomposition for the staggered grid, 
leading to a robust method with computational complexity of O(N log N).

We show results for the 3D lid-driven cavity benchmark, and investigate the performance of another recent 
development, the FROSch preconditioner available in Trilinos.



</blurb>
    <EventParentName>CP8 Numerical Methods for Flow Simulations</EventParentName>
    <external_id>68196-102021</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The Multiscale Perturbation Method for Elliptic Equations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP8</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 12:10PM</starts_at>
    <ends_at>Feb 14 2020 12:30PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Het Mankad</EventSpeakers>
    <EventSpeakerUniqueID>786088</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102031</EventHandoutURL>
    <blurb>In the formulation of multiscale methods for second order elliptic equations that are based on domain decomposition procedures, typically the computational domain is decomposed into subdomains, and for each subdomain a set of multiscale basis functions is numerically constructed. Consider the application of such a method to solve a multiphase flow problem where through an operator splitting algorithm, the velocity-pressure and transport equations are solved sequentially. From one time step to the next the multiscale basis functions should be recomputed, because of the coupling of the underlying PDEs. Instead of recomputing all multiscale basis function every time step of a numerical solution, we propose the Multiscale Perturbation Method (MPM). In MPM an approximate solution of velocity and pressure  for a new time is obtained by combining regular perturbation theory with multiscale basis functions computed in an earlier time. The solution obtained at the perturbation step is improved further by a local update step to give the final solution of the MPM.  An efficient parallel algorithm is implemented in multi-core machines and the method also fits well in GPU clusters. Numerical experiments, where the perturbation theory results are compared with direct fine grid solutions, are presented and discussed.

	</blurb>
    <EventParentName>CP8 Numerical Methods for Flow Simulations</EventParentName>
    <external_id>68196-102031</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>HMG: A Configurable High-Performance Multilevel Preconditioner</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP9</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:20AM</starts_at>
    <ends_at>Feb 14 2020 11:40AM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Fande Kong</EventSpeakers>
    <EventSpeakerUniqueID>788367</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102065</EventHandoutURL>
    <blurb>MOOSE is a general finite element framework for multiphysics simulations. Applications built upon MOOSE cover a broad range of engineering problems from many disciplines including nuclear energy, environmental science, and mining industries. Different sets of partial differential equations arise in each of these fields and there is no single preconditioning algorithm that works effectively for all these problems. In this work, we attempt to create a configurable preconditioner system where all aspects of the underlying algorithms can be configured and tuned for specific target applications. The numerical effectiveness of this system on modern supercomputers with tens of thousands of processor cores is demonstrated for multiple problems including neutron transport equations, Navier-Stokes equations, solid mechanics problems, and compressible flows.
	
</blurb>
    <EventParentName>CP9 Autotuning</EventParentName>
    <external_id>68197-102065</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Scalable Resilience Against Node Failures for Communication-Hiding Preconditioned Conjugate Gradient and Conjugate Residual Methods</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP10</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 11:15AM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Markus Levonyak</EventSpeakers>
    <EventSpeakerUniqueID>786105</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104195</EventHandoutURL>
    <blurb>The observed and expected continued growth in the number of nodes in large-scale parallel computers gives rise to two major challenges: global communication operations are becoming major bottlenecks due to their limited scalability, and the likelihood of node failures is increasing. We study an approach for addressing these challenges in the context of solving large sparse linear systems. In particular, we focus on the pipelined preconditioned conjugate gradient (PPCG) method, which has been shown to successfully deal with the first of these challenges. In this paper, we address the second challenge. We present extensions to the PPCG solver and two of its variants which make them resilient against the failure of a compute node while fully preserving their communication-hiding properties and thus their scalability. The basic idea is to efficiently communicate a few redundant copies of local vector elements to neighboring nodes with very little overhead. In case a node fails, these redundant copies are gathered at a replacement node, which can then accurately reconstruct the lost parts of the solver's state. After that, the parallel solver can continue as in the failure-free scenario. Experimental evaluations of our approach illustrate on average very low runtime overheads compared to the standard non-resilient algorithms. This shows that scalable algorithmic resilience can be achieved at low extra cost.</blurb>
    <EventParentName>CP10 Proceedings Papers - Part III of III</EventParentName>
    <external_id>68615-104195</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Multi GPU Algorithms for Hierarchical Matrix Computations </name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP13</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  3:05PM</starts_at>
    <ends_at>Feb 15 2020  3:25PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Wajih Halim Boukaram</EventSpeakers>
    <EventSpeakerUniqueID>768198</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102074</EventHandoutURL>
    <blurb>We present scalable distributed memory algorithms for algebraic compression and matrix-vector products involving hierarchical matrices. Motivated by the limited memory of GPUs, our representation of hierarchical matrices uses nested bases for asymptotically optimal memory storage. The representation consists of two trees for row and column bases and a matrix tree for the low rank matrix blocks. The trees are flattened and distributed at every level on the GPUs. This allows upsweep and downsweep operations on the basis trees, which are the fundamental blocks of hierarchical matrix algorithms, to use efficient and parallel marshaling of linear algebra operations (GEMV, GEMM, QR, and RRQR) for batched execution. Parallelism can then achieved both within individual GPUs as well as among GPUs. Our algorithms structure the computations so that the data communicated between GPUs is limited to the data produced by the bases trees, reducing the volume of data movement necessary. The implementation uses NCCL and MPI for inter and intranode communications, and we present results on a V100 cluster.</blurb>
    <EventParentName>CP13 Accelerating Software with GPUs</EventParentName>
    <external_id>68200-102074</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>HashGraph - Scalable Hash Tables using A Sparse Graph Data Structure</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP14</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:15PM</starts_at>
    <ends_at>Feb 15 2020  2:35PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Oded Green</EventSpeakers>
    <EventSpeakerUniqueID>780304</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101729</EventHandoutURL>
    <blurb>Hash tables are ubiquitous and used in a wide range of applications for efficient probing of large and unsorted data. If designed properly, hash-tables can enable efficient look ups in a constant number of operations. As data sizes continue to grow and data becomes less structured, the need for efficient and scalable hash table also grows. 
In this paper we introduce HashGraph, a new scalable approach for building hash tables that uses concepts taken from sparse graph representations---hence the name HashGraph. We show two different variants of HashGraph, a simple algorithm that outlines the method to create the hash-table and an advanced method that creates the hash table in a more efficient manner. HashGraph shows a new way to deal with hash-collisions that does not use ``open-addressing' or ``chaining', yet has all the benefits of both these approaches. We show that HashGraph can deal with a large number of hash-values per entry without loss of performance as most open-addressing and chaining approaches have. Further, we show that HashGraph is indifferent to the load-factor. Given the above, HashGraph is extremely fast and outperforms several state of the art hash-table implementations.  While HashGraph is not architecture dependent, using a NVIDIA GV100 GPU, HashGraph is anywhere from 2X-8X faster than cuDPP, WarpDrive, and cuDF. HashGraph is able to build a hash-table at a rate of 2.5 billion keys per second and can probe at nearly the same rate.</blurb>
    <EventParentName>CP14 HPC for Data Science and Large Graphs</EventParentName>
    <external_id>68201-101729</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Introduction to High Performance Computing for Decision Makers: Creating a One-Day Workshop for a Data Science Audience</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP14</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  2:10PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Cindy Orozco Bohorquez</EventSpeakers>
    <EventSpeakerUniqueID>790089</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102017</EventHandoutURL>
    <blurb>The excitement around data science has percolated multiple places in our society. Therefore, it is not surprising that a data-science workshop called \textit{Introduction to High Performance Computing} in Silicon Valley receives registrations from entry level engineers, department directors, entrepreneurs, undergraduate students, researchers and faculty. However, the diversity of backgrounds implies different student motivations and, together with the time constraint of a one-day workshop, it challenges the traditional approach to learn HPC. Students are not interested anymore in learning how to code prototypical algorithms using a particular library, they want to understand what HPC means to them and what it has to offer to their work.

Two years ago, we created a summer workshop with the aim of giving participants the tools to analyze HPC applications without previous experience in coding. We took a top to bottom approach that puts hardware scalability as the main constraint, and OpenMP, MPI and Spark as tools to attenuate synchronization and memory demands. Instead of focusing on the correct syntax or usage, we expose the abstraction behind the tools using everyday analogies. With this approach we provide a common framework to understand HPC libraries that facilitates understanding of hardware challenges. As a result, we close the gap between decision makers and developers, empowering them to question the best use of the available resources for their application. 
</blurb>
    <EventParentName>CP14 HPC for Data Science and Large Graphs</EventParentName>
    <external_id>68201-102017</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Strategies for Exploiting Multicore in High-Level Software Packages</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS1</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:25PM</starts_at>
    <ends_at>Feb 12 2020  1:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Pat  Quillen </EventSpeakers>
    <EventSpeakerUniqueID>751607</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101848</EventHandoutURL>
    <blurb>High-level software packages such as MATLAB\textregistered\ are typically designed to favor productivity over performance.  However, customers still have the expectation that software will optimally exploit available hardware resources to extract the best possible performance during computation.  This requirement can often prove challenging, as to serve a variety of customers, the software cannot in general be designed to target any one specific set of hardware.  In this talk, we share our experience developing mathematical libraries used by MATLAB to deliver scalability and performance on a variety of commodity multicore processors.  In particular, we focus on our strategy for implementing core algorithms for convolutional neural networks supporting applications in deep learning.





</blurb>
    <EventParentName>MS1 Industrial Mathematical Software</EventParentName>
    <external_id>67824-101848</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Implementation of Fast Eigensolvers on GPUs</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS3</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  1:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Chao Yang</EventSpeakers>
    <EventSpeakerUniqueID>701749</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101743</EventHandoutURL>
    <blurb>We discuss the potential benefits and challenges of developing fast eigensolvers on mulit-GPU systems. We will first examine the implementation of spectrum slicing algorithms for solving symmetric eigenvalue problems in which each spectral slice is computed on a single or a group of GPUs using either a shift-invert subspace iteration or a polynomial filtering subspace iteration.  We leverage the existing CUDA libraries for performing the matrix decomposition and matrix-matrix multiplications required in the shift-invert and polynomial filtering procedures. We discuss strategies for minimizing inter GPU communication. We then discuss challenges in implmenting an iterative many-body eigensolver in a legacy nuclear physics application code, and present some preliminary results.</blurb>
    <EventParentName>MS3 Experiences in Developing GPU Support for Department of Energy Math Libraries - Part I of II</EventParentName>
    <external_id>67790-101743</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>High Order Scalable FFT-Krylov Subspace Methods for the 3D Convection Diffusion Equation</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP2</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:50PM</starts_at>
    <ends_at>Feb 12 2020  2:10PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ron Gonzales</EventSpeakers>
    <EventSpeakerUniqueID>790102</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101981</EventHandoutURL>
    <blurb>In this presentation, we develop an efficient parallel iterative approach to the solution of the three-dimensional convection-diffusion equation with variable coefficients based on high resolution compact finite-difference approximation schemes. In our approach the resulting system is solved by a combination of a Krylov subspace-type method with a matching high order Fast Fourier Transform (FFT) preconditioner with coefficients depending only on one spatial variable. This preconditioner also includes the convection terms only in the spatial direction of the dependent coefficients. In the algorithms considered, the exact solution of the high order preconditioning system is based on a combination of the separation of variables technique and FFT-type methods developed in our previous publication. The resulting numerical methods allow efficient implementation on parallel computers. The results of implementation of these methods in OpenMP and MPI programming environments are presented. Numerical results on synthetic data confirm the high efficiency of the iterative algorithms.</blurb>
    <EventParentName>CP2 Efficient Methods for PDEs and IDEs</EventParentName>
    <external_id>68192-101981</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>High-Order Spatio-Temporally Parallel ``Fast-Hybrid' Wave Equation Solver at $\mathcal{O}(1)$ Sampling Cost</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP2</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:25PM</starts_at>
    <ends_at>Feb 12 2020  1:45PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Thomas Anderson</EventSpeakers>
    <EventSpeakerUniqueID>785181</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102064</EventHandoutURL>
    <blurb>We propose and demonstrate a frequency/time hybrid           
integral-equation method for the time dependent wave equation in two and three-dimensional spatial domains. Relying on  
Fourier Transformation in time, the method utilizes a fixed (time-independent) number of frequency-domain integral-equation solutions to evaluate, with superalgebraically-small errors, time domain solutions for arbitrarily long times. As a transform method, the required frequency-domain integral equation problems are completely decoupled and can be computed in an entirely parallel manner. The approach relies on two main elements, namely, 1) A smooth time-windowing methodology that enables accurate band-limited representations for arbitrary long time signals, and 2) A novel Fourier transform approach which, without causing spurious periodicity effects, delivers dispersionless spectrally accurate solutions. The algorithm can handle dispersive media, complex physical structures, it enables parallelization in time in a straightforward manner, and it allows for time leaping--that is, solution sampling at any given time T at $\mathcal{O}(1)$-bounded sampling cost, for arbitrarily large values of T, and without requirement of evaluation of the solution at intermediate times. The proposed frequency/time hybridization strategy provides significant advantages over other available alternatives such as volumetric discretization and convolution-quadrature approaches.</blurb>
    <EventParentName>CP2 Efficient Methods for PDEs and IDEs</EventParentName>
    <external_id>68192-102064</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Approaching Exa-Scalable and Accurate Green's Function Coupled Cluster Calculations of DNA Fragments</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP3</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:35PM</starts_at>
    <ends_at>Feb 12 2020  3:55PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Bo Peng</EventSpeakers>
    <EventSpeakerUniqueID>789957</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101558</EventHandoutURL>
    <blurb>Nucleobases are molecular building blocks of DNA. An accurate understanding of the photoelectron properties of these molecules is extremely important for unveiling the mechanism of the formation and damage of DNA chains. However, only limited knowledge in this area has been known. Ab initio calculations in various levels of theory have only been limited to single nucleic acid bases. When the system becomes larger and more quantum-mechanical meaningful, the obtained results will be greatly shifted by significant many-body effects. In this work, by developing Green's function coupled cluster theory and a more efficient tensor algebra library, we are able to compute the more accurate photoelectronic properties of a series structures ranging from single nucleobases up to three base pairs. The algorithm and library have been carefully designed for achieving exa-scaling performance. All the many-body calculations have been performed on Summit supercomputing facility. The obtained results of nucleobases and base pairs have exhibited significant improvements over single-particle results and great capability of obtaining full many-body spectra function. The results are so far the most accurate results for the most quantum-mechanical meaningful DNA fragments, and thus help us to unveil the photoelectronic properties change upon the structural change, calibrate the other cheaper theories, and obtain the exa-scaling experience of many-body calculations.</blurb>
    <EventParentName>CP3 Application - Part I of III</EventParentName>
    <external_id>68193-101558</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Tensor Processing Units for Financial Monte Carlo</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP4</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:20AM</starts_at>
    <ends_at>Feb 13 2020 11:40AM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Francois Belletti</EventSpeakers>
    <EventSpeakerUniqueID>789946</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101530</EventHandoutURL>
    <blurb>Monte Carlo methods are core to many routines in quantitative finance such as derivatives pricing, hedging and risk metrics.
Unfortunately, Monte Carlo methods are very computationally expensive when it comes to running simulations in high-dimensional state spaces where they are still a method of choice in the financial industry.
Recently, Tensor Processing Units (TPUs) have provided considerable speedups and decreased the cost of running Stochastic Gradient Descent (SGD) in Deep Learning.
After having highlighted computational similarities between training neural networks with SGD and stochastic process simulation, we ask in the present paper whether TPUs are accurate, fast and simple enough to use for financial Monte Carlo.
Through a theoretical reminder of the key properties of such methods and thorough empirical experiments we examine the fitness of TPUs for option pricing, hedging and risk metrics computation.
We show in the following that Tensor Processing Units (TPUs) in the cloud help accelerate Monte Carlo routines compared to Graphics Processing Units (GPUs) which in turn decreases the cost associated with running such simulations while leveraging the flexibility of the cloud.
In particular we demonstrate that, in spite of the use of mixed precision, TPUs still provide accurate estimators which are fast to compute. 
We also show that the Tensorflow programming model for TPUs is elegant, expressive and simplifies automated differentiation.</blurb>
    <EventParentName>CP4 Proceedings Papers - Part I of III</EventParentName>
    <external_id>68613-101530</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Landing on Mars: Petascale Unstructured-Grid CFD Simulations on Summit</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP5</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  3:40PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Eric Nielsen</EventSpeakers>
    <EventSpeakerUniqueID>747204</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102060</EventHandoutURL>
    <blurb>A campaign to investigate the use of supersonic retropropulsion as a means to land payloads on Mars large enough to enable human exploration is presented. Simulations are performed on the world’s largest supercomputer, Summit, located at Oak Ridge National Laboratory.  The engineering and computational challenges associated with retropropulsion aerodynamics and the need for large-scale resources like Summit are reviewed. For these simulations, a GPU implementation of NASA Langley Research Center's FUN3D flow solver is used.  The development history, performance, and scalability are compared with those of contemporary HPC architectures. The use of an optimized GPU-accelerated CFD solver on Summit has enabled simulations well beyond conventional computing paradigms.</blurb>
    <EventParentName>CP5 Application - Part II of III</EventParentName>
    <external_id>68194-102060</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Preconditioning Finite Element Equations via Nonconforming Reformulations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP6</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:35PM</starts_at>
    <ends_at>Feb 13 2020  4:55PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Delyan Kalchev</EventSpeakers>
    <EventSpeakerUniqueID>762980</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101991</EventHandoutURL>
    <blurb>We present an approach for building optimal preconditioners for algebraic linear systems coming from conforming finite element discretizations of partial differential equations. The preconditioners utilize the framework of auxiliary or fictitious space methods. Namely, the main idea is to reformulate the original problem in a nonconforming setting, using discontinuous elements across subdomains, obtaining the element-by-element assembly property. This is achieved by introducing additional unknowns associated with the interfaces between subdomains, which decouples the subdomains. The continuity is enforced weakly via interface penalty terms. The resulting nonconforming formulation (or a Schur complement (reduced) version of it, that "condenses" the formulation only to the interfaces) is used to precondition the original problem. The element-by-element assembly property can be useful in "matrix-free" computations for high order discretizations, since it minimizes the coupling across subdomains and interfaces. Also, this provides a natural setting to apply element-based algebraic multigrid (AMGe) techniques for solving the nonconforming formulation. The resulting decoupling and locality can be beneficial for parallel computing. The approach allows for directly obtaining coarse nonconforming formulations, or their reduced (Schur complement) versions. This largely reduces the cost of the "condensation" step.</blurb>
    <EventParentName>CP6 Multigrid and Preconditioning</EventParentName>
    <external_id>68195-101991</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>ThunderEgg: A Multigrid Solver for Variable Coefficient Elliptic Problems on Adaptive Cartesian Meshes</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP6</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:10PM</starts_at>
    <ends_at>Feb 13 2020  4:30PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Scott Aiton</EventSpeakers>
    <EventSpeakerUniqueID>786247</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102018</EventHandoutURL>
    <blurb>ThunderEgg is a library that provides geometric multigrid preconditioners for variable coefficient elliptic PDEs on octree and quadtree based Cartesian adaptive meshes. ThunderEgg is an object-oriented C++ library designed for flexibility and to allow users to implement multigrid preconditioners for various problems on octree and quadtree adaptive meshes. In this talk, we will give an overview of the design principles and use of the ThunderEgg library.  In addition, we will discuss numerical results from using this library to implement a variable coefficient elliptic multigrid preconditioned BiCGStab solver for the p4est based ForestClaw software package.</blurb>
    <EventParentName>CP6 Multigrid and Preconditioning</EventParentName>
    <external_id>68195-102018</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Automatic Scaling for Improved Conditioning in the Multiphysics Object-Oriented Simulation Environment</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP9</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:45AM</starts_at>
    <ends_at>Feb 14 2020 12:05PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Alexander Lindsay</EventSpeakers>
    <EventSpeakerUniqueID>785580</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102059</EventHandoutURL>
    <blurb>How to improve non-linear/linear convergence is a frequent question of PDE software. One reason for poor convergence can be an ill-conditioned linear operator. This scenario commonly arises in multi-physics simulations when different physics have different relative scales. It can even arise in single-physics cases when hanging-node constraints or Dirichlet boundary conditions are enforced strongly. To combat poor conditioning, we present an automatic scaling system implemented in the Multiphysics Object-Oriented Simulation Environment (MOOSE) that ensures the maximum absolute value for a single physics variable diagonal is unity. Improved convergence results for arbitrary multi-physics simulations employing either an explicit matrix or matrix-free linear operator are shown. Combination of automatic scaling with MOOSE’s automatic differentiation system for solution of thermo-mechanical problems is also demonstrated. Further research will focus on automatic scaling of saddle-point problems for application to mortar finite element problems like frictionless and frictional mechanical contact.
</blurb>
    <EventParentName>CP9 Autotuning</EventParentName>
    <external_id>68197-102059</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallel Sweeping Preconditioners For Domain Decomposition Methods Applied to the Helmholtz Equation</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP11</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:10PM</starts_at>
    <ends_at>Feb 14 2020  4:30PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ruiyang Dai</EventSpeakers>
    <EventSpeakerUniqueID>781131</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101522</EventHandoutURL>
    <blurb>Various wave propagation problems can be modelled in the frequency domain by the Helmholtz equation. Efficiently solving the Helmholtz equation at high frequencies in complex geometrical and/or material configurations is the subject of intense research, with recent promising contributions focused on high-order finite element methods coupled with optimized Schwarz Domain Decomposition (DD) schemes. Sweeping preconditioners for DD have recently gained a lot of interest because of their fast convergence, promising a number of DD iterations that is quasi independent of the number of subdomains. These preconditioners however have two major drawbacks: they rely on intrinsically sequential operations and they are naturally only suited for layered-type domain decompositions.

In this talk we will present a family of generalized sweeping preconditionners where sweeps can be done, in parallel, in several directions, for checkerboard-type domain decompositions. This contribution relies on the availability of accurate, high-order transmission conditions between the subdomains. Numerical results for high-frequency Helmholtz problems will be presented.</blurb>
    <EventParentName>CP11 Applications - Part III of III</EventParentName>
    <external_id>68198-101522</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Asynchronous Smoothers for Computational Fluid Dynamics on Graphics Processing Units</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP15</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:15PM</starts_at>
    <ends_at>Feb 15 2020  2:35PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Aditya Kashi</EventSpeakers>
    <EventSpeakerUniqueID>786279</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102051</EventHandoutURL>
    <blurb>Asynchronous iterations have started to regain interest over the past few years for fine-grain parallel solution of large sparse systems of equations. Recent efforts to use them for solving linear systems on modern parallel architectures include those by Anzt et al. ["A block-asynchronous relaxation method for graphics processing units", J. Parallel Distrib. Comput. (73), 2013] and Chow and Patel ["Fine-grained parallel incomplete LU factorization", SIAM J. Sci. Comput. (37:2), 2015]. However, application to specific domains has been lacking, especially in fluid dynamics. Hawkes et al. ["Chaotic linear-system solvers for unsteady CFD", VI International Conference on Computational Methods in Marine Engineering, 2015] demonstrated asynchronous linear iterations for incompressible flows in marine engineering, but on a traditional multi-core central processing unit (CPU).

Our work aims to demonstrate the applicability of asynchronous Gauss-Seidel and incomplete LU factorization smoothers to multigrid solvers for compressible turbulent flows in external aerodynamics, implemented for graphics processing units (GPUs). Our previous work has shown that modifications to existing asynchronous iterations are needed to achieve parallel scalability for compressible flows. In this work, we detail the modifications needed to achieve good performance on GPUs. Two- and three-dimensional benchmark cases of external aerodynamics on multi-block structured grids will be used as test cases.</blurb>
    <EventParentName>CP15 Highly Parallel Algorithms</EventParentName>
    <external_id>68202-102051</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Implementing Randomized Asynchronous Linear System Solvers</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP15</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  2:10PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Masha Sosonkina</EventSpeakers>
    <EventSpeakerUniqueID>700448</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102062</EventHandoutURL>
    <blurb>
	Asynchronous iterative methods present a mechanism to improve the performance of algorithms
for highly parallel computational platforms by removing the overhead associated with synchronization among computing elements. This paper considers a class of asynchronous iterative linear
system solvers that employ randomization to determine the component update orders, specifically focusing on the effects of drawing the order from non-uniform distributions. Results from
shared-memory experiments with a two-dimensional finite-difference discrete Laplacian problem
show that using distributions favoring the selection of components with a larger contribution to
the residual may lead to faster convergence than selecting uniformly. Multiple implementations
of the randomized asynchronous linear system solvers are considered and tested with various
distributions and parameters. In the best case of parameter choices, average times for the normal
and exponential distributions were, respectively, 13.3\% and 17.3\% faster than the performance
with a uniform distribution, and were able to converge in approximately 10\% fewer iterations
than traditional stationary solvers.</blurb>
    <EventParentName>CP15 Highly Parallel Algorithms</EventParentName>
    <external_id>68202-102062</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallel Tomographic Reconstruction - Where Combinatorics Meets Geometry</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP1</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  5:15PM</starts_at>
    <ends_at>Feb 12 2020  6:00PM</ends_at>
    <EventFilter>PP20|Invited Speaker</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Rob Bisseling</EventSpeakers>
    <EventSpeakerUniqueID>735416</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101777</EventHandoutURL>
    <blurb>Today, high-resolution tomographic reconstruction of 3D objects is within reach, but the associated data sets are huge and calling for parallel computation. A typical 3D reconstruction with 4k resolution already produces an image of 64 Gbytes. Tomographic reconstruction is often done using iterative algorithms that involve repeated sparse matrix-vector multiplication (SpMV). The matrix, however, may be too large to store, requiring Tbytes of memory, and hence each matrix row is recomputed upon use. In this talk, we present data partitioning methods for tomography matrices of increasing size. For small matrices, we can compute an optimal bipartitioning by an exact combinatorial method, as implemented in the packages MondriaanOpt and MP. This allows us to gauge the quality of medium-grain partitioning (default in the Mondriaan package), which is a heuristic combinatorial method that can handle larger problems. Medium-grain results in turn justified choosing row partitioning for the tomographic matrix-free SpMV. For this row partitioning, we developed a geometric recursive coordinate bisection
algorithm with nearly the same output quality as combinatorial partitioning that can handle huge, matrix-free problems and is also faster. We conclude with showing an actual reconstruction that was written using Bulk, a modern C++ library for easy development of parallel programs in bulk-synchronous parallel style. 
</blurb>
    <EventParentName>IP1 Parallel Tomographic Reconstruction - Where Combinatorics Meets Geometry</EventParentName>
    <external_id>67801-101777</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T17:15:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Modeling of Heterogeneous Computing Systems and Their Usages </name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP4</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  2:05PM</starts_at>
    <ends_at>Feb 14 2020  2:50PM</ends_at>
    <EventFilter>PP20|Invited Speaker</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hyesoon Kim</EventSpeakers>
    <EventSpeakerUniqueID>747195</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101778</EventHandoutURL>
    <blurb>The last decade has seen a paradigm shift in the architecture of computing platforms, with a trend toward combining general-purpose processors and specialized accelerators. For example, GPUs have made a significant impact in both the hardware industry and the application domain, as has been seen from the recent development of machine learning applications. Other platforms such as Processing-In-Memory and FPGA-based reconfigurable architectures have regained attention, and with these special accelerators, the computing platforms become more heterogeneous.  These heterogeneous architectures are especially attractive because they can provide high performance and energy efficiency for both general-purpose applications and high-throughput applications. Thus, from IoT devices to server processors, heterogeneous architectures have become increasingly popular.  However, these heterogeneous architectures introduce several new challenges, including programmability issues and designing the hardware architecture in a way that can maximally exploit the underlying heterogeneity. To address these issues, a wide variety of modeling has been used including analytical, regression based, and cycle-level. In this talk I will discuss how different modeling techniques have been addressed and how they can be helpful to guide architecture studies and software optimizations. </blurb>
    <EventParentName>IP4 Modeling of Heterogeneous Computing Systems and Their Usages </EventParentName>
    <external_id>67802-101778</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T14:05:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>What’s New in the Cuda Math Libraries</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS1</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:50PM</starts_at>
    <ends_at>Feb 12 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Timothy Costa</EventSpeakers>
    <EventSpeakerUniqueID>790037</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101849</EventHandoutURL>
    <blurb>Today’s fastest compute platforms are designed from the ground up to leverage the immense compute power of NVIDIA GPUs. As these platforms increase in scale and add specialized hardware, the CUDA Math Libraries are keeping up by constantly expanding, providing industry leading performance and coverage of common compute workflows across AI, ML and HPC. Major initiatives to support common workflows are: multi-GPU scalability, reduced and mixed precision computing, and libraries that allow kernel fusion and customizations. In this talk we review the latest developments in the CUDA Math Libraries including Tensor Core acceleration of HPC solvers without loss of accuracy, support for multiple GPUs in FFTs, BLAS and LAPACK routines, and the addition of new libraries with device function support and tensor linear algebra functionality.</blurb>
    <EventParentName>MS1 Industrial Mathematical Software</EventParentName>
    <external_id>67824-101849</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Performance Modeling of Applications by Benchmarks</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS2</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  2:15PM</starts_at>
    <ends_at>Feb 12 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Miwako Tsuji</EventSpeakers>
    <EventSpeakerUniqueID>780221</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101577</EventHandoutURL>
    <blurb>Widely used benchmarks, such as High Performance Linpack (HPL), no longer strongly correlate to real application performance. On the other hand, while using real applications or mini applications can give a direct estimation into application performance, more efforts have been required to port, to tune and to optimize them. In this work, we introduce a new methodology to estimate applications' performance by only performing simple benchmarks. We show and discuss some experimental results obtained several systems including X86, Arm, and so on. </blurb>
    <EventParentName>MS2 Meaningful Performance Indicators for Scientific Computing</EventParentName>
    <external_id>67722-101577</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Investigating Quasi-Newton Outer Product Representations on GPUs</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS3</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  2:15PM</starts_at>
    <ends_at>Feb 12 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Alp Dener</EventSpeakers>
    <EventSpeakerUniqueID>759916</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101746</EventHandoutURL>
    <blurb>The slowdown of Moore's Law and the growth of compute-intensive workloads such as artificial intelligence has pushed the development of high-performance computing (HPC) towards accelerator-based systems. Among supercomputers in the Top500 list, the share of accelerator FLOPs has grown from 20% in 2010 to 76% in 2018. As part of this trend, many libraries and application codes for solving partial-differential-equations (PDEs) have been ported and adapted to run on accelerator hardware such as graphical processing units (GPUs). Consequently, it has now become imperative for PDE-constrained optimization algorithms to also live on and exploit the capabilities of the same hardware used by the underlying PDE solvers. In the present work, we launch an investigation into the use of quasi-Newton (QN) methods on GPUs. QN approximations are among the most popular gradient-based kernels for solving large-scale nonlinear systems of equations, and are widely used for both continuous optimization and for PDE solutions. We implement both matrix-free and compact dense representations of several QN methods in PETSc/TAO, and leverage PETSc data structure abstractions to profile QN performance on both CPUs and GPUs using MPI and ViennaCL backends.</blurb>
    <EventParentName>MS3 Experiences in Developing GPU Support for Department of Energy Math Libraries - Part I of II</EventParentName>
    <external_id>67790-101746</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Toward Exascale Plasma Simulations using Particle in Cell (pic) Algorithms</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS5</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  1:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Matthew Bettencourt</EventSpeakers>
    <EventSpeakerUniqueID>740407</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101926</EventHandoutURL>
    <blurb>With the trend of computing resources moving toward more “advanced architectures” from more traditional architectures like Intel’s Phi systems (Trinity), to GPUs systems (Sierra/Summit)  to the first exascale Cray/Intel system (Aurora) the DOE is developing several new simulation tools in a variety of disciplines. This talk focuses Sandia National Laboratories’ code EMPIRE, which is a plasma simulation suite designed for performance on these new systems.  This talk will overview the many capabilities of the EMPIRE code but will focus on the unstructured mesh finite element particle in cell code, and the approach toward future proofing as computing moves toward exascale.  Results will be shown for realistic geometries on full system scale of Trinity, Astra and Sierra systems.

Sandia National Laboratories is a multimission laboratory managed and operated by National Technology &amp; Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy’s National Nuclear Security Administration under contract DE-NA0003525.
</blurb>
    <EventParentName>MS5 Parallel Simulation of Circuits, Devices, and Electromagnetics Environments Effects</EventParentName>
    <external_id>67840-101926</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallel Simulation of Large-Scale Integrated Circuits Using Xyce</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS5</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  2:15PM</starts_at>
    <ends_at>Feb 12 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Heidi Thornquist</EventSpeakers>
    <EventSpeakerUniqueID>719353</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101929</EventHandoutURL>
    <blurb>The Xyce Parallel Circuit Simulator is a SPICE-style analog circuit simulator that has been designed, from the ground up, to achieve scalable performance through distributed memory techniques.  With heterogeneous computing architectures becoming the norm, Xyce is going through a reinvention of the underlying infrastructure that has provided scalable computation for the last 20 years.  The infrastructure changes are wide-ranging, from device distribution and topology to the numerical methods that enable Xyce to perform time-domain and frequency-domain simulation.

In this talk we will present the basics of implementing a SPICE-style analog circuit simulator and take a closer look at the essential parts that make it scalable in circuit dimension, as well as parallel performance.  Some of the topics discussed will include techniques for dynamically distributing devices and the preconditioned linear solvers that are essential for large-scale circuit performance.  With a nod to the past capabilities of the Xyce circuit simulator, this talk will provide a forward-looking vision of what is coming next.

SNL is managed and operated by NTESS under DOE NNSA contract DE-NA0003525.
</blurb>
    <EventParentName>MS5 Parallel Simulation of Circuits, Devices, and Electromagnetics Environments Effects</EventParentName>
    <external_id>67840-101929</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The Numerical Stability Analysis of Parallel Conjugate Gradient Methods: Historical Context and Methodology</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS6</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  1:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Miro Rozloznik</EventSpeakers>
    <EventSpeakerUniqueID>707111</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101611</EventHandoutURL>
    <blurb>Numerical stability issues have been studied since the formulation of the conjugate gradient method in the middle of the last century, with many remarkable results achieved since then. 
Inexact computations in conjugate gradient method, due to either floating point roundoff error or due to intentional relaxation motivated by savings, 
have two basic effects, namely, slowing down convergence and limiting attainable accuracy. 
Although the methodologies for their investigation are different, these phenomena are closely related and cannot be separated from one another. 
Recently, the issues of attainable accuracy and delayed convergence caused by inexact computations became of interest in relation to pipelined conjugate gradient methods and their generalizations. 
In this contribution we recall the related early results and developments in synchronization-reducing conjugate gradient methods, 
identify the main factors determining possible numerical instabilities, and present a methodology for the analysis and understanding 
of pipelined conjugate gradient methods. We derive an expression for the residual gap that applies to any conjugate gradient method 
variant that uses a particular auxiliary vector in updating the residual, including pipelined conjugate gradient methods, 
and show how this result can be used to perform a full-scale analysis for a particular implementation. 

</blurb>
    <EventParentName>MS6 High Performance Krylov Subspace Methods: Theory, Implementation, and Application - Part I of III</EventParentName>
    <external_id>67748-101611</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Speculations on the Future of Krylov Methods using Results from Emerging Architectures</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS6</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:50PM</starts_at>
    <ends_at>Feb 12 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hannah Morgan</EventSpeakers>
    <EventSpeakerUniqueID>785744</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101614</EventHandoutURL>
    <blurb>Krylov methods are indispensable tools that are used to solve large, sparse systems of equations arising in many physical simulations. These methods spend considerable time in a few computational kernels such as vector dot products and AXPY operations. As such, it is crucial that we understand the execution of these operations in order to leverage good performance. However, as HPC architectures become more complex with the addition of compute units such as GPUs, this task becomes more complicated. In this talk, we detail PETSc benchmarking results from the CPU-GPU Summit computing system at the Oak Ridge Leadership Computing Facility. Through detailed study of the performance of the fundamental compute kernels, such as vector dot products, that are the building blocks of Krylov methods, we identify computational bottlenecks, guide the use of new systems, and perhaps inform the design of algorithms that we expect to run on heterogeneous architectures.
</blurb>
    <EventParentName>MS6 High Performance Krylov Subspace Methods: Theory, Implementation, and Application - Part I of III</EventParentName>
    <external_id>67748-101614</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Low Precision Floating-Point Arithmetic for the Solution of Linear System of Equations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS10</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Srikara Pranesh</EventSpeakers>
    <EventSpeakerUniqueID>785267</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101568</EventHandoutURL>
    <blurb>Motivated by the demand in machine learning, modern computer hardware is
increasingly supporting reduced precision floating-point arithmetic, which
provides advantages in speed, energy, and memory usage over single and
double precision. Given the availability of such hardware, mixed precision
algorithms that work in single or double precision but carry out part of a
computation in half precision are now of great interest for general
scientific computing tasks. GMRES-based iterative-refinement (GMRES-IR) is
one such algorithm for the solution of linear system of equations, that has
demonstrated 4 times speedup over state of the art solver using the tensor
cores feature of NVIDIA V100. Furthermore it has achieved a performance of
445 petaflops at scale on the Summit machine that leads the June 2019
TOP500 list. In this work we extend the applicability of GMRES-IR to linear
systems where the matrix is symmetric and positive definite. A major
challenge is that the matrix can lose its positive definiteness upon
conversion to half precision. To address this issue we propose a diagonal
perturbation based algorithm and experimentally demonstrate the
effectiveness of the half precision Cholesky factor in GMRES-IR. Since the
matrix is symmetric and positive definite, the conjugate gradient method
might seem preferable to GMRES. We demonstrate that for ill conditioned
matrices GMRES is the better choice because of its backward stability.</blurb>
    <EventParentName>MS10 Advances in Algorithms Exploiting Low Precision Floating-Point Arithmetic - Part I of II</EventParentName>
    <external_id>67712-101568</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Use of Deep Neural Networks for Estimating Subsurface Property Field from Time-Lapse Geophysical Imaging</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS13</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:35PM</starts_at>
    <ends_at>Feb 12 2020  3:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Xingyuan Chen</EventSpeakers>
    <EventSpeakerUniqueID>786718</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101628</EventHandoutURL>
    <blurb>The hydrological and biogeochemical processes at the groundwater and river water interface are largely controlled by the exchange dynamics between the two water bodies. Accurate characterization of the heterogeneous permeability field at such interface is critical for modeling the bulk flow as well as the biogeochemical processes that are coupled with the flow. Taking advantage of the distinct conductivities in groundwater and rive water, time lapse electrical resistivity tomography (ERT) can provide rich spatial and temporal data for characterizing the permeability field, by imaging the change in subsurface electric conductivity driven by river water intrusion and retreat.  In this study, we demonstrate the use of various deep neural networks including generative adversarial network for developing two-way mappings between the inputs to a system (i.e., heterogeneous permeability field and dynamic river stage etc) and system responses (i.e., the dynamic mixing of river water and groundwater imaged by electric resistivity tomography).  By developing such two-way mappings, deep learning methods were able to assist both parameter estimation and surrogate model development that are essential components when developing physics-based predictive models for complex systems. While the new methods are powerful in capturing complex signatures and patterns, we will also share the computational and data challenges we dealt with during the training phase. 
</blurb>
    <EventParentName>MS13 Physical-Based Modeling and Machine Learning for Biological and Environmental Sciences - Part II of II</EventParentName>
    <external_id>67752-101628</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The Canadian Hydrological Model</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS17</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Raymond Spiteri</EventSpeakers>
    <EventSpeakerUniqueID>47064</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102003</EventHandoutURL>
    <blurb>The Canadian Hydrological Model (CHM) is a C++ simulation code
designed to increase future capacity for hydrological simulations over
large spatial extents.  CHM combines multi-scale unstructured spatial
meshes with a plug-in architecture of modular process representations
for developing and simulating hydrological processes.  This
presentation provides an overview of CHM and its capabilities,
including recent technical advancements aimed at improving the
efficiency and scalability of CHM. These advancements include: 1) Mesh
organization for simplified element coupling, 2) Extension to
distributed computing environments through use of the Message Passing
Interface, and 3) Updated linear algebra solvers to efficiently deal
with advection-diffusion differential equation modules at large
spatial extents.  Preliminary results have shown that these
advancements have: 1) Significantly decreased the simulation run time
for a fixed simulation size and 2) Vastly increased the spatial domain
of a simulation without increasing simulation run time. For example,
using the Snowcast domain (www.snowcast.ca) with a snowdrift-resolving
mesh (approximately 100 m ridge scale) as a test, the speed of
simulation decreased by more than an order of magnitude.
</blurb>
    <EventParentName>MS17 HPC Simulation of the Hydrological Cycle</EventParentName>
    <external_id>67856-102003</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallelization of Computations Across Hierarchical River Networks</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS17</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:35PM</starts_at>
    <ends_at>Feb 12 2020  3:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Martyn Clark</EventSpeakers>
    <EventSpeakerUniqueID>748725</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102004</EventHandoutURL>
    <blurb>In this presentation, we will highlight some key cyberinfrastructure challenges in continental-domain hydrological modeling. We will discuss 1) multi-scale continental-domain instantiations from the same geospatial framework, introducing novel approaches for adaptive nests; (2) methods in large-domain parameter estimation, focusing on the development of model workflows and computational infrastructure; and (3) parallelization of continental-domain models, with focus on efficient hierarchical/hybrid spatial decomposition strategies that are necessary to efficiently process connected river networks. We will summarize recent progress on each of these topics as well as outstanding research challenges.</blurb>
    <EventParentName>MS17 HPC Simulation of the Hydrological Cycle</EventParentName>
    <external_id>67856-102004</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>DeepSparse: A Task-Parallel Framework for Sparse Solvers on Deep Memory Architectures</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS18</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>H. Metin Aktulga</EventSpeakers>
    <EventSpeakerUniqueID>751715</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101951</EventHandoutURL>
    <blurb>Data movement is an important performance bottleneck in large-scale sparse matrix computations, e.g., linear solvers, eigensolvers and graph analytics. We introduce a novel sparse solver framework, named DeepSparse, which adopts a fully integrated task-parallel approach. DeepSparse differs from existing work in that it adopts a holistic approach that targets all computational steps in a sparse solver rather than narrowing the problem into specific kernels (e.g., SpMVs, SpMMs). We present the implementation details of DeepSparse and demonstrate its merit in two commonly used eigensolvers, Lanczos and LOBPCG algorithms. We observe that DeepSparse achieves 2$\times$ - 16$\times$ fewer cache misses across different layers (L1, L2 and L3) and 2$\times$ - 3.9$\times$ reduction in execution time over implementations of the same solvers using optimized library function calls on traditional multicore architectures. We also discuss extensions to DeepSparse for facilitating effective memory management in multi-level memory hierarchies that is becoming prevalent in high-end systems as a result of the high-bandwidth device memory and DRAM coupling.
</blurb>
    <EventParentName>MS18 Exploiting Task Parallelism in Exascale Computing Era</EventParentName>
    <external_id>67850-101951</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Exploiting Task Parallelism in an Exascale Ecosystem: Some Issues and Possible Solutions</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS18</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:35PM</starts_at>
    <ends_at>Feb 12 2020  3:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Olivier Aumage</EventSpeakers>
    <EventSpeakerUniqueID>769421</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101954</EventHandoutURL>
    <blurb>With the advent of the exascale era, the level of scale reached in terms
of hardware parallelism, both within nodes and between nodes, is such
that new issues have to be addressed by task-based parallel runtime
system, to efficiently use computing resources and to orderly interact
with the ecosystem. This talk will discuss some of these issues, ranging
from parallelism expression, work mapping, dependence management, to
interoperability issues, and introduce some possible solutions explored
by INRIA Team STORM in Bordeaux, France.
</blurb>
    <EventParentName>MS18 Exploiting Task Parallelism in Exascale Computing Era</EventParentName>
    <external_id>67850-101954</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Homogenized Modeling of Microscopic Anisotropic Diffusion for Effective Diffusivities in Stratum Corneum</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS19</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:25PM</starts_at>
    <ends_at>Feb 12 2020  4:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Junxi Wang</EventSpeakers>
    <EventSpeakerUniqueID>790032</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101831</EventHandoutURL>
    <blurb>In pharmacology, mathematical models of molecular diffusion through the stratum corneum (SC) layer can be greatly used to predict transdermal delivery of drugs and to risk assessment of chemical exposures. The membrane microstructure is generally accepted as a periodic arrangement of corneocytes embedded in an anisotropic lipid matrix. We show how the effective diffusivity can be calculated using mathematical models from homogenization theory. Numerical results are expressed in the dimensionless effective diffusivity tensor $\bar{A}^{SC}$, which is a function of homogenization results $\chi_i$ in each field. $\chi_i$ is determined by the dimensionless parameters, $\lambda$ and $\gamma$. $\lambda$ is the degree of anisotropy in a lipid bilayer and $\gamma$ is the degree of drug hydrophilicity between corneocyte and lipid phases. The effective diffusion tensors are finally calculated for a classical and a spatial "brick-and-mortar" structures and a tetrakaidekahedral-shaped cell. A comparison with a previous study confirms that transdermal and lateral diffusivities are strongly influenced by $\lambda$ due to the barrier function of the lipid phase against transbilayer flow. The lateral diffusivities of poorly water-soluble drugs are limited by $\gamma$ in contract to transdermal diffusivity. This presented method may more precisely and more intensively predict the drug permeability and evaluate the membrane microstructure. 
</blurb>
    <EventParentName>MS19 Parallel Adaptive Multigrid - Part II of II</EventParentName>
    <external_id>67819-101831</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Reduced Precision in Weather Forecasting Models</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS21</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:45AM</starts_at>
    <ends_at>Feb 13 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Andrew McRae</EventSpeakers>
    <EventSpeakerUniqueID>789948</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101540</EventHandoutURL>
    <blurb>Double-precision arithmetic has become standard in numerical codes. However, in real applications, most of this precision is unnecessary. This is particularly true in the field of weather and climate modelling, in which model error is large, and parameterisations of unresolved physical processes introduce reasonably big errors at the grid scale. Indeed, several operational weather prediction models now successfully use single-precision in some or all of their code. Using software emulation of reduced precision, we have lowered numerical precision even further than this in a range of problems, from toy ODE models to components of state-of-the-art forecast systems. I will summarise some of this work, including identifying workarounds and calculations that require higher precision, with an eye towards future hardware in which precision may be adjustable on a per-operation basis.</blurb>
    <EventParentName>MS21 Advances in Algorithms Exploiting Low Precision Floating-Point Arithmetic - Part II of II</EventParentName>
    <external_id>67713-101540</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Strategies, Challenges, and Lessons Learned in Developing GPU Support for SUNDIALS</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS3</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:25PM</starts_at>
    <ends_at>Feb 12 2020  1:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Cody Balos</EventSpeakers>
    <EventSpeakerUniqueID>784882</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101744</EventHandoutURL>
    <blurb>SUNDIALS is a suite of robust and scalable solvers for systems of ordinary differential equations, differential-algebraic equations, and nonlinear equations designed to be used on a variety of computing systems ranging from laptops to super computers. With GPUs providing most of the FLOPS available on top-end machines, such as Summit at Oak Ridge National Laboratory, there has been a large effort to develop GPU support in SUNDIALS. Encapsulation of parallelism within SUNDIALS lends itself to a framework where data is offloaded to the GPU and control is kept on the CPU. In this framework, we can support GPUs in SUNDIALS by providing interfaces to GPU-enabled vector and linear solver operations. Another avenue of support involves taking advantage of the fact that SUNDIALS packages often operate at a fine-grained level of an application's parallel decomposition. In this talk we will discuss the strategies, challenges, and the lessons learned in our ongoing effort.
\newline\newline
This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. LLNL-ABS-789680.









</blurb>
    <EventParentName>MS3 Experiences in Developing GPU Support for Department of Energy Math Libraries - Part I of II</EventParentName>
    <external_id>67790-101744</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Scalable Deep Learning of Biological Dynamical Systems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS4</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  2:15PM</starts_at>
    <ends_at>Feb 12 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Bethany Lusch</EventSpeakers>
    <EventSpeakerUniqueID>789988</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101626</EventHandoutURL>
    <blurb>The discovery of governing equations from scientific data has the potential to transform data-rich fields that lack models. Advances in sparse regression are currently enabling the tractable identification of both the structure and parameters of a nonlinear dynamical system from data. The resulting models have the fewest terms necessary to describe the dynamics, balancing model complexity with descriptive ability, and thus promoting interpretability and generalizability. This provides an algorithmic approach to Occam's razor for model discovery. However, this approach fundamentally relies on an effective coordinate system in which the dynamics have a simple representation. In this work, we design a custom autoencoder to discover a coordinate transformation into a reduced space where the dynamics may be sparsely represented. Thus, we simultaneously learn the governing equations and the associated coordinate system. The resulting modeling framework combines the strengths of deep neural networks for flexible representation and sparse identification of nonlinear dynamics (SINDy) for parsimonious models. It is the first method of its kind to place the discovery of coordinates and models on an equal footing. We demonstrate this approach on several example high-dimensional biological dynamical systems with low-dimensional behavior. We then discuss how this method scales with the size of the system and the size of the dataset.
</blurb>
    <EventParentName>MS4 Physical-Based Modeling and Machine Learning for Biological and Environmental Sciences - Part I of II</EventParentName>
    <external_id>67751-101626</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Gemma: A Sandia National Laboratories Electromagnetic Code for Heterogeneous Computer Architectures</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS5</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:25PM</starts_at>
    <ends_at>Feb 12 2020  1:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Brian Zinser</EventSpeakers>
    <EventSpeakerUniqueID>790065</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101927</EventHandoutURL>
    <blurb>Gemma aims to provide Sandia National Laboratories with a tool for efficiently solving frequency domain electromagnetic problems on heterogeneous computer architectures, including CPUs, GPUs, and MICs, using Sandia’s Kokkos performance portability library. While Gemma will include alternative algorithms for select use cases, it will rely on the method of moments to accurately and reliably solve electromagnetic problems on the most powerful computers available. Through several case studies of simulating electromagnetic environment effects, this presentation will illustrate Gemma’s approach to establishing confidence and trustworthiness in its solutions. SNL is managed and operated by NTESS under DOE NNSA contract DE-NA0003525.
</blurb>
    <EventParentName>MS5 Parallel Simulation of Circuits, Devices, and Electromagnetics Environments Effects</EventParentName>
    <external_id>67840-101927</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Inexactness and Compression in Krylov Subspace Methods</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS6</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:25PM</starts_at>
    <ends_at>Feb 12 2020  1:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Nick Schenkels</EventSpeakers>
    <EventSpeakerUniqueID>789984</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101613</EventHandoutURL>
    <blurb>Krylov subspace methods have been around since the second half of the 20th century and remain among the most widely used algorithms for solving large linear systems. In order to adapt to the ever more parallel architecture of computer systems and -- soon -- exascale systems they remain the subject of ongoing research.

One recent development are so-called inexact Krylov subspace methods. These are inspired by the observation that the matrix vector product required in each iteration is often only calculated approximately [V. Simoncini and D. B. Szyld: Flexible inner-outer Krylov subspace methods. SIAM Journal on Numerical Analysis (2002)] [L. Giraud, S. Gratton and J. Langou: Convergence in backward error of relaxed GMRES. SIAM Journal on Scientific Computing (2007)]. At the same time, it has become clear that the main bottlenecks on HPC systems are the communication bandwidth and the storage capacity. Lossy data compression techniques have therefore gained a lot of interest [D. Tao et al.: Significantly improving lossy compression for scientific data sets based on multidimensional prediction and error-controlled quantization. IEEE IPDPS 2017] [S. G\"otschel and M. Weiser: Lossy Compression for Large Scale PDE Problems. arXiv preprint (2019)].

In this talk we will explore the connection between inexact Krylov subspace methods and lossy compression and discuss some practical strategies that allow us to reduce the memory footprint of these methods.
</blurb>
    <EventParentName>MS6 High Performance Krylov Subspace Methods: Theory, Implementation, and Application - Part I of III</EventParentName>
    <external_id>67748-101613</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Network Bottleneck Handling in Multi-Threaded MPI Context</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS8</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:25PM</starts_at>
    <ends_at>Feb 12 2020  1:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Julien Jaeger</EventSpeakers>
    <EventSpeakerUniqueID>780365</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101571</EventHandoutURL>
    <blurb>Nowadays, codes combine multiple parallel programming models. These models often follow the hybrid MPI+X pattern, with X being a threaded model, either directly in the code or in DSL abstraction layers. To offer a better support for hybrid programs, MPI provides different levels of thread support up to MPI\_THREAD\_MULTIPLE, which allows a program to call MPI functions in any thread of a process. This behavior has multiple impacts: first on the MPI library which has now to be thread-safe, then on the interconnect card and the network due to the flow of messages coming from all the threads.

MPC implements its own MPI library with two flavors: process-based and thread-based. Having a thread-based MPI enforces to have an efficient thread-safe MPI library. Hence, MPC has to deal with all the bottlenecks induced by multi-threaded MPI. In this presentation, we will describe some of the features implemented in MPC to loosen these bottlenecks.

</blurb>
    <EventParentName>MS8 Co-Design of Networking for Scientific HPC Applications</EventParentName>
    <external_id>67721-101571</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallel Geometric Multigrid for Continuum Models and Numerical Optimization</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS9</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  1:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Andreas Vogel</EventSpeakers>
    <EventSpeakerUniqueID>750552</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101823</EventHandoutURL>
    <blurb>Geometric multigrid is well-known for highly-scalable implementations on distributed machines by balancing the coarse grid hierarchy among the involved processes. It nicely interplays with adaptive mesh refinement to account for highly localized features and to speed up the computation by saving non-required degrees of freedom. We present an overview about our implementation in the massively parallel simulation framework UG4 (unstructured grids) focusing on distributed-memory architectures. The applicability for largest high-performance computing clusters is demonstrated on a variety of problems including PDE continuum model simulations and numerical optimization.</blurb>
    <EventParentName>MS9 Parallel Adaptive Multigrid - Part I of II</EventParentName>
    <external_id>67818-101823</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Dimension Switching Multigrid Method and Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS9</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:50PM</starts_at>
    <ends_at>Feb 12 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Qingguang Guan</EventSpeakers>
    <EventSpeakerUniqueID>790031</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101827</EventHandoutURL>
    <blurb>In this project, we developed an efficient MultiGrid Method using dimension switch technique. To reduce the computation cost for modeling signal propagation within and between neurons in 3D, we employed 1D partition to replace the coarse grid in 3D but keep the fine grid. This way balances both efficiency and accuracy of MultiGrid Method. We started from working on a perfect cylinder to represent the axon of a neuron, then extend the method to simulate signal transport in axons with general shapes. The ultimate goal is to couple the method into solving problems on whole neurons. Meanwhile, we considered the implementation of parallel computation on multiprocessors with distributed memory for the proposed method. 







</blurb>
    <EventParentName>MS9 Parallel Adaptive Multigrid - Part I of II</EventParentName>
    <external_id>67818-101827</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Space-Time Semi-Geometric Multigrid Method for Electrophysiology</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS9</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  2:15PM</starts_at>
    <ends_at>Feb 12 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Patrick Zulian</EventSpeakers>
    <EventSpeakerUniqueID>769510</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102047</EventHandoutURL>
    <EventParentName>MS9 Parallel Adaptive Multigrid - Part I of II</EventParentName>
    <external_id>67818-102047</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Nash Embedding: A Road Map to Realizing Quantum Hardware</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS11</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Faisal Shah  Khan</EventSpeakers>
    <EventSpeakerUniqueID>790297</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102075</EventHandoutURL>
    <blurb>The non-Euclidean nature of the state-space of qubits (and qudits in general) gives rise to the problem of practically implementing quantum circuits in physical hardware which necessarily resides in the Euclidean space R3. On the other hand, the Euclidean nature of bits (and dits in general) makes the implementation of reversible circuits in physical hardware relatively straight forward. I offer here a road-map to solving this problem in which the Nash embedding theorem isometrically maps qubits into bits and a quantum circuit into an equivalent reversible one, followed by the embedding of the resulting reversible circuit into R3 as a hardware graph.

</blurb>
    <EventParentName>MS11 Formal (Mathematical) Methods Enabling Applications of Quantum Computers</EventParentName>
    <external_id>67858-102075</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>GAMA: Quantum and Quantum-Inspired Algorithms for Non-Linear Integer Optimization</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS11</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:35PM</starts_at>
    <ends_at>Feb 12 2020  3:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sridhar Tayur</EventSpeakers>
    <EventSpeakerUniqueID>790298</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102076</EventHandoutURL>
    <blurb>We discuss two original approaches to solve non-linear integer optimization problems that arise in finance, cancer genomics and supply chain optimization. Our Graver Augmented Multi-seed Algorithm (GAMA) utilizes augmentation along Graver basis elements (a test set) from multiple initial feasible solutions.
•	A hybrid quantum-classical approach (GAMA-Q) is tested on D-Wave. Our experiments suggest that with a modest increase in coupler precision–along with near-term improvements in the number of qubits and connectivity that are expected–the ability to outperform classical best-in-class algorithms is within reach.
•	A (fully classical) approach (GAMA-C) is well suited for Cardinality Boolean Quadratic Problems (CBQP), Quadratic Semi- Assignment Problems (QSAP) and Quadratic Assignment Problems (QAP). We find that for several instances of practical relevance, GAMA-C vastly out-performs best-in-class commercial solvers in terms of time to find the optimal solution (by two or three orders of magnitude).

</blurb>
    <EventParentName>MS11 Formal (Mathematical) Methods Enabling Applications of Quantum Computers</EventParentName>
    <external_id>67858-102076</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Infusing Physics and Domain Knowledge into ML and DL Models</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS13</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Karthik Kashinath</EventSpeakers>
    <EventSpeakerUniqueID>786076</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101627</EventHandoutURL>
    <blurb>Simulating complex multi-scale physical systems often involves solving partial differential equations (PDEs) with closures for the unresolved scales. Although the advancement of HPC has made resolving small-scale physics possible, such simulations are still very expensive. Therefore, reliable and accurate closure models for the unresolved physics remains an important requirement for many computational physics problems. Recently, generative adversarial networks (GANs), have shown promise in emulating complex systems without explicitly solving their governing PDEs. However, GANs are known to be difficult to train and to achieve convergence. We present approaches to enforcing constraints either from the training data or from the underlying physics of the system. We also show that such approaches can lead to better performance, measured by (i) the model’s ability to better emulate physical properties of the system, and (ii) reduced training time. We exemplify this approach on canonical turbulent flows of relevance to fluid and climate dynamics. Given the ever-growing high-fidelity simulation databases of physical systems, this work shows potential as an alternative to the explicit modeling of closures or parameterizations for unresolved physics, which are known to be a major source of uncertainty in simulating multi-scale physical systems such as turbulent flows or Earth’s weather and climate</blurb>
    <EventParentName>MS13 Physical-Based Modeling and Machine Learning for Biological and Environmental Sciences - Part II of II</EventParentName>
    <external_id>67752-101627</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Large-Scale Multiphase Flow Simulations with AMR using Dynamic Load Balance</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS14</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 11:00AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>701</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Takayuki Aoki</EventSpeakers>
    <EventSpeakerUniqueID>735115</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101648</EventHandoutURL>
    <blurb>It is one of challenging topics to carry out large-scale simulations for multiphase flows such as gas-liquid or and solid-gas two-phase flows. We have developed a weakly compressible flow solver and applied AMR (Adaptive Mesh Refinement) fine meshes to the gas-liquid interfaces. Dynamic domain partitioning based on space-filling curves has been employed and a few simulations on liquid film dynamics and tsunami flows with a lot of debris are carried out on 10-100 GPUs of the GPU supercomputer TSUBAME 3.0.
</blurb>
    <EventParentName>MS14 Advanced Visualisation, Analysis, and Parallelisation Concepts for Multi-Scale CFD Simulations in Science and Engineering</EventParentName>
    <external_id>67759-101648</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Detecting and Mitigating Stagnation in Nonlinear Multiphysics Problems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS15</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:35PM</starts_at>
    <ends_at>Feb 12 2020  3:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Matthew Knepley</EventSpeakers>
    <EventSpeakerUniqueID>719838</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101833</EventHandoutURL>
    <blurb>We examine some straightforward tools to understand stagnation of nonlinear iterations for coupled systems, which mainly amount to monitoring portions of the residual. Using this information, we try to guide the choice of nonlinear preconditioner. Some illustrative examples are presented using PETSc.</blurb>
    <EventParentName>MS15 Nonlinear Preconditioning</EventParentName>
    <external_id>67821-101833</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Nonlinear Feti-Dp and Bddc Methods</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS15</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:25PM</starts_at>
    <ends_at>Feb 12 2020  4:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Axel Klawonn</EventSpeakers>
    <EventSpeakerUniqueID>720131</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101835</EventHandoutURL>
    <EventParentName>MS15 Nonlinear Preconditioning</EventParentName>
    <external_id>67821-101835</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Multi-Architecture Parallel Particle Simulations on HPC Systems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS22</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:20AM</starts_at>
    <ends_at>Feb 13 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>4000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Pietro Incardona</EventSpeakers>
    <EventSpeakerUniqueID>781843</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101800</EventHandoutURL>
    <blurb>Parallel numerical simulation using particles and/or meshes are common in both scientific and industrial applications. Popular numerical methods that fall into this category are Smoothed Particle Hydrodynamics (SPH), Vortex Methods (VM), Molecular Dynamics (MD), Discrete Element Methods (DEM), Finite-Difference Methods (FDM), Particle-in-Cell (PIC) codes, and Finite-Volume methods (FVM). Parallel implementation of all of these methods can be expressed as a set of common abstract data types and operators, as implemented in the OpenFPM middleware. OpenFPM is a scalable C++ framework for rapid and efficient development of particle- and particle-mesh simulations. It uses C++ Template Meta-Programming for compile-time code generation in order to allow arbitrary C++ objects to be particle or mesh node properties, and perform simulations in arbitrary-dimensional spaces. Originally developed for shared- and distributed-memory CPU computing, we here present an extension of OpenFPM to GPUs and multi-GPU setups. We demonstrate two different ways of implementing GPU computations (using kernels and using inline code) in OpenFPM without explicitly having to write any CUDA, OpenCL, or the like. This renders GPU computing much more accessible and enables rapid porting of existing CPU codes to GPUs and to using a mixture of CPU and GPU operations in a distributed environment. 
</blurb>
    <EventParentName>MS22 Parallel Processing for Particle Codes - Part I of II</EventParentName>
    <external_id>67814-101800</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The Sparse Matrix Vector Product on High-End GPU Clusters</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS23</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:20AM</starts_at>
    <ends_at>Feb 13 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yuhsiang Tsai</EventSpeakers>
    <EventSpeakerUniqueID>790038</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101884</EventHandoutURL>
    <blurb>Efficient processing of Irregular Matrices on SIMD-type architectures is a persistent challenge. Resolving it requires innovations in the development of data formats, computational techniques, and implementations that strike a balance between thread divergence, which is inherent for irregular matrices, and padding, which alleviates the performance-detrimental thread divergence but introduces artificial overheads. To this end, we address the challenge of designing high performance sparse matrix-vector product kernels for GPUs with a coordinate format suitable for unbalanced matrices. We also provide a hybrid algorithm that stores part of the matrix in SIMD-friendly ELL format. The ratio between the ELL- and the COO-part is determined using a theoretical analysis of the nonzeros-per-row distribution. For the over 2,800 test matrices available in the Suite Sparse matrix collection, we compare the performance against vender kernels providing the same functionality.
</blurb>
    <EventParentName>MS23 Advances and Challenges in Solvers on GPGPU-based High-Performance Computing Architectures - Part I of II</EventParentName>
    <external_id>67833-101884</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The Lanczos Method in Data Science: New Challenges and the Continued Importance of Stability</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS26</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christopher Musco</EventSpeakers>
    <EventSpeakerUniqueID>781678</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101619</EventHandoutURL>
    <blurb>The Lanczos method is one of linear algebra’s most remarkable and enduring algorithms. Introduced nearly 70 years ago, this algorithm is uniquely multipurpose: it can be used to invert linear systems, to compute eigenvectors, and to approximately solve dozens of other important matrix problems. For many of these problems, the Lanczos method gives both state-of-the-art theoretical guarantees and is the method of choice in practice.

I will discuss a resurgence of interest in applying the Lanczos method to new applications in machine learning and computational data science, including regularized function fitting, online eigenvector estimation, and spectral analysis. In data applications, Lanczos is especially powerful when composed with highly scalable stochastic optimization or randomized sketching methods. Combining Lanczos with these inherently noisy tools raises questions about the method's  stability that tie back to decades old work on understanding how the algorithm performs in finite precision. I will discuss recent work on addressing some of these questions, as well as a number of interesting open problems.
</blurb>
    <EventParentName>MS26 High Performance Krylov Subspace Methods: Theory, Implementation, and Application - Part III of III</EventParentName>
    <external_id>67750-101619</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Wasserstein Discriminant Analysis with Eigensolver</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS26</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:20AM</starts_at>
    <ends_at>Feb 13 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hexuan Liu</EventSpeakers>
    <EventSpeakerUniqueID>784079</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101620</EventHandoutURL>
    <blurb>The original Wasserstein Discriminant Analysis paper uses automatic differentiation to compute the partial derivative of the optimization objective with respect to the subspace P, and then uses projected gradient descent to compute the optimal P. In practice, this approach often takes too many iterations to reach convergence when applied to real data and the unconverged solution does not serve as a good approximation to the true optimal. We instead relax the optimization problem to a generalized eigenvalue problem and solve the nonlinear generalized eigenvalue problem using the Self-consistent field iteration. The new solver, which usually converges within 10 iterations in practice, provides a huge speedup in convergence compared to the original solver and therefore aids in iteratively applying WDA to find the most discriminative subspace in an unsupervised setting.
</blurb>
    <EventParentName>MS26 High Performance Krylov Subspace Methods: Theory, Implementation, and Application - Part III of III</EventParentName>
    <external_id>67750-101620</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>{AMR} in Core-Collapse Supernova Simulations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS27</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Anshu Dubey</EventSpeakers>
    <EventSpeakerUniqueID>762908</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101676</EventHandoutURL>
    <blurb>Successful simulations for scientific discovery on high-performance computing (HPC) platforms require careful planning, including verification of specific application configuration and runtime parameters, estimation of resource requirements, and steering and monitoring of the simulation. With rising heterogeneity in both platforms and solvers within applications, effective use of the available hardware is becoming more challenging on HPC platforms. Application configuration has to evaluate the mapping of
computation to hardware resources with cost-benefit analysis that weighs the overhead of using the target resource (e.g. data movement) against its computational efficiency in addition to the other concerns of simulation planning. The runtime at the application level needs this information to orchestrate the task assignment and data movement between devices on the compute nodes, and between different nodes. We present a methodology for building component-based cost models that can help in resource and simulation planning, and illustrate the methodology through formulation of a cost model for simulating thermonuclear supernovae using FLASH, a highly configurable adaptive mesh refinement based community code.
</blurb>
    <EventParentName>MS27 Challenges in Parallel Adaptive Mesh Refinement - Part I of III</EventParentName>
    <external_id>67770-101676</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Training Quantum Boltzmann Machines on Near-Term Quantum Devices</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS29</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 12:10PM</starts_at>
    <ends_at>Feb 13 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Nathan Wiebe</EventSpeakers>
    <EventSpeakerUniqueID>791811</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101655</EventHandoutURL>
    <EventParentName>MS29 Recent Advances and Trends in Hybrid Quantum-Classical Algorithms - Part I of II</EventParentName>
    <external_id>67762-101655</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>ENSIGN: A Framework for High-performance Tensor Decompositions</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS31</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Muthu Baskaran</EventSpeakers>
    <EventSpeakerUniqueID>758357</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101918</EventHandoutURL>
    <blurb>ENSIGN is a high-performance tensor decomposition framework developed by Reservoir Labs that is intended to transition tensor methods into a practical data science tool for addressing modern data analytics problems at scale. The core of ENSIGN is a suite of decomposition routines built on specialized data structures that are optimized for large shared- and distributed-memory architectures. These optimizations include operation-minimal parallel computations leading to near-ideal scaling of computations, reduced communications and synchronizations, and in-memory computation that avoids writing data to disk throughout the workflow. Through these optimizations, ENSIGN has been able to effectively exploit the ultra-low latency and high-bandwidth benefits offered by the HPE Superdome Flex server, a high-end modular system offering large-scale in-memory computing, and has also demonstrated improved performance on a distributed cluster of Intel Xeon multi-core nodes.

ENSIGN has been applied successfully to problems in cybersecurity and geospatial analytics, most recently assisting in the cybersecurity of SCinet 2019 where ENSIGN was able to uncover otherwise undetected patterns of distributed attacks and other malicious activity. With the addition of a Python-enabled in-memory workflow, ENSIGN can now be used in concert with other popular machine learning libraries.

</blurb>
    <EventParentName>MS31 Frameworks/Libraries for High-Performance Tensor Computations - Part II of II</EventParentName>
    <external_id>67795-101918</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The Sparse Tensor Algebra Compiler</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS31</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 12:10PM</starts_at>
    <ends_at>Feb 13 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Fredrik Kjolstad</EventSpeakers>
    <EventSpeakerUniqueID>790064</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101924</EventHandoutURL>
    <blurb>Tensor and Linear Algebra are powerful tools with applications in data analytics, machine learning, science, and engineering. The massive growth of data in these applications makes performance critical. For applications that use sparse tensors, where most components are zeros, programmers must choose between libraries with hand-optimized implementations of select operations and generalized software systems with poor performance.

In this talk, I will present compiler abstractions and techniques that combine tensor expressions with specifications of sparse irregular tensor data structures to produce efficient parallel source code. I will discuss the three main problems of sparse tensor algebra compilation and show how their solutions can be combined to compile and optimize sparse tensor expressions. We have implemented these ideas in the TACO sparse tensor algebra compiler. It is the first compiler to generate sparse code for any basic tensor expression on many sparse tensor representations. The generated code matches or exceeds the performance of hand-optimized libraries while generalizing to any expression and many user-specified irregular data structures.</blurb>
    <EventParentName>MS31 Frameworks/Libraries for High-Performance Tensor Computations - Part II of II</EventParentName>
    <external_id>67795-101924</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Transparency and Reproducibility: Case Studies, Formalisms, and Structured Guidance in Computational Social Science Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS32</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:45PM</starts_at>
    <ends_at>Feb 13 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>4000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Victoria Stodden</EventSpeakers>
    <EventSpeakerUniqueID>739488</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101752</EventHandoutURL>
    <EventParentName>MS32 Transparency, Reproducibility, Sustainability, and Security: The Four Pillars of the Next Generation Scientific Software Stack</EventParentName>
    <external_id>67792-101752</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Shifted CholeskyQR3 for High Performance Tall-Skinny QR Factorization</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS35</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Takeshi Fukaya</EventSpeakers>
    <EventSpeakerUniqueID>752089</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101944</EventHandoutURL>
    <blurb>The Cholesky QR algorithm is suitable for high performance computing of the QR factorization of a tall and skinny matrix but has a serious numerical instability. The CholeskyQR2 algorithm, that is Cholesky QR with reorthogonalization, has improved stability but still has the problem of breakdown for ill-conditioned matrices (i.e. when the condition number is larger than $10^8$ when using double precision arithmetic). Recently, we proposed the shifted Cholesky QR algorithm, where we introduced a positive diagonal shift into the computed Gram matrix so as to avoid the breakdown of its numerical Cholesky factorization. Then, we use shifted Cholesky QR as a preconditioning step before CholeskyQR2, and finally we can obtain an accurate QR factorization even if the target matrix is ill-conditioned (up to about $10^{16}$). We call the resulting algorithm shifted CholeskyQR3. 

In this talk, we will give an overview of shifted CholeskyQR3 algorithm and related theoretical results. Then, we will present detailed performance results on recent computer systems, which show the effectiveness of shifted CholeskyQR3. We will also mention extensions of shifted CholeskyQR3 such as the QR factorization in an oblique inner product space.</blurb>
    <EventParentName>MS35 Parallel Matrix Factorization Algorithms - Part II of III</EventParentName>
    <external_id>67848-101944</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallel Butterfly-Based Sherman–Morrison–Woodbury Inversion</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS35</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:35PM</starts_at>
    <ends_at>Feb 13 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yang Liu</EventSpeakers>
    <EventSpeakerUniqueID>780194</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101947</EventHandoutURL>
    <blurb>Butterfly decompositions, originally inspired by fast Fourier transforms (FFT), are promising numerical linear algebra tools for constructing fast direct solvers and preconditioners for highly oscillatory integral and differential equations. Previously, butterfly extensions of the Hierarchical matrix and HODLR (hierarchically off-diagonal low-rank) matrix formats have been developed to accelerate compression and factorization of dense linear systems arising from integral equations. These solvers represent off-diagonal blocks as butterfly decompositions and leverage complicated hierarchical butterfly arithmetic to achieve $O(N\mathrm{log}^2N)$ compression and $O(N^{1.5}\mathrm{log}N)$ factorization complexities. 
Here we further simplify the hierarchical representation via essentially representing the entire matrix as one butterfly decomposition plus manageable number of smaller blocks. This representation can be treated as butterfly generalization of HSS (hierarchically semi-separable) matrix format and reduce the compression complexity to $O(N\mathrm{log}N)$ compared to existing constructs. The essential operation for its factorization is the butterfly extension of the Sherman–Morrison–Woodbury inversion formula, which can be computed recursively leveraging the randomized butterfly arithmetic. In addition, a distributed-memory parallelization of the proposed format has been developed and integrated into the software package ButterflyPACK. 
</blurb>
    <EventParentName>MS35 Parallel Matrix Factorization Algorithms - Part II of III</EventParentName>
    <external_id>67848-101947</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Application Developers’ Experiences with RAJA</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS36</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:10PM</starts_at>
    <ends_at>Feb 13 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>David Beckingsale</EventSpeakers>
    <EventSpeakerUniqueID>789959</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101564</EventHandoutURL>
    <blurb>Modern high-performance computing (HPC) hardware provides a wide range of heterogeneous execution resources, each of which requires a vendor-specific API or programming model. RAJA provides a rich and unified interface for writing both simple and complex numerical kernels that target these execution resources in a portable manner. Production scientific applications are typically characterized by their long lives and large code bases. Such applications often need to run efficiently on multiple architectures at any time, and are under continual development. Plus, they must be viable across multiple generations of HPC platforms. It is often the case that writing and maintaining multiple versions for different architectures is untenable. RAJA eases this transition by allowing iterative adoption with a low barrier to entry, requiring manageable disruption to existing application source code, and enabling systematic performance tuning of different computational kernels. In this talk, we explain the motivation behind RAJA, share simple examples and discuss approaches and benefits of RAJA adoption in production applications at Lawrence Livermore National Laboratory and ECP applications, showing how RAJA can be used to enable high-impact scientific calculations on large-scale HPC systems.</blurb>
    <EventParentName>MS36 Progress and Challenges in Extreme Scale Computing and Big Data - Part II of II</EventParentName>
    <external_id>67710-101564</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Multilevel Hybrid Quantum-Classical Algorithms on Graphs</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS39</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:45PM</starts_at>
    <ends_at>Feb 13 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ruslan Shaydulin</EventSpeakers>
    <EventSpeakerUniqueID>788362</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101656</EventHandoutURL>
    <blurb>Quantum optimization methods have the potential to deliver computational speed-ups over classical state-of-the-art methods. However, in the near term the size of the problems that can be directly tackled using these quantum solvers is constrained by the size of NISQ hardware, which is expected to remain limited. To address these limitations, we propose a family of methods that utilize quantum optimization solvers within a problem decomposition scheme. We present Quantum Local Search (QLS) that integrates quantum optimization solvers into an iterative improvement scheme. We extend this approach to multilevel paradigm (ML-QLS) and demonstrate its potential on Graph Partitioning and Network Community Detection problems.</blurb>
    <EventParentName>MS39 Recent Advances and Trends in Hybrid Quantum-Classical Algorithms - Part II of II</EventParentName>
    <external_id>67763-101656</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>FEniCSX: A Sustainable Future for the FEniCS Project</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS42</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:45AM</starts_at>
    <ends_at>Feb 14 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Michal Habera</EventSpeakers>
    <EventSpeakerUniqueID>783695</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101783</EventHandoutURL>
    <blurb>The FEniCS Project was founded in 2003, and is open source software for the automatic solution of partial differential equations using the finite element method. Only a few individuals involved in the early days are still active in 2019, and the scientific software landscape has changed immeasurably in that time. Nonetheless, the FEniCS Project is still actively maintained, widely used, and is currently undergoing a complete overhaul in a project we call FEniCSX. In this talk I will highlight four of the most impactful changes with respect to the sustainability of the FEniCS Project:

\begin{itemize}
\item\emph{Formalisation of a governance structure within NumFOCUS.} By joining NumFOCUS, the FEniCS Project has gained access to a huge variety of legal, administrative and fundraising opportunities. 
\item\emph{Sustainable pathways for bringing in new contributors.} Examples include access to the Google Summer of Code programme, which has lead to some of the most significant new features (e.g. complex number support).
\item\emph{The FEniCSX technical redevelopment.} FEniCSX is a complete redevelopment of the FEniCS Project's software components, with a strong focus on simplicity, extensibility and standards compliance.
\item\emph{Reducing time burden on core developers with software as a service.} We used to administer many services e.g. continuous integration, ourselves. Wherever possible, we are now using external third party services.
\end{itemize}







</blurb>
    <EventParentName>MS42 Improving Productivity and Sustainability for Parallel Computing Software - Part I of II</EventParentName>
    <external_id>67772-101783</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Predict-and-Recompute Conjugate Gradient Variants</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS16</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tyler Chen</EventSpeakers>
    <EventSpeakerUniqueID>789938</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101615</EventHandoutURL>
    <blurb>The standard implementation of the conjugate gradient algorithm suffers from communication bottlenecks on parallel architectures, due primarily to the two global reductions required every iteration. In this paper, we introduce several conjugate gradient variants, which decrease the runtime per iteration by overlapping global synchronizations, and in the case of our pipelined variants, matrix vector products. Through the use of a predict-and-recompute scheme, whereby recursively updated quantities are first used as a predictor for their true values and then recomputed exactly at a later point in the iteration, our variants are observed to have convergence properties nearly as good as the standard conjugate gradient problem implementation on every problem we tested. It is also verified experimentally that our variants do indeed reduce runtime per iteration in practice, and that they scale similarly to previously studied communication hiding variants. Finally, because our variants achieve good convergence without the use of any additional input parameters, they have the potential to be used in place of the standard conjugate gradient implementation in a range of applications. </blurb>
    <EventParentName>MS16 High Performance Krylov Subspace Methods: Theory, Implementation, and Application - Part II of III</EventParentName>
    <external_id>67749-101615</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Improving Attainable Accuracy of the Deep Pipelined Conjugate Gradient Algorithm</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS16</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:35PM</starts_at>
    <ends_at>Feb 12 2020  3:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jeffrey Cornelis</EventSpeakers>
    <EventSpeakerUniqueID>785941</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101616</EventHandoutURL>
    <blurb>Conjugate Gradients (CG) is one of the most widely used algorithms for solving linear systems with a symmetric positive definite matrix. The classical formulation due to Hestenes and Stiefel requires two global reduction phases in each iteration, namely for the calculation of the dot-products. On HPC hardware this causes a bottleneck in the parallel scalability of the method. When executing CG on a large number of compute nodes the total runtime is dominated by the time it takes to communicate data. Moreover, during the global communication phase the cores stay idle waiting for the result of the dot-product.

The main idea of the deep pipelined CG algorithm is to loosen data dependencies in the classical formulation of the method by introducing auxiliary variables, such that the cores can continue to perform useful computations during the global communication phase. These auxiliary variables allow us to overlap the global reduction with the computation of l sparse matrix vector products, where l is a parameter of the algorithm called the pipeline length. The way the auxiliary variables are computed has a huge impact on the maximal attainable accuracy of the method. In this talk we present two different ways to recursively calculate the auxiliary variables, equivalent in exact arithmetic, but with significantly different finite precision behaviour. We analyse the round-off error in the recurrence relations and present numerical experiments which illustrate the analysis.

</blurb>
    <EventParentName>MS16 High Performance Krylov Subspace Methods: Theory, Implementation, and Application - Part II of III</EventParentName>
    <external_id>67749-101616</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Implementation and Performance Evaluation of High Performance Conjugate Gradients on Containers</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS16</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:00PM</starts_at>
    <ends_at>Feb 12 2020  4:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Neena Imam</EventSpeakers>
    <EventSpeakerUniqueID>783444</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101617</EventHandoutURL>
    <EventParentName>MS16 High Performance Krylov Subspace Methods: Theory, Implementation, and Application - Part II of III</EventParentName>
    <external_id>67749-101617</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Simulation of Density-Driven Flow in Aquifers with Phreatic Surface</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS19</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:00PM</starts_at>
    <ends_at>Feb 12 2020  4:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Dmitry Logashenko</EventSpeakers>
    <EventSpeakerUniqueID>781098</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101829</EventHandoutURL>
    <blurb>Phreatic surface is a typical element of subsurface groundwater flow. It influences the dynamics of the liquid phase significantly and should not be neglected in the modeling. In particular, tracking of its motion is very important in simulations of the density-driven groundwater flow which is very sensitive to the problem setting.

We present an approach for the simulation of the density-driven flow in aquifers with complicated layered geometry whose hydrogeological structure approximates real-world geological formations. The phreatic surface is modeled as a moving boundary of the saturated part of the domain. To resolve the geological layers by the discretization grid,
we represent this surface by the level-set approach and discretize the boundary conditions on it by the ghost-fluid method.

For the numerical solution of the model, we use a vertex-centered finite-volume discretization. The discretized equations are linearized in the Newton's method, in which the geometric multigrid solver with the ILU smoothing is used. The implementation is parallelized. Examples of the simulations are presented.
</blurb>
    <EventParentName>MS19 Parallel Adaptive Multigrid - Part II of II</EventParentName>
    <external_id>67819-101829</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Recent Half Precision Developments in the MAGMA Library</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS21</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ahmad Abdelfattah</EventSpeakers>
    <EventSpeakerUniqueID>773150</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101538</EventHandoutURL>
    <blurb>The huge advancements in machine learning algorithms and artificial intelligence applications have developed a significant demand for high performance low-precision arithmetic. This resulted in vendors providing hardware that is capable of performing native low-precision operations, such as the 16-bit floating point arithmetic (i.e. half precision) in modern GPUs. However, it has been shown that half precision can still be useful in other domains, such as mixed precision linear solvers. In this talk, we present the latest developments in the MAGMA library (Matrix Algebra for GPUs and Multicore Architectures) for dense linear solvers with mixed precision arithmetic and iterative refinement algorithms. We show that, under certain conditions, it is possible to exploit the high performance of half precision in solving linear systems, while maintaining the accuracy of higher precisions (32-bit or even 64-bit).</blurb>
    <EventParentName>MS21 Advances in Algorithms Exploiting Low Precision Floating-Point Arithmetic - Part II of II</EventParentName>
    <external_id>67713-101538</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Solving Neural ODEs using Fixed-Point Arithmetic with Stochastic Rounding</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS21</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:20AM</starts_at>
    <ends_at>Feb 13 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mantas Mikaitis</EventSpeakers>
    <EventSpeakerUniqueID>789947</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101539</EventHandoutURL>
    <blurb>Increasingly more processors are designed without hardware support for floating-point arithmetic with an expectation that either fixed-point arithmetic will be enough for intended applications or that software floating-point will be feasible.
In some specialized systems, even fixed-point support is highly limited due to reduced precision (8- or 16-bit) integer arithmetic units. The main reasons for doing this are potential improvements in energy efficiency and memory footprint and bandwidth. However, simply switching to low-precision data types and operations typically results in increased numerical errors. We investigate approaches to improving the accuracy of fixed-point arithmetic types, using examples in an important domain for numerical computation in neuroscience: the solution of Ordinary Differential Equations (ODEs). The Izhikevich neuron model is used to demonstrate that rounding has an important role in producing accurate spike timings from explicit ODE solution algorithms. In particular, fixed-point arithmetic with stochastic rounding consistently results in smaller errors compared to single-precision floating-point and fixed-point arithmetic with round-to-nearest across a range of neuron configurations and ODE solvers. The experiments were run on the SpiNNaker chip, an 18-core (ARM968) 32-bit integer processor.

</blurb>
    <EventParentName>MS21 Advances in Algorithms Exploiting Low Precision Floating-Point Arithmetic - Part II of II</EventParentName>
    <external_id>67713-101539</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Extending Quantum Molecular Dynamics to the Exascale: Latte, Progress and BML Libraries</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS22</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:45AM</starts_at>
    <ends_at>Feb 13 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>4000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christian Negre</EventSpeakers>
    <EventSpeakerUniqueID>779799</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101801</EventHandoutURL>
    <blurb>We have developed a modern framework to extend Quantum Molecular Dynamics (QMD) simulation to the exascale. The main deliverable consists of a flexible library ecosystem for Quantum Chemistry Applications adapted to pre-exascale architectures. This framework is composed by two libraries: BML which is a low-level API for linear algebra or mathematically related operations, and PROGRESS which consists of a collection of O(N) electronic structure solvers that rely entirely on BML. PROGRESS implements an electronic structure solver based on a recursive Fermi operator expansion (SP2) that exploits the underlying matrix sparsity. In order to achieve distributed memory parallelism, a graph-based approach where the calculations are distributed across multiple nodes following a predicted data dependency graph has been implemented. By combining the two approaches, practical QMD simulations of million atom biomolecular systems are within reach in the very near future.
</blurb>
    <EventParentName>MS22 Parallel Processing for Particle Codes - Part I of II</EventParentName>
    <external_id>67814-101801</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Optimizing Molecular Dynamics Simulations with Dynamic Auto-Tuning</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS22</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 12:10PM</starts_at>
    <ends_at>Feb 13 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>4000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Fabio Gratl</EventSpeakers>
    <EventSpeakerUniqueID>784894</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101813</EventHandoutURL>
    <blurb>Molecular Dynamics simulations are highly diverse by nature and require different optimization strategies depending on the scenario. For example, the simulation of a homogeneous gas poses different challenges than droplets. To make matters worse this optimum can change over the course of a simulation if said gas begins to form droplets.

To tackle this challenge we published the C++ library project AutoPas, which acts as a node-level performance library for arbitrary N-Body simulations. It implements multiple types of particle containers, parallelization strategies, data layouts, and further optimization techniques. During runtime AutoPas then chooses the fastest combination of aforementioned aspects for an optimal time to solution of the pairwise force calculation in the system. Throughout the simulation, the library periodically reevaluates the current state of the simulation and is capable to adapt all of its internal algorithm configurations to sustain optimal performance for the whole duration of the simulation.

In this talk, we present recent optimization options and parallelization strategies implemented in the library and their impact while integrated into the simulation software ls1-mardyn. The introduction of even more parameters and choices inevitably increases the search space for the auto-tuning process. Therefore techniques are discussed how to efficiently find a near-optimal configuration and first results are presented.
</blurb>
    <EventParentName>MS22 Parallel Processing for Particle Codes - Part I of II</EventParentName>
    <external_id>67814-101813</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Communication-Avoiding Sparse Direct Solvers for Linear Systems \&amp; Graph Problems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS24</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:20AM</starts_at>
    <ends_at>Feb 13 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Piyush Sao</EventSpeakers>
    <EventSpeakerUniqueID>790070</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101942</EventHandoutURL>
    <blurb>Graph shortest path problems are min-plus semiring equivalent of solving a system of linear equations.  In this talk, we show how to exploit graph sparsity in the  \textsc{Floyd-Warshall}  (\textsc{Fw}) algorithm for the all-pairs shortest path (\textsc{Apsp}) problem. (\textsc{Fw}) is an attractive choice for \textsc{Apsp} on high-performing systems due to its structural similarity to solving dense linear systems and matrix multiplication. However, if sparsity of the input graph is not properly exploited,  (\textsc{Fw}) will perform unnecessary asymptotic work and thus may not be a suitable choice for many input graphs. To overcome this limitation, the key idea in our approach is to use the known algebraic relationship between \textsc{Fw} and  Gauss-Jordan elimination, and import several algorithmic techniques from sparse Cholesky factorization, namely, fill-in reducing ordering, symbolic analysis, supernodal traversal,  and elimination tree parallelism. When combined, these techniques reduce computation and communication,  improve locality and enhance parallelism. We implement these ideas in an efficient shared memory  parallel prototype that is orders of magnitude faster than an efficient multi-threaded baseline (\textsc{Fw}) that does not exploit sparsity. Our experiments suggest that (\textsc{Fw}) algorithm can be  competitive with Dijkstra's algorithm (the algorithmic core of Johnson's algorithm) for several classes sparse graphs.
</blurb>
    <EventParentName>MS24 Parallel Matrix Factorization Algorithms - Part I of III</EventParentName>
    <external_id>67847-101942</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallel Direct Matrix Factorization Methods for Dynamic Optimization</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS24</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:45AM</starts_at>
    <ends_at>Feb 13 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Samah Karim</EventSpeakers>
    <EventSpeakerUniqueID>790068</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101943</EventHandoutURL>
    <blurb>Dynamic optimization problems are a special class of optimization problems that involve decision or control variables, and environment or state variables which vary with time. Their goal is to minimize/maximize some cost functional which is a function of these variables, subject to a set of differential and algebraic constraints. The most common approach to solve these problems is a direct control transcription that replaces the differential equations with discretized approximations, hence transforming the dynamic optimization problem into a nonlinear programming problem (NLP).  That NLP can then be solved with a dedicated NLP solver which more often than not utilizes Newton’s method to compute the step at every iteration. Furthermore Newton’s method at its core involves the solution of a symmetric indefinite linear system stemming from the Karush–Kuhn–Tucker 
(KKT) conditions. And for problems with a large number of variables, solving the resulting large sparse linear systems is the most expensive computational part. Thus it becomes crucial to use a parallel sparse direct solver that can take advantage of the specific sparsity patterns of these KKT systems. 


</blurb>
    <EventParentName>MS24 Parallel Matrix Factorization Algorithms - Part I of III</EventParentName>
    <external_id>67847-101943</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Exascale Node-Level Parallel Programming Environments: Overview and Deciding What's Right for You</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS25</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Michael Heroux</EventSpeakers>
    <EventSpeakerUniqueID>706361</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101559</EventHandoutURL>
    <blurb>Accelerated architectures have been around in some form or another for decades.  For example, Control Data and Cray vector systems were available nearly 40 years ago.  In the recent past, Nvidia GPUs have been the most prominent accelerated devices, and now represent a mature software and hardware ecosystem that reliably delivers performance across a broad set of applications.

In the coming phase of high-performance computing, accelerated architectures will represent a greater diversity of platforms. Vector processors based on Arm and its scalable vector extensions (SVE) represent a new approach to vectorization requiring significant algorithmic concurrency.  In addition, new GPU accelerators are emerging which, while similar in concurrency strategy as Nvidia, represent new software stacks and important variations in processor details.  The advent of these new platforms means that performance portability becomes increasingly important.

In this presentation, we give a brief overview of publicly-available information about new highly-concurrent processors (vector and GPU) and discuss the programming model and environment options available to programmers, along with some guidance to help programmers make good choices.</blurb>
    <EventParentName>MS25 Progress and Challenges in Extreme Scale Computing and Big Data - Part I of II</EventParentName>
    <external_id>67709-101559</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Polynomial Preconditioned GMRES in Trilinos: Practical Considerations for High-Performance Computing</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS26</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:45AM</starts_at>
    <ends_at>Feb 13 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jennifer Loe</EventSpeakers>
    <EventSpeakerUniqueID>775808</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101621</EventHandoutURL>
    <blurb>Polynomial preconditioners for Krylov solvers are well-known but are not normally used in large-scale software libraries or applications. This may be due to stability problems or complicated algorithms, especially for nonsymmetric matrices. We implement a new GMRES polynomial preconditioner in the software library Trilinos and demonstrate that it is stable and effective for parallel computing. We give several examples with GMRES and compose the polynomial with other preconditioners such as ILU($k$) and algebraic multigrid. Trade-offs when selecting a polynomial degree and combining with other preconditioners are analyzed. We also discuss communication-avoiding properties of the polynomial preconditioner.  



</blurb>
    <EventParentName>MS26 High Performance Krylov Subspace Methods: Theory, Implementation, and Application - Part III of III</EventParentName>
    <external_id>67750-101621</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Improving the Performance of GMRES with Mixed Precision</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS26</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 12:10PM</starts_at>
    <ends_at>Feb 13 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Neil Lindquist</EventSpeakers>
    <EventSpeakerUniqueID>789995</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101632</EventHandoutURL>
    <blurb>Krylov solvers, such as GMRES, are often memory bound on modern systems due to the gap in improvement between arithmetic performance and memory access performance. Reducing the precision of the preconditioner is a known approach to improve performance by reducing the pressure on the memory bandwidth. However, this idea has not been extended to use lower precision for other parts of the solver. By judiciously storing vectors in single precision, we are able to achieve the accuracy of a fully double precision GMRES implementation while reducing the memory traffic and footprint. This approach for mixed precision provides an avenue for improving the performance of GMRES, and possibly other solvers, without compromising the accuracy of the solution.</blurb>
    <EventParentName>MS26 High Performance Krylov Subspace Methods: Theory, Implementation, and Application - Part III of III</EventParentName>
    <external_id>67750-101632</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The Validation of the Simulated HIRF Effects in Metallic Cases using Parallel FDTD Solver on a High Performance Computer</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP1</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:25PM</starts_at>
    <ends_at>Feb 12 2020  1:45PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yueqian Wu</EventSpeakers>
    <EventSpeakerUniqueID>790185</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102035</EventHandoutURL>
    <blurb>To accurately predict High Intensity Radiated Field (HIRF) effects in various systems is highly required for most commercial as well as defense applications, which is mainly based on electromagnetic simulation. Further, the validation of the simulated HIRF effects at different levels is also required. However, it is often difficult and needs some appropriate techniques and facilities. Here, in-house developed parallel Finite-Difference Time-Domain (FDTD) solver is at first employed for simulating the HIRF effects of two metallic cases with multiple slots and apertures, which is carried out on a high performance computer with tens of thousands of processors. The uncertainty analysis of the incident plane wave angle related to the electric field distribution in the metallic cases is finished with the help of non-intrusive polynomial chaos (NIPC) method, and the feature selective validation (FSV) is performed to quantify the difference between the measured and simulated field results. It is shown that the results obtained from the FSV method provide high reliability and confidence for our numerical characterization of HIRF effects in the metallic cases and even other complex structures.

	</blurb>
    <EventParentName>CP1 UQ and Stochastic Processes</EventParentName>
    <external_id>68191-102035</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Large-Scale DPG Finite Element Simulation of a Nonlinear Multiphysics Fiber Amplifier Model</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP3</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  3:30PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Stefan Henneking</EventSpeakers>
    <EventSpeakerUniqueID>767468</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101984</EventHandoutURL>
    <blurb>Multi-mode fiber lasers in high-power operation suffer from undesired thermal coupling effects such as the transverse mode instability (TMI). The TMI is a major obstacle in power-scaling of large mode area, active gain, fiber amplifiers. A better understanding of these nonlinear coupling effects is beneficial in the design of new fibers. We present details on the implementation and numerical results for a high-fidelity fiber amplifier model. This model is based on the 3D vectorial time-harmonic Maxwell equations for two weakly coupled electromagnetic fields. Thermal effects are modeled via coupling with the heat equation. The high-frequency nature of the wave propagation problem requires the use of high-order discretizations to effectively counter numerical pollution. The discontinuous Petrov-Galerkin (DPG) finite element method provides a stable discretization with a built-in error indicator suitable for hp-adaptivity. For simulating a significant fiber length of more than one thousand wavelengths, a scalable parallel implementation is critical. For this, we have developed an MPI/OpenMP version of an hp-adaptive finite element software that supports high-order discretizations for complex multiphysics problems with variables of the entire H$^1$-H(curl)-H(div)-L$^2$ exact sequence, hybrid meshes with elements of all shapes, and anisotropic hp-refinements. We show scalability results, in the context of this particular fiber laser application, for modern manycore architectures.</blurb>
    <EventParentName>CP3 Application - Part I of III</EventParentName>
    <external_id>68193-101984</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>General Memory-Independent Lower Bound for MTTKRP</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP4</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 11:15AM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kathryn Rouse</EventSpeakers>
    <EventSpeakerUniqueID>779288</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104196</EventHandoutURL>
    <blurb>Our goal is to establish lower bounds on the communication required to perform the Matricized-Tensor Times Khatri-Rao Product (MTTKRP) computation on a distributed-memory parallel machine. MTTKRP is the bottleneck computation within algorithms for computing the CP tensor decomposition, which is an approximation by a sum of rank-one tensors and frequently used in multidimensional data analysis. The main result of this paper is a communication lower bound that generalizes previous results, tightening the bound so that it is attainable even when the tensor dimensions vary (the tensor is not cubical) and when the number of processors is small relative to the tensor dimensions. The attainability of the bound proves that the algorithm that attains it, which is based on a block distribution of the tensor and communicating only factor matrices, is communication optimal. The proof technique utilizes an established inequality that relates computations to data access as well as a novel approach based on convex optimization.</blurb>
    <EventParentName>CP4 Proceedings Papers - Part I of III</EventParentName>
    <external_id>68613-104196</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Tensor Decomposition for Malware Detection</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS55</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jee Choi</EventSpeakers>
    <EventSpeakerUniqueID>760044</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101636</EventHandoutURL>
    <blurb>In this talk, we will discuss our experience in applying tensor decomposition for malware detection. We compare using n-gram and information gain with SVM to using n-gram and other features with information gain and then applying tensor decomposition as an unsupervised method. We show that tensor decomposition demonstrates comparable performance to using SVM, and may be usable with much fewer labeled data compared traditional ML methods.</blurb>
    <EventParentName>MS55 High-Performance Tensor Computation and Applications - Part I of III</EventParentName>
    <external_id>67756-101636</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Numerical Method and Parallelization for the Computation of Coherent Synchrotron Radiation</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP5</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:10PM</starts_at>
    <ends_at>Feb 13 2020  4:30PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Boqian Shen</EventSpeakers>
    <EventSpeakerUniqueID>789996</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101634</EventHandoutURL>
    <blurb>The purpose of this work is to develop and parallelize an accurate and efficient numerical method for the computation of synchrotron radiation from relativistic electrons in the near field. The high-brilliance electron beam and coherent short-wavelength light source provides a powerful method to understand the microscopic structure and dynamics of materials. Such a method supports a wide range of applications including matter physics, structural biology, and medicine development. To understand the interaction between the beam and synchrotron radiation an accurate and efficient numerical simulation is needed. With millions of electrons, the computational cost of the field would be large. Thus, multilevel parallelism and performance portability are desired since modern supercomputers are getting more complex and heterogeneous. </blurb>
    <EventParentName>CP5 Application - Part II of III</EventParentName>
    <external_id>68194-101634</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Scalable Implementation of Parallel Additive Multigrid with Scaled Correction</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP6</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  3:40PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Syam Vangara</EventSpeakers>
    <EventSpeakerUniqueID>785760</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102052</EventHandoutURL>
    <blurb>Multigrid methods have proven to be efficient and scalable solvers in large-scale scientific computing applications. However, in most parallel implementations, communication-to-computation ratio on coarse grids increases and the number of messages grows with the number of grids in the multigrid solver. Also, for Galerkin coarsening, number of non-zeroes per row in the coarse grid matrix operator increases leading to increased number of neighboring processes. These communication complexities can lead to significant degradation of performance and scalability of multigrid methods. Whereas, additive variants of multigrid methods allow for overlap of communication and computation as well as reduction in the number of messages, thus increasing parallel performance. Additive variants of multigrid such as filtered additive multigrid and additive multigrid with scaled correction provide convergence rates faster than that of classical additive multigrid and slightly slower than that of multiplicative multigrid. In this talk, we present a scalable implementation of additive multigrid with scaled correction that mitigates the effects of communication complexity by overlapping communication and computation across grids and reducing number of messages. Its performance is compared with multiplicative multigrid and other variants of additive multigrid.</blurb>
    <EventParentName>CP6 Multigrid and Preconditioning</EventParentName>
    <external_id>68195-102052</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Two-Level Dynamic Load Balancing for High Performance Scientific Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP7</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:10PM</starts_at>
    <ends_at>Feb 13 2020  4:30PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ali Mohammed</EventSpeakers>
    <EventSpeakerUniqueID>790010</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104188</EventHandoutURL>
    <blurb>Scientific applications are often complex, irregular, and computationally-intensive. To accommodate their ever-increasing computational demands, the high performance computing (HPC) systems have become larger and more complex, offering increased hardware parallelism at multiple levels. Scientific applications need to exploit all multilevel hardware parallelism to harness the available computational power. The performance of applications executing on such HPC systems may adversely be affected by load imbalance at multiple levels, caused by problem, algorithmic, and systemic characteristics. Existing dynamic load balancing methods do not simultaneously address load imbalance at multiple software parallelism levels. This work investigates the joint impact of load imbalance on the performance of three scientific applications at the thread and process levels. We jointly apply and evaluate selected dynamic loop self-scheduling (DLS) techniques to both levels. We conduct an exhaustive set of experiments to assess and compare the combination of six DLS techniques at the thread level and eleven at the process level. The results show that improved overall application performance, by up to 21%, can only be achieved by jointly addressing load imbalance at both software parallelism levels. We offer insights into the performance of the selected DLS techniques and discuss the interplay of load balancing at the thread and process levels.</blurb>
    <EventParentName>CP7 Proceedings Papers - Part II of III</EventParentName>
    <external_id>68614-104188</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Projection-Based Data Partitioning Method for Distributed Tomographic Reconstruction</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP7</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:45PM</starts_at>
    <ends_at>Feb 13 2020  4:05PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jan-Willem Buurlage</EventSpeakers>
    <EventSpeakerUniqueID>780457</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104190</EventHandoutURL>
    <blurb>Tomography is a non-destructive technique for imaging the interior of a 3D object. We present an efficient data partitioning strategy for distributed tomographic reconstruction algorithms. Our novel partitioning method is a refinement of the previously published GRCB algorithm. Instead of taking as input a discrete set of lines corresponding to source--pixel pairs, the introduced algorithm works directly on the (cone-shaped) projections. We introduce a geometric characterization of communication volume, as well as a continuous model for load-balancing based on the varying line densities throughout the object volume. The resulting algorithm is orders of magnitude faster than the original algorithm while producing partitionings of similar quality. We introduce a novel communication data structure that can efficiently represent the communication metadata. An implementation on top of Bulk and the ASTRA toolbox is discussed. We provide experimental results of our method for various commonly used acquisition geometries. We achieve a speedup of 2.8x compared to ASTRA-MPI when using 32 GPUs to reconstruct an image for a circular-cone beam acquisition geometry.</blurb>
    <EventParentName>CP7 Proceedings Papers - Part II of III</EventParentName>
    <external_id>68614-104190</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>FastSV: A Distributed-Memory Connected Component Algorithm with Fast Convergence</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP7</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  3:40PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yongzhe Zhang</EventSpeakers>
    <EventSpeakerUniqueID>791896</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104193</EventHandoutURL>
    <blurb>This paper presents a new distributed-memory algorithm called FastSV for finding connected components in an undirected graph. Our algorithm simplifies the classic Shiloach-Vishkin algorithm and employs several novel and efficient hooking strategies for faster convergence. We map different steps of FastSV to linear algebraic operations and implement them with the help of scalable graph libraries. FastSV uses sparse operations to avoid redundant work and optimized MPI communication to avoid bottlenecks. The resultant algorithm shows high-performance and scalability as it can find the connected components of a hyperlink graph with over 134B edges in 30 seconds using 262K cores on a Cray XC40 supercomputer. FastSV outperforms the state-of-the-art algorithm by an average speedup of 2.21x (max 4.27x) on a variety of real-world graphs.
</blurb>
    <EventParentName>CP7 Proceedings Papers - Part II of III</EventParentName>
    <external_id>68614-104193</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Efficient Vectorised Cuda Kernels for High-Order Finite Element Flow Solvers</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP8</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 11:15AM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jan Eichstaedt</EventSpeakers>
    <EventSpeakerUniqueID>775730</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102027</EventHandoutURL>
    <blurb>Modern shared-memory systems like GPGPUs offer massively parallel compute power, while the gap between available FLOPS and memory bandwidth increases. This demands for parallel schemes with high arithmetic intensity, such as high-order finite element methods.
In this work, we optimise the performance of elemental operators for matrix-free Newton-Krylov solvers. We consider triangular, quadrilateral, tetrahedral, prismatic and hexahedral elements from curved unstructured high-order meshes to maintain geometric flexibility for engineering applications. The tensor-product bases of all these spectral elements further allows the exploitation of the sum-factorisation approach that reduces the number of operations. We implement efficient CUDA kernels for all operators of advection-diffusion and Helmholtz equations for a range of polynomial orders. 
We place special emphasis on efficient data-structures by interleaving the data of a group of 32 elements in order to achieve contiguous memory access for the vectorised operations within a CUDA warp.
We further investigate the benefit of increasing the arithmetic intensity by re-computing quadrature metrics and mappings within each kernel, instead of loading pre-computed data from global memory.
We ultimately present the aggregate performance of our kernels in simple 2D and 3D advection-diffusion solvers.</blurb>
    <EventParentName>CP8 Numerical Methods for Flow Simulations</EventParentName>
    <external_id>68196-102027</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>rocFFT: An Open-Source GPU FFT Library for Exascale Systems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS53</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:35PM</starts_at>
    <ends_at>Feb 14 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Malcolm Roberts</EventSpeakers>
    <EventSpeakerUniqueID>790000</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101925</EventHandoutURL>
    <blurb>rocFFT is an open-source software FFT library which is part of AMD's
Radeon Open Compute Platform (ROCm).  rocFFT is written in the HIP
programming language, and designed for distributed computing with
GPU-enabled MPI communications.  In this presentation, I will
introduce rocFFT and talk about how we are adapting the project to
work on exascale systems.</blurb>
    <EventParentName>MS53 Parallel Sparse and Conventional FFTs, Applications and Implementation - Part II of II</EventParentName>
    <external_id>67765-101925</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MaMiCo: Scalable Coupling of Particle Ensembles to Transient Continuum Flow Simulations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP8</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:45AM</starts_at>
    <ends_at>Feb 14 2020 12:05PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Piet Jarmatz</EventSpeakers>
    <EventSpeakerUniqueID>790233</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102058</EventHandoutURL>
    <blurb>Particle systems, such as molecular dynamics (MD) simulations, are an indispensable tool for a variety of scientific applications. However, the computational demand limits MD applicability to rather small spatial setups, even for massively parallel simulations on high-performance computing systems. One approach to extend this applicability to larger simulation domains is given by multiscale coupling schemes which link particle methods to continuum flow simulations.
Advanced coupling methods filter MD data to reduce thermal noise, improve stability and reduce computational cost. We present a new hydrodynamic noise filter based on the non-local means (NLM) algorithm, which was developed for image processing originally, and we compare it to conventional filtering based on Proper Orthogonal Decomposition (POD).
This presentation aims to point out scalable ways to design, implement and execute molecular-continuum coupled systems, with focus on new parallelization paradigms and distributed data filtering and processing. We utilize and extend the Macro-Micro-Coupling tool (MaMiCo), a modular C++ framework for parallel coupling of arbitrary continuum and particle solvers.
Our results suggest that in many cases the NLM noise filter in MaMiCo is able to improve signal-to-noise ratios of fluctuating MD data compared to POD filtering, and thus enables stable coupling on shorter time scales, while having only a small impact on simulation performance.</blurb>
    <EventParentName>CP8 Numerical Methods for Flow Simulations</EventParentName>
    <external_id>68196-102058</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Hybrid Empirical and Simulation-Based Autotuning of a Dense Linear Algebra Library for Heterogeneous Architectures</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP9</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 11:15AM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jesús Cámara</EventSpeakers>
    <EventSpeakerUniqueID>790149</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102024</EventHandoutURL>
    <blurb>Parallel numerical libraries for modern architectures can be finely optimized. However, this optimization process may require to carefully set up many parameters, which can be a pretty cumbersome task when done manually. For instance, modern dense linear algebra libraries often split the matrix in submatrices whose size may significantly impact performance. Similarly, the scheduling policy may be of importance, especially when dealing with heterogeneous architectures. Possibly, it may also be interesting to let computational units idle. In this work, we discuss how to automatically decide the selection of these parameters. The idea is that, during a preliminary phase at install time, well chosen tests are performed, so that, once the library has been installed, these parameters get automatically (and instantly) decided while achieving an overall performance close to optimum. We illustrate our discussion with the Chameleon dense linear algebra library running on top of the StarPU runtime system. We use the SimGrid simulator to predict the performance of time consuming executions (large matrices) and hence reduce the time spent in the autotuning phase.</blurb>
    <EventParentName>CP9 Autotuning</EventParentName>
    <external_id>68197-102024</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Leveraging One-Sided Communication for Sparse Triangular Solvers</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP10</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:20AM</starts_at>
    <ends_at>Feb 14 2020 11:40AM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Samuel Williams</EventSpeakers>
    <EventSpeakerUniqueID>735132</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101995</EventHandoutURL>
    <blurb>In this paper, we implement and evaluate a one-sided communication-based distributed-memory sparse triangular solve (SpTRSV). SpTRSV is used in conjunction with Sparse LU to affect preconditioning in linear solvers while one-sided communication paradigms enjoy higher effective network bandwidth and lower synchronization costs compared to their two-sided counterparts. We use a passive target mode in one-sided communication to implement a synchronization-free task queue to manage the messaging between producer-consumer pairs. Whereas some numerical methods lend themselves to simple performance analysis, the DAG-based computational graph of SpTRSV demands we construct a critical path performance model in order to assess our observed performance relative to machine capabilities. In alignment with our model, our foMPI-based one-sided implementation of SpTRSV reduces communication time by 1.5x to 2.5x and improves SpTRSV solver performance by up to 2.4x compared to the SuperLU\_DIST's two-sided MPI implementation running on 64 to 4,096 processes on Cray supercomputers.
</blurb>
    <EventParentName>CP10 Proceedings Papers - Part III of III</EventParentName>
    <external_id>68615-101995</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Graph Partitioning for Computational Mechanics Simulations with GDSW</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP11</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  3:40PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>David Day</EventSpeakers>
    <EventSpeakerUniqueID>703352</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101987</EventHandoutURL>
    <blurb>In the application of scalable parallel computing algorithms and hardware to computational mechanics problems for aerospace structures, typically the bottleneck is the iterative linear solver.  In the case of the Generalized Dryja, Widlund, Schmidt (GDSW) overlapping Schwarz domain decomposition method with different levels of overlap, evidence will be presented that preconditioner quality and effectiveness, and hence overall simulation time, is very sensitive to the graph partition.  Numerical case studies will be presented using graph partitioners that wrap the Zoltan, ParMetis and Chaco graph partitioners with a layer adding application specific geometric and topological information.  Overall simulation time will be shown to best correlate with the load imbalance of the subdomain overlaps.  This observation will be shown to be consistent with a rudimentary complexity analysis of GDSW.  Modifications of the graph partitioner wrapper will be shown to significantly reduce simulation time by incrementally improving the overlap load balance.

	</blurb>
    <EventParentName>CP11 Applications - Part III of III</EventParentName>
    <external_id>68198-101987</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Training and Best Practices to Develop Portable Yet Performant Code</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS42</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 12:10PM</starts_at>
    <ends_at>Feb 14 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sunita Chandrasekaran</EventSpeakers>
    <EventSpeakerUniqueID>747239</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101784</EventHandoutURL>
    <blurb>As architectures evolve, it is only becoming increasingly important to develop performance portable software. Such diverse architectures require their own code optimization strategies, while the application developers prefer a “write-once” code development strategy in which a single code will execute efficiently on all targeted architectures. In addition to considering the underlying hardware, a software solution---or let's call it a programming mode---is also expected to address requirements of applications and their algorithms. The programming language that implements the model should provide the right abstractions to improve the productivity of scientific developers. 
\linebreak

Programmers often resort to a trade-off between achieving portability and high performance. Why? The issue is two-fold. Adequate application parallelism will not be exposed to the hardware architecture if the algorithm is structured in a way that limits the level of concurrency that a programming model can benefit from. Secondly, such a single code representation is possible only if the programming abstractions are carefully crafted for the programming models to provide informative hints to the compilers to generate optimized code across platforms.
\linebreak

This talk will discuss this art of developing a single source code base that demonstrates the best performance and at the same time does not lose portability.



</blurb>
    <EventParentName>MS42 Improving Productivity and Sustainability for Parallel Computing Software - Part I of II</EventParentName>
    <external_id>67772-101784</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Implementation of Parallel 3-D Real FFT with 2-D Decomposition on Intel Xeon Phi Clusters</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS43</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 12:10PM</starts_at>
    <ends_at>Feb 14 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daisuke Takahashi</EventSpeakers>
    <EventSpeakerUniqueID>83250</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101659</EventHandoutURL>
    <blurb>In this talk, we propose an implementation of a parallel 3-D real fast Fourier transform (FFT) with 2-D decomposition on Intel Xeon Phi clusters. The proposed implementation of the parallel 3-D real FFT is based on the conjugate symmetry property of the discrete Fourier transform (DFT) and the row-column FFT algorithm.
We vectorized FFT kernels using the Intel Advanced Vector Extensions 512 (Intel AVX-512) instructions.
Performance results of parallel 3-D real FFTs on Intel Xeon Phi clusters are reported. We successfully achieved a level of performance over 10 TFlops on 2048 nodes of Fujitsu PRIMERGY CX1640 M1 cluster for an $8192^3$-point FFT.</blurb>
    <EventParentName>MS43 Parallel Sparse and Conventional FFTs, Applications and Implementation - Part I of II</EventParentName>
    <external_id>67764-101659</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Fine-Grained Parallel Incomplete Factorizations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS44</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Edmond Chow</EventSpeakers>
    <EventSpeakerUniqueID>707106</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101948</EventHandoutURL>
    <blurb>In the fine-grained parallel method for computing incomplete
factorizations, a small number of highly parallel fixed-point iterations
are used.  We present some recent developments that improve
the generality and robustness of the method.  We also discuss
issues in using a parallel iterative method for solving with the
sparse triangular factors.</blurb>
    <EventParentName>MS44 Parallel Matrix Factorization Algorithms - Part III of III</EventParentName>
    <external_id>67849-101948</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>PartILUT - a Parallel Threshold Incomplete Factorization Preconditioner for Multicore and GPU</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS44</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 12:10PM</starts_at>
    <ends_at>Feb 14 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hartwig Anzt</EventSpeakers>
    <EventSpeakerUniqueID>763986</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101949</EventHandoutURL>
    <blurb>The explosion of hardware-parallelism inside a single node asks for a shift in the programming paradigms and disruptively-different algorithm designs that allow to exploit the compute power available in new hardware technology. We propose a parallel algorithm for computing a threshold incomplete LU (ILU) factorization. The main idea is to interleave an element-parallel fixed-point iteration that approximates an incomplete factorization for a given sparsity pattern with a procedure that adjusts the pattern to the problem characteristics. We describe and test a strategy for identifying nonzeros to be added and nonzeros to be removed from the sparsity pattern. The resulting pattern may be different and more effective than that of existing threshold ILU algorithms. Also in contrast to other parallel threshold ILU algorithms, much of the new algorithm has fine-grained parallelism. Most notably, it is the first GPU-capable algorithm for computing a threshold ILU.</blurb>
    <EventParentName>MS44 Parallel Matrix Factorization Algorithms - Part III of III</EventParentName>
    <external_id>67849-101949</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Scalable Framework for Simulating Particle Collisions at the Energy Frontier</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS45</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:45AM</starts_at>
    <ends_at>Feb 14 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Stefan Hoeche</EventSpeakers>
    <EventSpeakerUniqueID>790193</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102037</EventHandoutURL>
    <blurb>Computer simulations of high-energy particle collisions exhibit an exponential scaling with the number of observed objects in the final state. Applications to experiments like ATLAS and CMS at the Large Hadron Collider (LHC) are presently constrained by the limited computing resources on the worldwide LHC computing grid. This talk discusses a novel simulation framework, designed for computations at scale on DOE supercomputing facilities. We exemplify how these simulations can be used to obtain predictions for Drell-Yan type processes, which constitute the most relevant irreducible backgrounds in searches for phenomena beyond the Standard Model of particle physics.
</blurb>
    <EventParentName>MS45 High Performance Computing in Scientific Applications</EventParentName>
    <external_id>67767-102037</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Multigrid Reduction in Time for High-Order Discretizations of Hyperbolic Problems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS47</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:20AM</starts_at>
    <ends_at>Feb 14 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Robert Falgout</EventSpeakers>
    <EventSpeakerUniqueID>708034</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101770</EventHandoutURL>
    <blurb>Because computer clock speeds are no longer increasing, the sequential time marching approach used in science simulation codes is becoming a bottleneck. Parallel time integration is a way of creating concurrency in a simulation that can be exploited to remove this bottleneck and provide speed ups, sometimes dramatic. The multigrid reduction in time (MGRIT) approach applies existing knowledge and expertise in parallel spatial multigrid methods to the time dimension. The MGRIT method is designed to be as non-intrusive as possible and to take advantage of existing simulation codes and techniques as much as possible. This has worked well for parabolic equations, but parallel-in-time methods for advection-dominated or purely hyperbolic problems has proven to be difficult to develop.
In this talk, we will present progress on the development of a non-standard coarse grid operator for MGRIT that has been demonstrated to produce scalable results for scalar advection problems discretized with either explicit or implicit high-order (up to order 5) Runge-Kutta methods. Our initial proof-of-principle approach computes coarse-grid operators by solving a least-squares problem based on existing MGRIT theory. We will also present newer more practical approaches under development and discuss various insights we have learned for solving hyperbolic problems parallel in time.

</blurb>
    <EventParentName>MS47 Parallel-in-Time Integration Methods - Part I of II</EventParentName>
    <external_id>67796-101770</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallelizing Exponential Integrators with PFASST</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS47</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:45AM</starts_at>
    <ends_at>Feb 14 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Michael Minion</EventSpeakers>
    <EventSpeakerUniqueID>758224</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101771</EventHandoutURL>
    <blurb>

A new approach for parallel-in-time methods combining exponential integrators and the PFASST method will be introduced.
Exponential integrators for ODEs use the matrix exponential operator to integrate a linear part of the equation exactly while nonlinear terms are typically included using an operator splitting approach.  A serial exponential integrator due to Tommaso Buvoli that treats nonlinear terms to arbitrarily high order by an extension of Spectral Deferred Corrections (SDC) will be discussed followed by extensions to multi-level SDC  and PFASST.  Preliminary numerical results on the parallel performance of the PFASST algorithm using exponential SDC sweepers and comparisons with more traditional parallel-in-time methods based on parareal and PFASST will then be presented. 

</blurb>
    <EventParentName>MS47 Parallel-in-Time Integration Methods - Part I of II</EventParentName>
    <external_id>67796-101771</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Exploiting Generic Tiled Algorithms Toward Scalable H-Matrices Factorizations on Top of Runtime Systems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS48</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Rocío Carratalá-Sáez</EventSpeakers>
    <EventSpeakerUniqueID>790189</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101878</EventHandoutURL>
    <blurb>Hierarchical matrices (H-matrices) have become important in applications where accuracy can be reduced to decrease to a logarithmic order both the execution time and memory consumption. It happens for instance when solving Boundary Element Methods (BEM) problems. However the natural hierarchical structure of the H-Matrices makes it more difficult to efficiently parallelize with modern programming paradigm such as task-based implementations. We discuss in this presentation how we can combine, Chameleon, a tiled dense linear algebra software relying on sequential task-based algorithms and runtime systems such as StarPU, and Hmat-oss, a library focused on providing a set of sequential algorithms for H-algebra operations. We will discuss the limitations in terms of H-matrices structure and memory compression that are imposed by the use of a tiled algorithm, and we will show the performance that can be brought by such a generic solution with respect to more advanced implementation fully exploiting the hierarchical data structure.</blurb>
    <EventParentName>MS48 Accelerating Data Sparse Applications on Massively Parallel Systems - Part III of III</EventParentName>
    <external_id>67827-101878</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Hierarchical Techniques for Solvers with Regulated Accuracy</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS48</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 12:10PM</starts_at>
    <ends_at>Feb 14 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Micah Beck</EventSpeakers>
    <EventSpeakerUniqueID>790047</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101881</EventHandoutURL>
    <blurb>By regulating the accuracy of the operations in a parallel solver it is
possible to drastically increase the resulting performance without much
reduction in the quality of the solution.  One such accuracy regulation may
occur is the use of mixed precision while another is a class of approximation
techniques that take advantage of the structure in the originating domain.
When used in the context of modern parallel hardware, these methods allow to
take full advantage of the available resources and increase both the available
performance peak as well as increase the optimality of the communication.</blurb>
    <EventParentName>MS48 Accelerating Data Sparse Applications on Massively Parallel Systems - Part III of III</EventParentName>
    <external_id>67827-101881</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Low-Synchronization Orthogonalization Schemes for S-Step and Pipelined Krylov Solvers in Trilinos</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS50</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mark Hoemmen</EventSpeakers>
    <EventSpeakerUniqueID>735477</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101755</EventHandoutURL>
    <blurb>We investigate two single-reduce orthogonalization schemes for both $s$-step and pipelined GMRES. The first is based on classical Gram Schmidt with reorthogonalization (CGS2), and the second on inverse compact $WY$ modified Gram Schmidt (MGS).
Standard iterated CGS2 requires three global reductions. In standard MGS, the number of global reductions is proportional to the number of vectors against which we are orthogonalizing. In both cases, we can reduce this to a single global reduction,
including reorthogonalization for accuracy.

Our implementation is based on Trilinos software components, and therefore, is portable to different machine architectures with a single code base. We first demonstrate solver performance on the Intel Haswell nodes of the NERSC Cori Supercomputer.  For these experiments, we integrated our solvers into {\tt Nalu-wind}, a computational fluid dynamics application. At each time step, Nalu uses GMRES with a smoothed aggregation algebraic multigrid (SA-AMG) preconditioner to solve a pressure Poisson linear system. By combining $s$-step GMRES with low-synchronization orthogonalization algorithms,
we reduced Nalu's total GMRES solve time by a factor of $1.4\times$.

We also benchmarked the single-reduce orthogonalization schemes on the ORNL Summit supercomputer on both NVIDIA V100 GPUs and IBM Power9 CPUs. 








</blurb>
    <EventParentName>MS50 Advances in Parallel Sparse Linear System Solver Stacks for Exascale - Part I of II</EventParentName>
    <external_id>67788-101755</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Productivity and Sustainability in a Community-Driven Software Ecosystem for Watershed Science</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS52</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>David  Moulton</EventSpeakers>
    <EventSpeakerUniqueID>703852</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101785</EventHandoutURL>
    <blurb>Through its Science Focus Area (SFA) projects the Subsurface and Biogeochemical Research program within the U.S. Department of Energy is tightly integrating observations, experiments, and modeling to advance a systems-level understanding of how watersheds function, and to translate that understanding into advanced science-based models of watershed systems. To broaden the impact of the existing SFAs, the IDEAS-Watersheds project builds on the success of a synergistic family of IDEAS projects initiated in 2014. Specifically, it strives to improve watershed modeling capacity by increasing software development productivity through an agile approach to creating a sustainable, reliable, parallel, software ecosystem with interoperable components.

In this talk we highlight the unique structure of the IDEAS-Watersheds project and describe how it addresses many challenges of software development with interdisciplinary teams. Specifically, it is organized around six Research Activities that develop concrete use cases to drive advances in our parallel software ecosystem. These use cases are designed to balance advancing software design and development practices for parallel architectures with the model and algorithmic development needed to address scientific challenges. In addition, we use a co-funding model to create a team of early career researchers that will be trained in modern software engineering, while acting as liaisons that contribute to shared deliverables.

</blurb>
    <EventParentName>MS52 Improving Productivity and Sustainability for Parallel Computing Software - Part II of II</EventParentName>
    <external_id>67773-101785</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Scalable Space-Time Adaptivity for Simulations of Binary Black Hole Intermediate-Mass-Ratio-Inspirals</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS27</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:45AM</starts_at>
    <ends_at>Feb 13 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hari Sundar</EventSpeakers>
    <EventSpeakerUniqueID>751754</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101686</EventHandoutURL>
    <blurb>We present a highly scalable framework that targets problems of interest to the numerical relativity and broader astrophysics communities. This framework combines a parallel octree-refined adaptive mesh with a wavelet adaptive multiresolution and a physics module to solve the Einstein equations of general relativity. The goal of this work is to perform advanced, massively parallel numerical simulations of intermediate-mass-ratio inspirals of binary black holes with mass ratios on the order of 100:1. These studies will be used to generate waveforms as used in the data analysis of the Laser Interferometer Gravitational-Wave Observatory and to calibrate semi-analytical approximate methods. 
Our framework consists of a distributed memory octree-based adaptive meshing framework in conjunction with a sophisticated code generator from symbolic expressions. In addition, high-levels of adaptivity are also required to ensure scalability when the mass ratios become large. The code generator makes our code portable across different architectures, including SIMD vectorization, OpenMP, and CUDA combined with efficient distributed memory adaptive data-structures. The equations corresponding to the target application are written in symbolic notation, and generators for different architectures can be added independently of the application. 

</blurb>
    <EventParentName>MS27 Challenges in Parallel Adaptive Mesh Refinement - Part I of III</EventParentName>
    <external_id>67770-101686</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Asynchronous Task-Based {AMR} with Distributed Parallel Objects</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS27</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:20AM</starts_at>
    <ends_at>Feb 13 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>James Bordner</EventSpeakers>
    <EventSpeakerUniqueID>719808</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102066</EventHandoutURL>
    <blurb>
Cello is a highly scalable "array-of-octree" based adaptive mesh
refinement (AMR) software framework, implemented using Charm++, an object-oriented message-driven parallel programming system.  Enzo-E, being developed concurrently with Cello, is a branch of the ENZO astrophysics and cosmology application that has been modified to use the Cello scalable AMR framework.  The Cello framework provides a scientific application with mesh adaptivity, data-driven ghost cell refresh, generic field and particle data types, and task-based asynchronous distributed computation on block data.  In this presentation we describe Cello's distributed data structures and asynchronous algorithms, which include a revised buffered refresh scheme, and a recently-implemented domain-decomposition based scalable gravity solver.  We also present parallel scaling results of Enzo-E simulations of cosmological structure formation, including more recent experiments with dynamic load balancing.
	




</blurb>
    <EventParentName>MS27 Challenges in Parallel Adaptive Mesh Refinement - Part I of III</EventParentName>
    <external_id>67770-102066</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Entropically Consistent Models of Geophysical Fluid Dynamics</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS28</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 12:10PM</starts_at>
    <ends_at>Feb 13 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christopher Eldred</EventSpeakers>
    <EventSpeakerUniqueID>774541</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101959</EventHandoutURL>
    <blurb>The dynamics of real fluids are governed by both reversible (entropy-conserving, such advection) and irreversible (entropy-generating, such as dissipation) processes, both of which conserve energy. Underlying this is a geometric structure: a metriplectic formulation, which combines a Poisson bracket for the reversible dynamics with a metric/dissipation bracket for the irreversible dynamics. By discretizing this geometric structure directly using what are known as mimetic discretizations and therefore preserving its essential elements, it is possible to construct numerical models with many desirable properties. This talk will present one possible approach to this in the context of geophysical fluids: the nonhydrostatic fully compressible Euler equations with some typical subgrid turbulence parameterizations. Specifically, we use mimetic Galerkin differences coupled with a second-order, implicit energy-conserving metriplectic integrator. The resulting model conserves mass and energy to machine precision, while the reversible dynamics conserve entropy and the irreversible dynamics (parameterizations) generate it. Results using this model from planar versions of the commonly used DCMIP test suite will be shown. If time permits, there will be some discussion of future work on the extension of these ideas to multicomponent/multiphase fluids (including moisture), more sophisticated turbulence parameterizations and other areas of geophysical fluids/physics more generally.

</blurb>
    <EventParentName>MS28 High-Performance Numerics and Model Development for Geophysical Systems - Part I of II</EventParentName>
    <external_id>67822-101959</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Hybrid Quantum-Classical Approaches to Exact and Approximate Optimization</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS29</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Stuart Hadfield</EventSpeakers>
    <EventSpeakerUniqueID>785304</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101652</EventHandoutURL>
    <blurb>Hybrid algorithms utilizing emerging quantum technologies offer new approaches to hard computational problems. A promising family of quantum heuristics for combinatorial optimization is the quantum approximate optimization algorithm, and more generally, the quantum alternating operator ansatz (QAOA). We show new results that relate the behavior of QAOA to the structure of the underlying cost function. We then discuss how our results may aid in the design of new quantum approaches to optimization, and may be extended to hybrid approaches to other scientific problems such as the electronic structure problem of quantum chemistry. Finally, we will address research challenges for hybrid algorithms with near-term quantum hardware.</blurb>
    <EventParentName>MS29 Recent Advances and Trends in Hybrid Quantum-Classical Algorithms - Part I of II</EventParentName>
    <external_id>67762-101652</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Scaling Up Streaming Tensor Decompositions</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS65</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 11:00AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Shaden Smith</EventSpeakers>
    <EventSpeakerUniqueID>790061</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101641</EventHandoutURL>
    <EventParentName>MS65 High-Performance Tensor Computation and Applications - Part II of III</EventParentName>
    <external_id>67757-101641</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Streaming Cp Tensor Decompositions</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS65</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:30AM</starts_at>
    <ends_at>Feb 15 2020 11:50AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tamara Kolda</EventSpeakers>
    <EventSpeakerUniqueID>701703</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101643</EventHandoutURL>
    <blurb>We talk about ways of creating locality for randomized tensor decomposition methods, including specialized techniques for sampling large-scale sparse tensors.</blurb>
    <EventParentName>MS65 High-Performance Tensor Computation and Applications - Part II of III</EventParentName>
    <external_id>67757-101643</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Evaluation of Programming Models to Address Load Imbalance on Distributed Multi-Core CPUs: A Case Study with Block Low-Rank Factorization</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS30</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:20AM</starts_at>
    <ends_at>Feb 13 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yu Pei</EventSpeakers>
    <EventSpeakerUniqueID>790044</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101871</EventHandoutURL>
    <blurb>To minimize data movement, many parallel applications statically distribute computational tasks among the processes. However, modern simulations often encounters irregular computational tasks. As a result, load imbalance among the processes must be dealt with at the programming level.

One critical application for many domains is the LU factorization of a large dense matrix stored in the Block Low-Rank format. Using the low-rank format can significantly reduce the cost of factorization in many scientific applications, including the boundary element analysis of electrostatic field. However, the partitioning of the matrix based on underlying geometry leads to different sizes of the matrix, thus load imbalance among the processes at each step of factorization.

We use BLR LU factorization as a test case to study the programmability and performance of five different programming approaches: (1) flat MPI, (2) Adaptive MPI  (3) MPI + OpenMP, (4) parameterized task graph, and (5) dynamic task discovery (DTD). The last two versions use a task-based system (PaRSEC) to express the algorithm. We first point out programming features needed to efficiently solve this category of problems, hinting at possible alternatives to the MPI+X programming paradigm. We then evaluate the programmability of the different approaches. Finally, we show the performance result on the Intel Haswell system and analyze the effectiveness of the implementations to address the load imbalance.</blurb>
    <EventParentName>MS30 Accelerating Data Sparse Applications on Massively Parallel Systems - Part I of III</EventParentName>
    <external_id>67825-101871</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Decoupling Structure Analysis in Hierarchical Matrix Approximations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS30</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 12:10PM</starts_at>
    <ends_at>Feb 13 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Maryam Mehri Dehnavi</EventSpeakers>
    <EventSpeakerUniqueID>790045</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101873</EventHandoutURL>
    <blurb>Hierarchical matrix approximations have gained significant traction in the machine learning and scientific community as they exploit available low-rank structures in kernel methods to compress the kernel matrix. The resulting compressed matrix, HMatrix, is used to reduce the computational complexity of operations such as HMatrix-matrix multiplications with tuneable accuracy in an evaluation phase. Existing implementations of HMatrix evaluations do not preserve locality and often lead to unbalanced parallel execution with high synchronization. Also, current solutions require the compression phase to re-execute if the kernel method or the required accuracy change. In this work, we describe MatRox, a framework that uses novel structure analysis strategies,
blocking and coarsen, with code specialization and a storage format to improve locality and create load-balanced parallel tasks for HMatrix-matrix multiplications. Modularization of the matrix compression phase enables the reuse of computations when there are changes to the input accuracy and the kernel function.
</blurb>
    <EventParentName>MS30 Accelerating Data Sparse Applications on Massively Parallel Systems - Part I of III</EventParentName>
    <external_id>67825-101873</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The Design and Implementation of Large-Scale Tensor Decomposition in SPLATT</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS31</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:20AM</starts_at>
    <ends_at>Feb 13 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Shaden Smith</EventSpeakers>
    <EventSpeakerUniqueID>790061</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101919</EventHandoutURL>
    <blurb>Tensor factorization is a powerful technique for analyzing multi-way data and has applications in fields such as cybersecurity, social network analysis, and health analytics. The tensors that arise in these domains are increasingly large, sparse, and high dimensional. The ubiquity of parallel processors and large-scale clusters motivates the development of scalable parallel approaches for sparse tensor computations.

This talk presents an overview on the design of SPLATT, an open source toolkit for sparse tensor factorization that is used by academia, industry, and government. Topics include data structures for sparse tensors and flexible and incorporation of factorization constraints.


</blurb>
    <EventParentName>MS31 Frameworks/Libraries for High-Performance Tensor Computations - Part II of II</EventParentName>
    <external_id>67795-101919</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Transparency and Reproducibility: Case Studies, Formalisms, and Structured Guidance in Scientific Applications at Scale</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS32</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>4000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Michela Taufer</EventSpeakers>
    <EventSpeakerUniqueID>784892</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101751</EventHandoutURL>
    <EventParentName>MS32 Transparency, Reproducibility, Sustainability, and Security: The Four Pillars of the Next Generation Scientific Software Stack</EventParentName>
    <external_id>67792-101751</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The Mission of Cybersecurity in Science: Productivity, Reproducibility, and Trust</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS32</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:35PM</starts_at>
    <ends_at>Feb 13 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>4000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Von Welch</EventSpeakers>
    <EventSpeakerUniqueID>784933</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101754</EventHandoutURL>
    <EventParentName>MS32 Transparency, Reproducibility, Sustainability, and Security: The Four Pillars of the Next Generation Scientific Software Stack</EventParentName>
    <external_id>67792-101754</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Massively Parallel DEM Simulation and Stress Chain Characterization with over Billion Particles</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS33</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>4000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mikito Furuichi</EventSpeakers>
    <EventSpeakerUniqueID>715778</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101804</EventHandoutURL>
    <blurb>The efficient parallel implementation of the Discrete Element Method (DEM) is a still big challenge because it requires complex computations for the data stored at each contact points for pairwise tangential forces. Here, we present the overview of our parallel implementation of the DEM for a large parallel computer system. Our method utilizes the action-reaction law to save memory and halve the arithmetic costs. A load-balancer with the flexible domain decomposition is applied to manage the load-imbalances between MPI procs. The shape of sub-domains is iteratively updated within the framework of an iterative non-linear solver. The parallel scaling test showed good scalabilities up to 2.4 billion particles. Our complex parallel implementation of the code is verified with a reproducibility test. We will also present the challenge for using modern manycore architectures such as GPGPU and PEZY-SCs. To understand the granular mechanisms in the simulation, we also consider new iterative method for large-scale stress chain analysis. These algorithms and code development enable us to perform the real scale granular simulations in various science and engineering fields.



</blurb>
    <EventParentName>MS33 Parallel Processing for Particle Codes - Part II of II</EventParentName>
    <external_id>67815-101804</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Computing Particle Trajectories with Spectral Deferred Corrections</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS33</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:45PM</starts_at>
    <ends_at>Feb 13 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>4000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daniel Ruprecht</EventSpeakers>
    <EventSpeakerUniqueID>748212</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101805</EventHandoutURL>
    <blurb>The Lorentz equations describe movement of charged particles in electric and magnetic fields and are widely used in plasma physics. Arguably the most popular numerical method to solve them is the Boris algorithm, a variant of the Verlet scheme. It is computationally cheap, has good conservation properties but is only second order accurate. The talk will introduce Boris-SDC, a high-order generalization of the Boris algorithm using spectral deferred corrections. Numerical examples will be shown, illustrating performance of Boris-SDC when tracking fast ions in fusion reactors as well as using it for particle pusing in particle-in-cell (PIC) codes.</blurb>
    <EventParentName>MS33 Parallel Processing for Particle Codes - Part II of II</EventParentName>
    <external_id>67815-101805</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallel-in-Time and Aymptotic-Preserving Monte Carlo Methods for Particle-Based Systems in a Diffusive Scaling</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS33</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:10PM</starts_at>
    <ends_at>Feb 13 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>4000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Bert Mortier</EventSpeakers>
    <EventSpeakerUniqueID>791258</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101806</EventHandoutURL>
    <blurb>In many applications, such as plasma edge simulation of a nuclear fusion reactor, a coupled PDE/kinetic description is required, which is usually solved with a coupled finite-volume/Monte-Carlo method. The plasma in a fusion reactor, for instance, can usually be modeled using fluid equations (for mass, momentum and energy). However, the reactor also contains neutral (non-charged) particles (which are important in its operation), of which both the position and velocity distribution is important. This leads to a Boltzmann-type transport equation that needs to be discretized with a Monte Carlo method. One then obtains a coupled finite-volume/Monte-Carlo simulation, of which the results possess both a bias and a variance. In many relevant regimes, the simulation cost is dominated by the high collisionality of the neutral particles, which lead to an unacceptably small time step. However, in the limit of infinite collisionality, a macroscopic fluid equation arises, which is much easier to simulate. In this talk, we discuss how this limiting equation can be used to accelerate convergence, either by creating an asymptotic-preverving particle method base on the limiting fluid equation or by using a parallel-in-time method.</blurb>
    <EventParentName>MS33 Parallel Processing for Particle Codes - Part II of II</EventParentName>
    <external_id>67815-101806</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MueLu's Algorithmic Performance on GPU</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS34</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:45PM</starts_at>
    <ends_at>Feb 13 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Luc Berger-Vergiat</EventSpeakers>
    <EventSpeakerUniqueID>780431</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101893</EventHandoutURL>
    <blurb>With the push of exascale computing, HPC architectures have evolved significantly in the last decade. But the change is not limited to hardware, designing scientific codes to take advantage of the power offered by GPUs is a significant challenge for multiple reasons. At least two come to mind: 1. the hardware is still evolving quickly and with it the run-times or language extensions needed to program this hardware, 2. the amount of parallelism provided by GPU is penalizing data movement and until recently code divergence within computational kernels.

These challenges have created a need for upgraded scientific libraries that implement efficiently computational methods. Trilinos' MueLu package provides scalable distributed memory multigrid algorithms and has recently been rewritten in large parts to utilize GPUs. This talk will provide an overview of the implementation of MueLu's smoothed aggregation and geometric interpolation algorithms on GPU. It will also explain what parts of the code have been identified as performance bottlenecks. A comparison between the performance of multigrid on GPUs and on recent CPUs will be provided. Finally a path forward to eliminate the remaining roadblocks will be outlined.
</blurb>
    <EventParentName>MS34 Advances and Challenges in Solvers on GPGPU-Based High-Performance Computing Architectures - Part II of II</EventParentName>
    <external_id>67834-101893</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>GPU Acceleration of a High-Order Arbitrary Lagrangian-Eulerian Code</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP13</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:40PM</starts_at>
    <ends_at>Feb 15 2020  3:00PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Arturo Vargas</EventSpeakers>
    <EventSpeakerUniqueID>790105</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101982</EventHandoutURL>
    <blurb>With the introduction of advanced architectures such as GPUs, a major effort has been set
forth to develop algorithms and implementations for hydrodynamics codes which can realize performance on next gen computer systems. In this talk, we present our two-fold approach in tailoring Blast, an arbitrary Lagrangian-Eulerian (ALE) multi-material hydrodynamics code developed at LLNL, for advanced architectures. A distinguishing feature of Blast is the choice of high order finite element methods leading to higher arithmetic intensity per data access; a trait favored by modern computing processors.

Our first approach leverages partial assembly techniques and exploits the tensor product structure of quad/hex elements. Partial assembly decomposes global operators into a sequence of operators with cascading scope. Through this approach, the action of global operators only requires values at quadrature points enabling reduced storage and on the fly calculations. Second, to remain portable across computing platforms, our implementation builds on the RAJA programming model and Umpire resource manager developed at LLNL. Together RAJA and Umpire enable maintaining a single source code suitable for heterogenous computing systems.

This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. LLNL-ABS-788139.</blurb>
    <EventParentName>CP13 Accelerating Software with GPUs</EventParentName>
    <external_id>68200-101982</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>GPU-Accelerated Barycentric Treecodes</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP13</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:15PM</starts_at>
    <ends_at>Feb 15 2020  2:35PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Nathan Vaughn</EventSpeakers>
    <EventSpeakerUniqueID>790130</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102014</EventHandoutURL>
    <blurb>This talk presents recent developments in the GPU implementations of the barycentric treecodes. 
Treecodes are fast summation techniques that accelerate the calculation of N-body interactions, for example charged particles interacting via the Coulomb or screened Coulomb kernel.  
They can also be applied to accelerate the discrete sums that arise in Green's function methods.  
Treecodes organize source particles into a hierarchy of clusters, typically an octree, then approximate the interactions between a target particle and clusters of source particles at various depths of the tree, reducing the complexity of the calculation from $O(N^2)$ to $O(N\log N)$.
In particular, the barycentric treecodes use the barycentric form of an interpolating polynomial to approximate the particle-cluster interactions.
This work uses the Lagrange and Hermite interpolating polynomials.
These treecodes are capable of achieving very good accuracy by increasing the interpolation order.
In contrast to previous approaches based on the Taylor expansions, this form of the particle-cluster approximation has a readily parallelizable structure which enables an efficient GPU implementation.
I will discuss the approximation, demonstrate its parallel structure, present the GPU implementation, and show results from various test cases.</blurb>
    <EventParentName>CP13 Accelerating Software with GPUs</EventParentName>
    <external_id>68200-102014</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Scalable Triangle Counting on Distributed-Memory Systems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP14</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:40PM</starts_at>
    <ends_at>Feb 15 2020  3:00PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Seher Acer</EventSpeakers>
    <EventSpeakerUniqueID>790202</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102045</EventHandoutURL>
    <blurb>Triangle counting is a canonical graph-analysis problem arising in various network science applications such as spam detection, link recommendation and dense neighborhood graph discovery. It is also one of the IEEE HPEC Graph Challenge problems. In this talk, we describe a novel, hybrid parallel triangle counting algorithm based on a linear algebra formulation of this problem. In our algorithm, we exploit MPI and Cilk frameworks at the distributed-memory and shared-memory parallelism levels, respectively. For each of these parallelism levels, we use a different partitioning strategy tailored to the specific needs of that level. At the distributed-memory level, we partition the problem among MPI processes using 2D Cartesian block partitioning, which is commonly used for reducing communication overheads. At the shared-memory level, we use 1D block partitioning within each Cartesian block using Cilk programming model, which has been proven to be more efficient than OpenMP in the triangle counting context. Our algorithm demonstrates very good strong scaling behavior in almost all tested graphs. It also achieves the fastest time on the 1.4B edge real-world twitter graph, 3.217 seconds. Compared to the results of the previous years’ champion of the Graph Challenge, we demonstrate a speed up of 2.7x on this twitter graph using 5.6x fewer cores. </blurb>
    <EventParentName>CP14 HPC for Data Science and Large Graphs</EventParentName>
    <external_id>68201-102045</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Prismatic Space-Time Tiling as an Approach to High Efficient Stencil Computations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP15</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:40PM</starts_at>
    <ends_at>Feb 15 2020  3:00PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sergey Khilkov</EventSpeakers>
    <EventSpeakerUniqueID>790256</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102068</EventHandoutURL>
    <blurb>Low operation intensity is the main reason of low efficiency for HPC application. This problem
proved to be quite hard. There are automatic solutions like polyhedral compilers [Bondhugula et al, Automatic Transformations for
Communication-Minimized Parallelization and Communication-Minimized Parallelization and Locality
Optimization in the Polyhedral Model] but they usually show lower performance than state-of-the-art codes. In this work we present
a way to design scalable parallel applications that could utilize all levels of parallel execution for modern computational systems.
Our approach is based on the same ideas as polyhedral optimization. Since we mostly deal with stencil computations and hand-tailor the code
we can reduce the complicated theory to a much simpler one. Our approach also provides a way to quite accurately evaluate application performance
[Levchenko et al, Locally Recursive Non-Locally Asynchronous Algorithms for Stencil Computation]. This fact is employed to find the best tiling 

parameters for computational systems. This approach was successfully applied for different physical problems,
numerical methods and computational systems [Levchenko et al, LRnLA Lattice Boltzmann Method: A Performance Comparison of Implementations on GPU and CPU; Korneev et al, Numerical
simulation of increasing initial perturbations of a bubble in the bubble–shock interaction problem;
Zakirov et al, DiamondTorre Algorithm for High-Performance Wave Modeling]. 
</blurb>
    <EventParentName>CP15 Highly Parallel Algorithms</EventParentName>
    <external_id>68202-102068</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Methods and Models for Reducing Communication</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP5</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  8:30AM</starts_at>
    <ends_at>Feb 15 2020  9:10AM</ends_at>
    <EventFilter>PP20|Invited Speaker</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Luke Olson</EventSpeakers>
    <EventSpeakerUniqueID>724312</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101780</EventHandoutURL>
    <blurb>Parallel communication costs can lead to significant performance limitations in many computational kernels.  From structured operations across grids to sparse matrix computations to global reductions, understanding the roadblocks is critical in achieving the highest performance.  In addition, new architectures and networks are exposing opportunities and challenges in organizing
communication even in the most fundamental routines.  In this talk we survey new approaches for modeling performance and offer several routes to reorganizing computation by increasing locality in a node aware fashion.

As an example, algebraic multigrid methods are becoming robust solvers for a variety of problems.  Yet the strong scaling limitations in the underlying sparse matrix routines can severely hinder performance.  Likewise, a key feature of current architectures is node-level parallelism. Standard approaches to inter-process communication will send data regardless of the locations of send and receive processes. Yet, there are notable differences in the cost of intra- and inter-node communication. In response, communication can be reorganized to take advantage of the less costly intra-node communication, reducing both the number and size of inter-node messages.  We will introduce some new models and new approaches to organizing communication that can significantly speed up solvers in this setting.

</blurb>
    <EventParentName>IP5 Methods and Models for Reducing Communication</EventParentName>
    <external_id>67804-101780</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T08:30:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Why Run Real Benchmarks? Can't I Just Run Linpack to Understand My System?</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS2</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  1:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>William Kramer</EventSpeakers>
    <EventSpeakerUniqueID>786355</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101574</EventHandoutURL>
    <blurb>Understanding the true performance of systems, including complex computational and data analysis systems is increasingly difficult and getting more challenging as we experience a new explosion of architectural choices, development of new and more dynamic algorithms, convergence of Big Data and Extreme Computing and increasingly complex workflows.  

This talk will evaluate the current state of the challenges and approaches for system assessment - both initial and ongoing - that are influencing the HPC marketplace and investments.  The talk will then explore how to improve our performance evaluation and system assessment state of the art and suggest ways to become more effective in understand productivity and achievable performance - not just isolated performance. 
</blurb>
    <EventParentName>MS2 Meaningful Performance Indicators for Scientific Computing</EventParentName>
    <external_id>67722-101574</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>PETSc's Accelerator Model and Algebraic Multigrid Work and Data Placement at Extreme-Scale</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS3</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:50PM</starts_at>
    <ends_at>Feb 12 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mark Adams</EventSpeakers>
    <EventSpeakerUniqueID>735429</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101745</EventHandoutURL>
    <blurb>This talk will discuss several new solvers being developed for fusion particle codes. We will discuss Structure preserving methods for the Vlasov-Maxwell-Fokker-Planck system, new Fokker-Planck collision operator in Landau form optimized for emerging architectures, 3D fluid preconditioners for implicit kinetic methods, and new GPU alagebraic multigird solvers in PETSc at scale on SUMMIT.
</blurb>
    <EventParentName>MS3 Experiences in Developing GPU Support for Department of Energy Math Libraries - Part I of II</EventParentName>
    <external_id>67790-101745</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Drug Response Prediction and Generative Models for Molecules</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS4</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:25PM</starts_at>
    <ends_at>Feb 12 2020  1:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Fangfang Xia</EventSpeakers>
    <EventSpeakerUniqueID>789986</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101624</EventHandoutURL>
    <blurb>Since the early wins in computer vision, deep learning has increasingly been applied to hard problems that have defied previous modeling efforts. This is particularly true in chemistry and drug development where there are dozens of efforts to replace the traditional drug development computational pipelines with machine learning based alternatives. In our work we are applying deep learning to the problem of predicting tumor drug response for both single drugs and drug combinations. We have developed drug response models for cell lines, patient derived xenograft models and orgnoids that are used in preclinical drug development. Our approaches leverage work on attention, weight sharing between closely related runs for accelerated training and active learning for prioritization of experiments. Our goal is a broad set of models that can be used to screen drugs during early stage drug development as well as predicting tumor response for pre-clinical study design. Results to date include response classifications that achieve &gt;92% balanced classification accuracy on a pan-cancer collection of tumor models and broad collection of drugs. Additionally, we present preliminary results in applying machine learning to de novo molecule generation and production of surrogate models for activity screening based on high quality simulation and experimental datasets. 
 
</blurb>
    <EventParentName>MS4 Physical-Based Modeling and Machine Learning for Biological and Environmental Sciences - Part I of II</EventParentName>
    <external_id>67751-101624</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Data-Driven Models of the Mouse Mesoscale Connectome: Network Structure and Functionality</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS4</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:50PM</starts_at>
    <ends_at>Feb 12 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hannah Choi</EventSpeakers>
    <EventSpeakerUniqueID>789987</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101625</EventHandoutURL>
    <blurb>The complex connectivity structure unique to the brain network is believed to underlie its robust and efficient coding capability. The recent development of the structural mouse brain network available at the Allen Mouse Brain Connectivity Atlas, makes it possible to conduct in-depth analyses on connections between structure and computation in the brain network. The recent expansion of the Allen Mouse Brain Connectivity data includes cell-type and layer-specific cortical network, constructed from viral tracing experiments in Cre-transgenic mice. Using this large anatomical connectivity dataset, we developed an unsupervised method to find the hierarchical organization of the mouse cortical-thalamic network, based on the layer-specific connectivity. The implemented method discovers the hierarchy of the mouse brain areas based on their anatomical connectivity patterns, and provides a measure of “hierarchy scores” for different connectomes. The uncovered hierarchy provides insights into the direction of information flows in the mouse brain, which has been less well-defined compared to the primate brain. Furthermore, the newly introduced “global hierarchy score” of the mouse brain network which measures the self-consistency of the obtained hierarchy suggests that the mouse cortical-thalamic network has a relatively shallow but clear hierarchical organization. 
</blurb>
    <EventParentName>MS4 Physical-Based Modeling and Machine Learning for Biological and Environmental Sciences - Part I of II</EventParentName>
    <external_id>67751-101625</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Technical Computer Aided Design Modeling of Semiconductor Devices in Parallel Computing Architectures</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS5</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:50PM</starts_at>
    <ends_at>Feb 12 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Lawrence Musson</EventSpeakers>
    <EventSpeakerUniqueID>790168</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102028</EventHandoutURL>
    <blurb>Sandia National Laboratories’ technical computer aided design (TCAD) device simulator, Charon, is based on a multi-physics code for simulating general transport-reaction phenomena in semiconducting and insulating materials including the effects of heat generation and radiation. The semiconductor modeling capability in Charon was developed to work in a manner similar to other commercially available TCAD codes. What sets it apart is that it supports large-scale parallel execution through its use of decomposition and parallel solution tools available in the Trilinos suite of algorithms and enabling technologies.  We will present the current state of TCAD modeling at large and Charon’s current parallel capabilities.  Strong and weak scaling and grind times will be presented for a range of typical semiconductor devices.  Plans for Charon’s future approaches to using next generation platforms will be discussed.

Sandia National Laboratories is a multimission laboratory managed and operated by National Technology &amp; Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy’s National Nuclear Security Administration under contract DE-NA0003525.</blurb>
    <EventParentName>MS5 Parallel Simulation of Circuits, Devices, and Electromagnetics Environments Effects</EventParentName>
    <external_id>67840-102028</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallel-in-Time with Sundials and Xbraid</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS7</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  1:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>David J. Gardner</EventSpeakers>
    <EventSpeakerUniqueID>773860</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101906</EventHandoutURL>
    <blurb>Numerical methods for integrating evolutionary equations in time typically rely on sequential time marching schemes and parallelization in the spatial dimension. As gains in computational power shift from faster processors to massively parallel systems serial time integration becomes a bottleneck in parallel efficiency. In order to leverage the greater concurrency on these systems parallel-in-time methods introduce an additional dimension of parallelism by distributing the workload in time across multiple processors. Recent work on multigrid-reduction-in-time (MGRIT) has shown significant speedups over sequential time stepping and is a relatively non-intrusive approach. In this talk we will discuss the recent efforts to combine the adaptive-step explicit, implicit, and IMEX time integration methods from the SUNDIALS ARKode library with the XBraid MGRIT library.
</blurb>
    <EventParentName>MS7 Advances in Parallel-in-Time Integration Methods</EventParentName>
    <external_id>67845-101906</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallel-in-Time Simulation of Electrical Powergrids with Unscheduled Events</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS7</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:25PM</starts_at>
    <ends_at>Feb 12 2020  1:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Stefanie Guenther</EventSpeakers>
    <EventSpeakerUniqueID>790057</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101908</EventHandoutURL>
    <blurb>Numerical simulations of electrical power grids often require accounting for unscheduled events such as equipment limits, deadbands, and faults. Time-serial integration approaches typically capture those events at runtime, modifying grid components and their dynamics in a sequential step-by-step manner. However, some power grid simulation studies involve long time domains and various time scales, serial time integration becomes a bottleneck for fast and scalable simulation solvers.

In order to reduce the time-to-solution of power grid simulations, we apply a time-parallel integration scheme based on multigrid reduction in time, which allows for concurrency along the time-domain. While time-parallel simulations for power grids show excellent scaling behavior if a priori knowledge of the event times is available, handling unscheduled events can impair the multigrid convergence. In this talk we will summarize approaches for capturing unscheduled events in a time-parallel multigrid scheme and present first results on regulation study problems that include unscheduled component limits and faults.
</blurb>
    <EventParentName>MS7 Advances in Parallel-in-Time Integration Methods</EventParentName>
    <external_id>67845-101908</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Tool (interconnect) is Only As Good As its User (the Network Stack): Solving Network Bottlenecks via Software Stack Simulation in the Structural Simulation Toolkit</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS8</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:50PM</starts_at>
    <ends_at>Feb 12 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jeremiah Wilke</EventSpeakers>
    <EventSpeakerUniqueID>758533</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101572</EventHandoutURL>
    <blurb>Designing interconnects for future HPC systems now goes far beyond questions of topology and routing. High-performance "lossless" interconnects like Infiniband are increasingly incorporating features long popular in Ethernet like quality of service (QoS), congestion control, and software-defined networks (SDN). Combining these features with new system trends including heterogenous accelerators, further node disaggregation, and optical switches are creating a new and challenging network design space. Architecture simulation has been used successfully to predict and understand network performance. To simulate large systems, these simulations are often based on lightweight models or traces specifically designed for MPI.  This co-design disconnect between production network stacks and simulation limits collaboration between architecture and runtime teams.  Tuning new interconnect features for maximum performance creates more and more burden on the network software stack, meaning simulation tools must adapt to develop network software as well as hardware.  Here we present recent developments in compiler tools for the Structural Simulation Toolkit (SST) designed to bridge the gap between simulator-specific models and real applications.  In particular, we address auto-skeletonization via compilers that transforms large codes spanning thousands of processes and PBs of memory into a single simulator process needing only GBs of memory to simulate.</blurb>
    <EventParentName>MS8 Co-Design of Networking for Scientific HPC Applications</EventParentName>
    <external_id>67721-101572</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Opportunities for Multi Precision Computation in Memory Bound Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS10</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:00PM</starts_at>
    <ends_at>Feb 12 2020  4:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mawussi Zounon</EventSpeakers>
    <EventSpeakerUniqueID>771969</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101536</EventHandoutURL>
    <blurb>Initially introduced for data storage only, the half precision floating-point format has been quickly adopted for computation by the machine learning community. Latterly, the iterative refinement algorithm for solving systems of equations has been revisited to harness the performance gain of half precision. It is for such compute-intensive operations that half precision has gained popularity. However, with as few as 16 bits for floating-point number representation, half precision also has the advantage of accelerating data movement operations. It is therefore worthwhile evaluating its potential for improving the performance of memory bandwidth bound applications. In this talk, we firstly discuss various ways to exploit reduced precision in memory bandwidth bound applications, with a special focus on sparse linear algebra routines. Secondly, we will present a set of benchmarks to assess the gain of reduced precision in these applications, when the time to convert input data to the reduced precision format is considered. Finally, we will share the key lessons learned from re-engineering real-world application to exploit reduced precision.
</blurb>
    <EventParentName>MS10 Advances in Algorithms Exploiting Low Precision Floating-Point Arithmetic - Part I of II</EventParentName>
    <external_id>67712-101536</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Fully Unspilt Wave Propagation Algorithm for Shallow Water Flows on GPUs</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS56</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:10PM</starts_at>
    <ends_at>Feb 14 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Donna Calhoun</EventSpeakers>
    <EventSpeakerUniqueID>714749</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101544</EventHandoutURL>
    <blurb>The focus of this talk is on a GPU implementation of the fully multi-dimensional patch solver based on the wave propagation algorithm (R. J. LeVeque, Clawpack).  Our CUDA implementation is designed for use on small, fixed size patches (32x32) used in a larger the block-based adaptive code ForestClaw (D. Calhoun and C. Burstedde).  To update patches on the GPU, we batch-process O(1000) patches per kernel call.  Each patch is assigned a single CUDA thread block, eliminating the need for syncing between blocks.  By redesigning the WPA, we are able to completely update the solution on each patch in a single batch kernel call. To avoid branch divergence, special attention is given to the implementation of wave limiters.  Resulting time using the GPU is about 5-7x speedup over a single CPU.  We will demonstrate our algorithm using examples from the shallow water wave equations implemented in ForestClaw.
</blurb>
    <EventParentName>MS56 HPC Aspects of Tsunami Simulation</EventParentName>
    <external_id>67714-101544</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallel-in-Time Simulation of the Schrödinger Equation</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS58</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:35PM</starts_at>
    <ends_at>Feb 14 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hannah Rittich</EventSpeakers>
    <EventSpeakerUniqueID>767496</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101775</EventHandoutURL>
    <blurb>We discuss the applicability of parallel-in-time integration methods to the Schrödinger equation.

Modern supercomputers consist of millions of cores. Hence, to use these machines efficiently for numerical simulations, implementations need to be designed to have minimal serial parts.

Many current parallel solvers for time-dependent partial differential equations compute a sequence of time-steps. Here, each core computes the solution for a portion of the spatial domain, which can be done in parallel for each time-step. Each time-step, however, is computed sequentially, often limit the scalability of the implementation. Parallel-in-time integrators
avoid this limit by allowing to compute multiple time-steps in parallel. While this approach works particularly well for diffusive problems, parallel-in-time integration of oscillatory problems, like the Schrödinger equation, is more difficult.

The Schrödinger equation describes the time evolution of quantum mechanical systems. To understand atomic and molecular phenomena it is often useful to inspect these time evolutions. Hence, efficient ways of solving the equation are needed. Being essentially the heat equation in imaginary time, it is similarly easy to implement. It is, however, more challenging to obtain
parallel speedup in time due to its oscillatory nature. Solving the Schrödinger equation hopefully provides insight on how to solve other oscillatory problems more efficiently as well.</blurb>
    <EventParentName>MS58 Parallel-in-Time Integration Methods - Part II of II</EventParentName>
    <external_id>67797-101775</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Recent Developments Around the Block Low-Rank PaStiX Solver</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS60</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:35PM</starts_at>
    <ends_at>Feb 14 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mathieu Faverge</EventSpeakers>
    <EventSpeakerUniqueID>747161</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101869</EventHandoutURL>
    <blurb>Recent works in sparse direct solvers have multiplied the solutions to exploit data-sparsity additionally to sparse structure in these solvers. Among these solution, the PaStiX solver exploits block low-rank structures to reduce both its memory consumption and time to solution. In this talk, we will discuss recent changes made to the solver in this context such as the study of suitable low-rank compression kernels, adapted ordering heuristics, or scheduling strategies through the use of runtime systems. Performance results of the latest release including the presented features will be shown and discussed.

</blurb>
    <EventParentName>MS60 Low-Rank Compression-Based Fast Sparse Direct Solvers</EventParentName>
    <external_id>67831-101869</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Towards Optical Neural Networks and Annealing Machines at the Quantum Limit</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS62</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:05AM</starts_at>
    <ends_at>Feb 15 2020 11:25AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ryan Hamerly</EventSpeakers>
    <EventSpeakerUniqueID>790052</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101894</EventHandoutURL>
    <blurb>Progress in deep learning has relied on exponential growth of available compute power, but is facing a resource crunch as electronic systems become limited by energy consumption, interconnect bottlenecks, and the end of Moore's Law.  Optics offers an intriguing alternative due to potential orders-of-magnitude advantages in latency, throughput, and energy efficiency at the bottleneck tasks for deep learning.  This talk reviews the field of optical neural networks (ONNs) and introduces our new approach based on optical homodyne detection.  Designing an ONN that can benefit from the advantages of optics involves maximizing the amount of optical parallelism so that a computation is not bottlenecked by I/O costs.  In conventional ONNs, the optical parallelism is greatly limited by available chip area, but that the homodyne scheme circumvents this limitation by trading off spatial and time complexity.  We analyze the energy consumption of ONNs and introduce a ``standard quantum limit' for ONNs, set by quantum fluctuations in coherent states.  This limit, which can be as low as 50 zJ/op, is more than 6 orders of magnitude beyond the current state-of-art and suggests that performance below the thermodynamic (Landauer) limit is theoretically possible in ONNs.</blurb>
    <EventParentName>MS62 Novel Computational Algorithms for Future Computing Platforms - Part II of III</EventParentName>
    <external_id>67838-101894</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Neuromorphic Computing: A Platform for Machine Learning and Beyond</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS62</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:55AM</starts_at>
    <ends_at>Feb 15 2020 12:15PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Catherine Schumann</EventSpeakers>
    <EventSpeakerUniqueID>790053</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101895</EventHandoutURL>
    <blurb>Neuromorphic computing systems provide a promising approach for performing machine learning tasks (both training and inference) for the future of computing.  They are also a compelling platform for performing neuroscience simulation.  However, neuromorphic computers also have promise in performing non-machine learning, non-neuroscience tasks.  Neuromorphic systems are a novel compute platform that perform specific types of computation in a massively parallel and often event-driven way, with collocated processing and memory, and often implement some form of stochasticity in the way computation is performed that provide noise.  These characteristics can be leveraged to perform other types of computation, including certain graph algorithms.  In this talk, the various use cases of neuromorphic systems, as well as how those use cases leverage the unique properties of neuromorphic computers, will be presented.</blurb>
    <EventParentName>MS62 Novel Computational Algorithms for Future Computing Platforms - Part II of III</EventParentName>
    <external_id>67838-101895</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The HACC Code: Particle Methods for Large-Scale Cosmological Simulations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS64</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 11:00AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Salman Habib</EventSpeakers>
    <EventSpeakerUniqueID>750668</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101857</EventHandoutURL>
    <blurb>The coming generation of cosmological surveys will provide a wealth of information regarding a number of mysteries -- the nature of dark energy and dark matter, and the origin of the primordial fluctuations from which all structure in the Universe came to be. The interpretation of the observations requires a predictive computational capability that can simultaneously cover a large dynamic range in space, time, and density. Modern particle methods for the Vlasov-Poisson system of equations, as well as for an additional gasdynamic component, provide high performance on next-generation platforms as well as good control on accuracy. In this talk, I will describe the algorithms underlying HACC (Hardware/Hybrid Accelerated Cosmology Code) and describe its capabilities for carrying out extreme-scale cosmological simulations on a number of different computational platforms. 

</blurb>
    <EventParentName>MS64 Particle Methods: Algorithms and Software Technology for Exascale - Part II of III</EventParentName>
    <external_id>67829-101857</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Verified Numerical Computations for Eigenvalue Problems on Large-Scale Parallel Systems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS66</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:05AM</starts_at>
    <ends_at>Feb 15 2020 11:25AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Takeshi Terao</EventSpeakers>
    <EventSpeakerUniqueID>780268</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101600</EventHandoutURL>
    <blurb>Eigensolvers are essential parts in a number of applications of interest. We focus on both standard and generalized symmetric eigenvalue problems. We assume that all approximate eigenpairs are given. We propose a verification method that provides error bounds of the approximate eigenvalues on the basis of the Gershgorin circle theorem. Since the main cost of the verification method is devoted to matrix multiplication, scalability of the proposed method is expected to be very high on large-scale parallel systems. We will show scalability of the verification method on the RIKEN K computer and the FUJITSU Supercomputer PRIMEHPC FX100. In addition, comparison of the error bounds of given approximate eigenvalues will be provided. It is confirmed that there is no multiple eigenvalue for test matrices in ELSES Matrix Library, which is a collection of matrix data generated by a quantum mechanical nanomaterial simulator.</blurb>
    <EventParentName>MS66 New Approaches for Software Auto-Tuning and Accuracy Assurance - Part I of II</EventParentName>
    <external_id>67742-101600</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Porting Sewas Task-Based Seismic Code on Arm Platforms</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS68</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:30AM</starts_at>
    <ends_at>Feb 15 2020 11:50AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>1900</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Fabrice Dupros</EventSpeakers>
    <EventSpeakerUniqueID>718023</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101912</EventHandoutURL>
    <blurb>The production of reliable three-dimensional images of the subsurface remains a major challenge in the oil and gas industry and strongly relies on the efficient exploitation of supercomputers. In recent years, heterogeneous hardware has gained a lot of traction, requiring a profound shift on the way numerical applications are implemented. In this paper, we highlight the key role of high-level programming models and efficient runtime systems to design next generation seismic wave propagation codes. In our case, the application dataflow is built on top of PaRSEC, a generic task-based runtime system. We discuss the results obtained with the implementation of a finite-differences numerical scheme on various platforms including AArch64 architectures.

</blurb>
    <EventParentName>MS68 Advanced HPC Trends Oil and Gas Applications - Part I of II</EventParentName>
    <external_id>67846-101912</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Power-Aware Scheduling with Slurm: Simulation and Practice</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS69</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:55AM</starts_at>
    <ends_at>Feb 15 2020 12:15PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tapasya Patki</EventSpeakers>
    <EventSpeakerUniqueID>768277</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101741</EventHandoutURL>
    <blurb>Power management is one of the key research challenges on the path to exascale. Supercomputers today are designed to be worst-case power provisioned, leading to two main problems -- limited application performance and under-utilization of procured power. In this talk, we will present a practical and simulation perspective on the development and evaluation of a low-overhead resource manager targeted at future power-constrained clusters. Our approach is based on an adaptive policy, which derives job-level power bounds in a fair-share manner and supports overprovisioning and power-aware backfilling.</blurb>
    <EventParentName>MS69 The Many Faces of Simulation for HPC - Part I of II</EventParentName>
    <external_id>67786-101741</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Performance Portable and Productive Resilience using Kokkos </name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS70</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:05AM</starts_at>
    <ends_at>Feb 15 2020 11:25AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jeffery Miles</EventSpeakers>
    <EventSpeakerUniqueID>790008</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101717</EventHandoutURL>
    <blurb>Performance portable abstractions, such as Kokkos, RAJA, and HPX are becoming more and more prevalent among HPC applications. Kokkos in particular not only defines performance portable execution abstractions, but also performance portable data abstractions, such as the \texttt{Kokkos::View}.
We propose a framework for both implicit and explicit checkpointing of \texttt{Kokkos::View} in addition to a resilient version of \texttt{Kokkos::parallel\_for} to provide redundant computation for reliability. Our framework provides a natural API for checkpointing that allows applications already using Kokkos for performance portability to implement fault tolerance with minimal and non-intrusive code changes.  In the presentation, we will discuss the implementation of the resilience Kokkos extension and a couple of application use cases.
</blurb>
    <EventParentName>MS70 Resilience and Fault Tolerance for Extreme Computing Systems - Part II of III</EventParentName>
    <external_id>67784-101717</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Leveraging Random Walks and Neuromorphic Hardward to Solve Elliptical Integro-PDEs</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS72</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>J. Darby Smith</EventSpeakers>
    <EventSpeakerUniqueID>791690</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101896</EventHandoutURL>
    <blurb>The bulk of recent research into neuromorphic hardware applications has focused on whether it is suitable for use with emerging AI algorithms.  The field of neuromorphic computing must determine whether hardware will primarily be used for algorithms that brains implement or if hardware can have a broader impact outside of AI applications.  We explore the latter, providing a framework for numerically solving certain PDEs on large-scale spiking neuromorphic platforms.

In previous work, we have demonstrated two efficient and parallelizable random walk algorithms for spiking circuits.  Random walk solutions to PDEs, including results such as the Feynman-Kac formula, are ideal applications of highly efficient and parallel algorithms.    In this talk we showcase the relation between a class of elliptical integro-PDEs and jump diffusion SDEs, developing a general algorithm for implementation on neuromorphic hardware.  We demonstrate proof of concept on three applications: basic diffusion, Black-Scholes option pricing models, and particle transport.  Finally we discuss the scaling implications of our algorithm on existing spiking neuromorphic platforms.
</blurb>
    <EventParentName>MS72 Novel Computational Algorithms for Future Computing Platforms - Part III of III</EventParentName>
    <external_id>67839-101896</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Search Space Reduction Through Analytical Modeling</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS75</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  3:05PM</starts_at>
    <ends_at>Feb 15 2020  3:25PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tze Meng Low</EventSpeakers>
    <EventSpeakerUniqueID>780272</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101609</EventHandoutURL>
    <blurb>As architectures become more complex, and more tuning factors are introduced, the time required to obtain a fast implementation via auto-tuning is expected to continue to increase. In this talk, we focused on using analytical modeling techniques to model the hardware in order to reduce the search space so that auto-tuning can be employed on parameters and hardware features that are harder to model. 
</blurb>
    <EventParentName>MS75 New Approaches for Software Auto-Tuning and Accuracy Assurance - Part II of II</EventParentName>
    <external_id>67743-101609</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>WarpX: Electromagnetic Particle-in-Cell with Adaptive Mesh Refinement for Advanced Particle Accelerators</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS73</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Maxence Thevenet</EventSpeakers>
    <EventSpeakerUniqueID>784808</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101863</EventHandoutURL>
    <blurb>Turning the current experimental plasma accelerator state-of-the-art from a promising technology into mainstream scientific tools depends critically on high-performance, high-fidelity modeling of complex processes that develop over a wide range of space and time scales. As part of the U.S. Department of Energy’s Exascale Computing Project (ECP), WarpX[1] is an electromagnetic Particle-In-Cell (PIC) code for plasma dynamics, with specific features for plasma-based particle accelerators. It combines adaptive mesh refinement from the ECP co-design center AMReX with advanced algorithms, including a Lorentz boosted frame technique and spectral solvers based on local FFTs. 

WarpX has been used for production runs on CPU, and has been ported to GPU with significant speedup. The general organization and main features of the code will be presented. When running on a GPU-accelerated architecture, the whole PIC loop is executed on the GPU to minimize host-device communications. Main challenges and strategy for porting the code to GPU will be addressed, with special focus on particle handling.

[1] [J.-L.Vay, Warp-X: A new exascale computing platform for beam–plasma simulations, Nuclear Instruments and Methods in Physics Research Section A, 909, p. 476]



</blurb>
    <EventParentName>MS73 Particle Methods: Algorithms and Software Technology for Exascale - Part III of III</EventParentName>
    <external_id>67830-101863</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Towards Efficient Communication on Heterogeneous Architectures</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS34</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:35PM</starts_at>
    <ends_at>Feb 13 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Amanda Bienz</EventSpeakers>
    <EventSpeakerUniqueID>769391</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101900</EventHandoutURL>
    <blurb>Emerging heterogeneous architectures are comprised of nodes that contain both CPUs and GPUs, with many paths for communication inbetween.  As solvers are optimized for heterogeneous machines, inter-GPU communication becomes a dominant source of cost.  However, communication costs are highly dependent on the relative locations of the sending and recieving GPUs, with intra-socket messages accelerated by GPU Direct RDMA, while communication across sockets yields larger costs.  Furthermore, inter-node messages sent directly between GPUs with Cuda-Aware MPI require additional costs as they are injected into the network.  In this talk, we present performance models for a Power9 system, displaying the variation in message costs.  Furthermore, we present node-aware strategies to improve inter-GPU communication performance.

</blurb>
    <EventParentName>MS34 Advances and Challenges in Solvers on GPGPU-Based High-Performance Computing Architectures - Part II of II</EventParentName>
    <external_id>67834-101900</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>On Leveraging Communication-Optimal QR Factorization in Dense Symmetric Eigensolvers</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS35</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:45PM</starts_at>
    <ends_at>Feb 13 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Edward Hutter</EventSpeakers>
    <EventSpeakerUniqueID>780246</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101945</EventHandoutURL>
    <blurb>Architectural trends over the past two decades have enlarged the algorithmic search space to include and prioritize algorithms that asymptotically decrease communication and synchronization along their critical path, even at the expense of extra computation.

Recent theoretical developments in both numerically stable QR factorizations and dense symmetric eigensolvers focus on asymptotically decreasing the communication and synchronization, yet few have been shown to be practical and thus have not been incorporated into popular distributed-memory linear algebra libraries such as ScaLAPACK and Elemental.

We investigate the performance and scalability of successive band reduction in dense symmetric eigensolvers that leverage communication-optimal QR factorizations , as determined by the feasible memory footprint. Specifically, we study the practical potential for bulge chasing more than one intermediate band (recently proven to incur the minimal amount of communication) using a new practical communication-optimal QR factorization algorithm.

</blurb>
    <EventParentName>MS35 Parallel Matrix Factorization Algorithms - Part II of III</EventParentName>
    <external_id>67848-101945</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Composition in Scientific and Engineering Applications: Lessons Learned from the Legion Programming System</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS36</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:45PM</starts_at>
    <ends_at>Feb 13 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Galen Shipman</EventSpeakers>
    <EventSpeakerUniqueID>780655</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101565</EventHandoutURL>
    <blurb>Complexity of scientific and engineering software for HPC is often attributed to the requirements of performance and scalability on modern supercomputers. Another axis of complexity is a result of the complex compositions of models, libraries, frameworks, and utilities that make up some Scientific and Engineering software projects. This talk will focus on lessons learned from the Legion Programming System in building software compositions and how a strong data model coupled with apparently sequential semantics can be useful in managing complexity. </blurb>
    <EventParentName>MS36 Progress and Challenges in Extreme Scale Computing and Big Data - Part II of II</EventParentName>
    <external_id>67710-101565</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Toward AI-based Medical Data Analytics and Clinical Workflows</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS36</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:35PM</starts_at>
    <ends_at>Feb 13 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Weichung Wang</EventSpeakers>
    <EventSpeakerUniqueID>35418</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101566</EventHandoutURL>
    <blurb>We lay out our plan to build a platform called Artificial Intelligence for Medical Image  Analysis (AIMIA). The AIMIA platform consists of Artificial Intelligent Engines (AI Engines) and Augmented Intelligence Workflows (AI Workflows). The AI Engines include high-performance algorithms and software modules aiming to extract insightful information from a large volume of medical image datasets accurately, efficiently, and robustly. In particular, the AI Engines include Image Processing, Quantitative Analytics, Deep Learning, Machine Learning, and High Dimensional Data Analysis Toolboxes to analyze medical images. By taking these algorithms and software modules as the building blocks, we further build up innovative AI Workflows in various clinical applications. AI Workflows examples include precision cancer treatments in a lung, hypopharyngeal, hepatocellular carcinoma, digital pathology whole slide image analysis for prostate cancers, pancreatic masses classification and detection, radiotherapy treatment planning in lung cancer, and psychiatric disorders phenotyping. These examples illustrate how we apply the AI Engines to configure AI Workflows in clinical medical cares and biomedical research. AIMIA is also a platform allowing interdisciplinary experts from academia and industry in medical, mathematical, statistical, computational, and information sciences to work together to ensure the research and development efforts can benefit the society broadly.</blurb>
    <EventParentName>MS36 Progress and Challenges in Extreme Scale Computing and Big Data - Part II of II</EventParentName>
    <external_id>67710-101566</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>On the Theory of Discrete, Adaptive Space Filling Curves</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS37</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:35PM</starts_at>
    <ends_at>Feb 13 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Johannes Holke</EventSpeakers>
    <EventSpeakerUniqueID>768215</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101691</EventHandoutURL>
    <blurb>We present a newly developed, self-contained theory for discrete space-filling
curves (SFCs). Mesh partitioning according to such SFCs has been established
as a fast and reliable technique, in particular when combined with frequent
adaptive mesh refinement (AMR) and coarsening. SFCs map the elements of a
uniform or adaptive mesh onto a finite index set, thus providing a linear
order of the elements. AMR operations change this order only locally.

In addition to practical use in HPC, investigating the properties of SFCs and
developing new constructions are subjects of many theoretical studies. The
definition for discrete SFCs is usually stated as an iteration step in a
sequence that converges to an analytical SFC, or provided in the language of
L-systems. Both of these definitions, however, are not ideally suited to
represent the complexity of arbitrarily refined adaptive meshes.

To address this issue, we provide a set of self-contained concepts and
definitions, introducing new underlying structures such as refinement spaces
and refinement rules. Discrete SFCs map these refinement spaces to an index
set and satisfy certain locality properties. Our construction is independent
of any particular geometric embedding. To demonstrate the usefulness of this
approach, we present as first application the cross-product binary operation
between SFCs that allows us to construct a new SFC for prism elements.
</blurb>
    <EventParentName>MS37 Challenges in Parallel Adaptive Mesh Refinement - Part II of III</EventParentName>
    <external_id>67771-101691</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>CLIMA-atmos: a Non-Hydrostatic Model of the Atmosphere for Next-Generation Super-Computing Systems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS38</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Thomas Gibson</EventSpeakers>
    <EventSpeakerUniqueID>774447</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101840</EventHandoutURL>
    <blurb>The Climate Modeling Alliance (CliMA) is an ambitious project with the goal to deliver actionable predictions for the future state of the Earth's climate. The project is a coalition of scientists, engineers, and applied mathematicians from Caltech, MIT, the Naval Postgraduate School, and NASA’s Jet Propulsion Laboratory. Leveraging recently advances in computational and data science, the model known as "CLIMA" is an open-source Earth-system framework built in the Julia programming language. In this talk, we present the atmospheric dynamical core of CLIMA: CLIMA-atmos. The model utilizes a non-hydrostatic formulation of the governing equations and state-of-the-art high-order discontinuous Galerkin discretizations on tensor-product grids. We provide an overview of the dynamical core and highlight its role in the CLIMA Earth system model.
</blurb>
    <EventParentName>MS38 High-Performance Numerics and Model Development for Geophysical Systems - Part II of II</EventParentName>
    <external_id>67823-101840</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Discontinuous Galerkin Method for Idealised Hurricane Storm Surge</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS38</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:45PM</starts_at>
    <ends_at>Feb 13 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Nicole Beisiegel</EventSpeakers>
    <EventSpeakerUniqueID>771535</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101841</EventHandoutURL>
    <blurb>In recent years, Discontinuous Galerkin (DG) discretisations of non-linear 2D shallow water equations have evolved to be a viable tool for many geophysical applications, including hurricane storm surge simulations. As these flood simulations are time critical and computational resources are limited, there is a growing demand for the increase of computational efficiency and reduced run time. For that reason we have developed a DG storm surge model on a triangular and dynamically adaptive mesh. The dynamic mesh refinement and coarsening are driven by physics-based refinement indicators capturing major model sensitivities. In general, non-uniform, dynamically adaptive meshes are a useful tool for reducing computational complexities for simulations that exhibit strongly localised features as is the case for hurricane storm surges. A thorough comparison with simulation results obtained on a uniform mesh can be difficult as commonly used metrics and measures such as the classical definition of convergence do not apply without modification. To gain more insight into these adaptive meshes and to build a theoretical framework for their assessment, we present and discuss a number of mesh metrics that we apply to our triangular adaptive mesh and that demonstrate the benefits.

We acknowledge funding by the Irish Research Council under the research project GOIPD/2018/248 and Proyecto Mayor UTA 8718-16, Universidad de Tarapaca.


</blurb>
    <EventParentName>MS38 High-Performance Numerics and Model Development for Geophysical Systems - Part II of II</EventParentName>
    <external_id>67823-101841</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Higher-Order Model for Glacier Flow via Spectral Semi-Discretization</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS38</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:10PM</starts_at>
    <ends_at>Feb 13 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daniel Shapero</EventSpeakers>
    <EventSpeakerUniqueID>755456</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101842</EventHandoutURL>
    <blurb>Mathematical models of glacier flow velocity have historically been divided into 2D models that are easy to solve but only applicable over particular stress regimes, and 3D models that are applicable everywhere but expensive computationally. In this talk I will describe a model of glacier flow that can capture all stress regimes but that is much more tractable computationally. Three tricks are involved in defining the model: terrain-following coordinates; vertical semi-discretization; and convex variational principles. I will describe how this model was implemented using the package Firedrake and, finally, I will show some results for real glaciers.
</blurb>
    <EventParentName>MS38 High-Performance Numerics and Model Development for Geophysical Systems - Part II of II</EventParentName>
    <external_id>67823-101842</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>TBD</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS39</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mario Szegedy</EventSpeakers>
    <EventSpeakerUniqueID>790330</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102112</EventHandoutURL>
    <EventParentName>MS39 Recent Advances and Trends in Hybrid Quantum-Classical Algorithms - Part II of II</EventParentName>
    <external_id>67763-102112</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Utopia: a Performance-Portable C++ Library for Non-Linear Algebra</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS41</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:20AM</starts_at>
    <ends_at>Feb 14 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Nur  Fadel</EventSpeakers>
    <EventSpeakerUniqueID>754574</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102038</EventHandoutURL>
    <blurb>We introduce Utopia, a C++ library designed for parallel non-linear solution strategies that is performance-portable by virtue of using Kokkos. Scientific-computing libraries require continuous development and updating to support new hardware architectures, new numerical methods, and new programming models. We wish to avoid such changes to libraries and their dependencies requiring modifications in user and application code that depend on them. State-of-the-art software achieves this by leveraging high-level programming interfaces that provide abstractions for the economical expression of complex numerical procedures. These interfaces separate the model from the computation, hiding low-level implementation details from the code of applications, such as non-linear solution algorithms. 
Utopia has multiple interoperable back-ends, such as PETSc and Trilinos, which allows users to choose between the algorithms offered by the two libraries without modifying their code. 
Users can also seamlessly and transparently run on all hardware architectures supported by the chosen back end, which is Kokkos when using Trilinos.
A wide range of tensor manipulation abstractions are accessible via Utopia's front-end, which facilitates implementation of custom algorithms that can use both PETSc or Trilinos back-ends. Several solvers have been implemented on top of Utopia's front-end and are provided out of the box, including multigrid and recursive multilevel trust-region strategies.

</blurb>
    <EventParentName>MS41 Trilinos and Hardware Independent Computing (Kokkos)</EventParentName>
    <external_id>67852-102038</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The Sustainability Lessons of the SLATE Project</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS42</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:20AM</starts_at>
    <ends_at>Feb 14 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mark Gates</EventSpeakers>
    <EventSpeakerUniqueID>752091</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101782</EventHandoutURL>
    <blurb>The Software for Linear Algebra Targeting Exascale (SLATE) project is a US Department of Energy (DOE)-funded effort to replace the legacy libraries LAPACK and ScaLAPACK with a modern software package that targets both GPU-accelerated and homogeneous multi-core systems---and will be capable of reaching exascale performance. The development of SLATE faces numerous risks due to the uncertainty of the target hardware and the rapid evolution of the software stack. This presentation highlights the software engineering practices meant to minimize the impact of changes in technology and to maximize SLATE's sustainability in the long run.








</blurb>
    <EventParentName>MS42 Improving Productivity and Sustainability for Parallel Computing Software - Part I of II</EventParentName>
    <external_id>67772-101782</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Designing An Adaptable Framework for Highly Scalable Multidimensional Spectral Transforms</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS43</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:45AM</starts_at>
    <ends_at>Feb 14 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Dmitry Pekurovsky</EventSpeakers>
    <EventSpeakerUniqueID>790063</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101921</EventHandoutURL>
    <blurb>Since multidimensional Fast Fourier Transforms (FFTs) is a ubiquitous algorithm in many areas of High Performance
Computing, it is important to maximize its access for multiple use cases in a consistent way, without sacrificing
scalable performance. P3DFFT++ was conceived as a universal toolbox for spectral transforms on pre-Exascale and
Exascale platforms. It is a unique adaptable software framework encompassing a wide range of options for using
FFTs and other spectral-like transforms in three and more dimensions. It includes user-defined data layout and
distribution schemes, a variety of transform types, transpose utilities, as well as derivative and convolution
operations. The framework is modular and flexible for various use cases as well as architectures, including
heterogeneous platforms. In addition, it includes an autotuning mechanism for maximizing performance. This
talk provides the background of this work, touches on P3DFFT++ design choices and discusses performance
considerations. A typical use scenario is demonstrated.

</blurb>
    <EventParentName>MS43 Parallel Sparse and Conventional FFTs, Applications and Implementation - Part I of II</EventParentName>
    <external_id>67764-101921</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallel Sparse Indefinite Solvers</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS44</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:20AM</starts_at>
    <ends_at>Feb 14 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Florent Lopez</EventSpeakers>
    <EventSpeakerUniqueID>790069</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101950</EventHandoutURL>
    <blurb>Many applications in science and engineering require the solution of large sparse linear systems of equations. For solving such problems, direct methods are frequently employed because of their robustness, accuracy and usability as black-box solvers. As modern architectures become more and more complex, with an increasing number of cores per chip, a deeper memory hierarchy and the integration of accelerators such as GPUs, it becomes all the more challenging to exploit the potential performance of such machines for sparse matrix factorization algorithms especially in the context of symmetric indefinite systems. Although significant efforts has gone into positive-definite systems, little progress has been reported in the much harder indefinite case. One major advance for tackling these problems is the design of the APTP (a posteriori threshold pivoting) strategy that has been implemented in the SSIDS solver and proven to be both efficient on multicore architectures compared to the state-of-the-art direct solvers. In this talk, we present the DAG-based solver SpLDLT that relies on a APTP strategy and uses the StarPU runtime system for implementing it parallel version. We show the benefits of our approach for exploiting heterogeneity in the the context of GPU-accelerated multicore systems.</blurb>
    <EventParentName>MS44 Parallel Matrix Factorization Algorithms - Part III of III</EventParentName>
    <external_id>67849-101950</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>On the Use of GPU-Enabled Clusters in Cardiac Electrophysiology</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS49</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:20AM</starts_at>
    <ends_at>Feb 14 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>701</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Johannes Langguth</EventSpeakers>
    <EventSpeakerUniqueID>778867</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101527</EventHandoutURL>
    <blurb>Recent advances in personalized arrhythmia risk prediction show that computational models can provide not only safer but also more accurate results than invasive procedures.  However, detailed organ-scale simulations of calcium handling and electrical signal transmission in the human heart require the stochastic simulation of a large number of ion channels in each cell, which consumes immense processing power for the simulation of a single heartbeat, thereby creating the need for large scale parallel implementations. 

Fortunately, the cost of computing has fallen dramatically in the past decade. A prominent reason for this is the recent introduction of manycore processors such as GPUs, which by now power the majority of the world’s leading supercomputers. These devices owe their success to the fact that they are optimized for massively parallel workloads, such as applying similar ODE kernel computations to millions of mesh elements in scientific computing applications.

In this talk, we present codes for solving such cardiac models on structured and unstructured meshes, and discuss the challenges involved in modernizing these codes to run on heterogeneous supercomputers. We focus on the interaction between OpenMP, MPI, and CUDA in such computations, as well as optimizations to communication and vector processing, and discuss the challenges involved in building the scalable simulation codes needed to close the gap to accurate real-time computations.</blurb>
    <EventParentName>MS49 GPU Computing for Solving Large Scale Scientific Problems</EventParentName>
    <external_id>67664-101527</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Addressing the Communication Bottleneck: Towards a Modular Precision Ecosystem for High Performance Computing</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS10</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:25PM</starts_at>
    <ends_at>Feb 12 2020  4:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Thomas Grützmacher</EventSpeakers>
    <EventSpeakerUniqueID>789966</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101537</EventHandoutURL>
    <blurb>Over the last years, we have observed a growing mismatch between the arithmetic performance of processors in terms of the number of floating point operations per second (FLOPS) on the one side, and the memory performance in terms of how fast data can be brought into the computational elements (memory bandwidth) on the other side. As a result, more and more applications can utilize only a fraction of the available compute power as they are waiting for the required data. With memory operations being the primary energy consumer, data access is pivotal also in the resource balance and the battery life of mobile devices. In this talk we will introduce a disruptive paradigm change with respect to how scientific data is stored and processed in computing applications. The goal is to 1) radically decouple the data storage format from the processing format; 2) design a "modular precision ecosystem' that allows for more flexibility in terms of customized data access; 3) develop algorithms and applications that dynamically adapt data access accuracy to the numerical requirements.</blurb>
    <EventParentName>MS10 Advances in Algorithms Exploiting Low Precision Floating-Point Arithmetic - Part I of II</EventParentName>
    <external_id>67712-101537</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Adapting Engineering Design Practices to Mathematical Methods in Quantum Computing</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS11</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:00PM</starts_at>
    <ends_at>Feb 12 2020  4:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Steven Bleiler</EventSpeakers>
    <EventSpeakerUniqueID>790299</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102077</EventHandoutURL>
    <blurb>In realizing a quantum computation, frequent practice in Math and CS is “top down”, i.e. quantum computations are realized via the creation of a “big” unitary matrix, whereas in Engineering common practice is “bottom up” design, i.e. quantum computations are realized via the assembly of well vetted “blocks”, e.g. the adder, counter, or shifter. Incompatibility arises via the latter’s use of ancillary bits, which can make the blocks and/or their final assembly non-unitary. This is a reflection of why we don’t see more quantum algorithms, namely that many classical algorithms are difficult to quantize as the matrix structures involved are non-unitary, e.g. finite element methods, graph coloring, or set covering. Our presentation will discuss an approach around these difficulties that is proving particularly useful in Oracle based solution algorithms, e.g. Grover, quantum walk, or phase estimation, noting that “small“ quantum computers are easier to construct and control than “large” ones. Our approach uses a classical computer to organize and reorganize a family of “small” quantum computers into a variety of structures, each specialized to the problem at hand, e.g. adding more constraints to the oracle of a Grover type algorithm as the algorithm progresses. Our presentation closes with applications to quantum machine learning and CAD via the approach’s implicit quantization of classical uses of decision trees and graphs.
</blurb>
    <EventParentName>MS11 Formal (Mathematical) Methods Enabling Applications of Quantum Computers</EventParentName>
    <external_id>67858-102077</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>AMReX on GPUs: Strategies, Challenges and Lessons Learned</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS12</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kevin Gott</EventSpeakers>
    <EventSpeakerUniqueID>786064</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101747</EventHandoutURL>
    <blurb>AMReX is a software framework for building massively parallel block-structured AMR applications using mesh operations, particles, linear solvers and/or complex geometry. AMReX was originally designed to use MPI + OpenMP on multicore systems and recently has ported the majority of its features to GPU accelerators. AMReX’s porting strategy has been designed to allow code teams without a heavy computer science background to port their codes efficiently and quickly with the software framework of their choosing, while minimizing impact to CPU performance or the scientific readability of the code. Further elements of this strategy include providing a clear and concise recommended strategy to application teams, supporting features that allow porting to GPUs in a piece-meal fashion as well as creating sufficiently general interfaces to facilitate adaptation to future changes without user intervention.
 
This talk will give an overview of AMReX's GPU porting strategy to date. This includes a general overview of the porting philosophy and some specific examples that generated noteworthy lessons about porting a large-scale scientific framework. The discussion will also include the current status of AMReX applications that have begun to migrate to hybrid CPU/GPU systems, detail into GPU specific features that have given substantial performance gains, issues with porting a hybrid C++/Fortran code to GPUs and an overview of the limitations of the strategy.
</blurb>
    <EventParentName>MS12 Experiences in Developing GPU Support for Department of Energy Math Libraries - Part II of II</EventParentName>
    <external_id>67791-101747</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MFEM: Accelerating Efficient Solution of PDEs at Exascale</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS12</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:35PM</starts_at>
    <ends_at>Feb 12 2020  3:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yohann Dudouit</EventSpeakers>
    <EventSpeakerUniqueID>783201</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101748</EventHandoutURL>
    <blurb>Traditional implementations of high-order finite element methods using sparse matrices are unsuited to achieve high performance on GPUs due to their low arithmetic intensity and high data movements. On the opposite side, a matrix free formulation of high-order finite element methods can achieve high arithmetic intensity and optimal data movement, making them ideal for GPUs. Finding good algorithms is only the beginning of the journey though. Careful implementation of the algorithms is required to fully exploit the raw power of GPUs. This talk will explore the journey from redesigning finite element algorithms to implementing them efficiently on GPUs to achieve peak performance in the MFEM finite element library.</blurb>
    <EventParentName>MS12 Experiences in Developing GPU Support for Department of Energy Math Libraries - Part II of II</EventParentName>
    <external_id>67791-101748</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Distributed Domain Generation for Large Scale CFD Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS14</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:05AM</starts_at>
    <ends_at>Feb 15 2020 11:25AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>701</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christoph Ertl</EventSpeakers>
    <EventSpeakerUniqueID>769373</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101649</EventHandoutURL>
    <blurb>One of the big challenges in numerical computing on modern high performance clusters for the simulation of real world phenomena is the efficient handling and management of the computational domain. Classical approaches may need to store the complete (logical) topology with each process / node in order to ensure global consistency. This entails significant memory requirements and expensive global communication overhead especially when the domain configuration is altered during run time. Our current work addresses this shortcoming by employing a local, i.e. decentral approach to domain organisation, where the essential idea is to limit the domain view of each participating unit to their direct neighbours.

The new approach affects various parts of the computational pipeline significantly, e.g. the octree-based domain generation which had to be revamped from a central based one to support also decentral structures similarly. To this end, an algorithm has been devised which refines an input geometry on all processes up to a predetermined depth, before distributing the resulting leaf nodes of the geometry as starting points for a subsequent refinement on the respective processes. We will discuss the key points of this approach, including the distribution of the root tree among all processes, avoiding costly communication patterns for neighbourhood searches, and an efficient tree balancing algorithm for a decentral environment.</blurb>
    <EventParentName>MS14 Advanced Visualisation, Analysis, and Parallelisation Concepts for Multi-Scale CFD Simulations in Science and Engineering</EventParentName>
    <external_id>67759-101649</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Additive and Hybrid Nonlinear Two-Level Schwarz Methods and Energy Minimizing Coarse Spaces for Unstructured Grids</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS15</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Martin Lanser</EventSpeakers>
    <EventSpeakerUniqueID>774576</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101832</EventHandoutURL>
    <EventParentName>MS15 Nonlinear Preconditioning</EventParentName>
    <external_id>67821-101832</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Globalization in Nonlinear Feti-Dp</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS15</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:00PM</starts_at>
    <ends_at>Feb 12 2020  4:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Oliver Rheinbach</EventSpeakers>
    <EventSpeakerUniqueID>763792</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101834</EventHandoutURL>
    <EventParentName>MS15 Nonlinear Preconditioning</EventParentName>
    <external_id>67821-101834</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Stable One-Reduce Gram Schmidt Orthogonalization Algorithms</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS16</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:25PM</starts_at>
    <ends_at>Feb 12 2020  4:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Katarzyna Swirydowicz</EventSpeakers>
    <EventSpeakerUniqueID>785023</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101612</EventHandoutURL>
    <blurb>In many important algorithms (i.e. Krylov solvers, eigenvalue computations) orthogonality among multiple vectors is required. The orthogonality is usually achieved  using one of the Gram-Schmidt variants or using Householder decomposition. The modified Gram-Schmidt algorithm can produce O($\varepsilon$)$\kappa\left(A\right)$ orthogonal vectors  and this is sufficient for the backward stability of the MGS-GMRES algorithm, (Paige, Rozlo\v{z}nik and Strako\v{s} (2006)). The original MGS algorithm required $k-1$ inner products and a norm for each column or iteration. Iterated classical Gram-Schmidt with re-orthogonalization (CGS-2) requires $2$ matrix-vector products and a norm, whereas Householder requires at least $4$ reductions. Global reductions are very costly on modern large scale systems,  hence minimizing the number of reductions is paramount for performance.  It turns out that both modified Gram-Schmidt and CGS2  (and the corresponding Arnoldi-QR) can be reformulated to require only one reduction, while being equivalent to the original algorithm. During the talk, both  theoretical and  performance results will be presented.
</blurb>
    <EventParentName>MS16 High Performance Krylov Subspace Methods: Theory, Implementation, and Application - Part II of III</EventParentName>
    <external_id>67749-101612</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Interactive Computing at Scale: Applications in Climate and Hydrologic Modeling</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS17</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:00PM</starts_at>
    <ends_at>Feb 12 2020  4:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Joseph Hamman</EventSpeakers>
    <EventSpeakerUniqueID>790134</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102005</EventHandoutURL>
    <blurb>Climate and hydrologic data produced by both simulation models and observational platforms is being made available to researchers at an overwhelming volume. While the availability of large volumes of climate and hydrologic data presents a myriad of new and exciting opportunities for ground breaking research, it also brings to the forefront significant computational challenges that need to be addressed before we can make the most of our big data for geoscientific research. In this presentation, I will outline some of the central computational challenges that face geoscientists today and how the Pangeo Project is working to address these. In particular, I will focus on the Pangeo Project is making interactive computing on very large datasets possible, through the use of open source software, cloud computing, and new data analysis techniques. Finally, I will highlight some of the scientific applications employing the these emerging tools.




</blurb>
    <EventParentName>MS17 HPC Simulation of the Hydrological Cycle</EventParentName>
    <external_id>67856-102005</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parameter Inference for a Massively Parallel Global Hydrologic Model</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS17</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:25PM</starts_at>
    <ends_at>Feb 12 2020  4:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Luis Samaniego</EventSpeakers>
    <EventSpeakerUniqueID>790137</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102007</EventHandoutURL>
    <blurb>The Advanced Earth System Modelling Capacity
(www.esm-project.net) aims at a more realistic description of the
global hydrology at multiple scales. Two of the main challenges to
achieve this goal are: (1) a parallel routing algorithm suitable
for a high resolution global streamflow network and (2) a parallel
multi-scale parameter estimation technique able to assimilate time
series of observations and remotely sensed data gathered at multiple
resolutions across the globe.

Here we use a hybrid MPI-OpenMP parallelization approach in mHM (www.ufz.de/mhm) which
recursively cuts off equal sized subdomains to gain
computational load balancing while distributing them among all available
nodes using several effective scheduling algorithms for
the high-performance supercomputer JUWELS. Speedup tests carried out on a
European data set [Samaniego et al. 2019 BAMS] indicate that nested
multiscale simulations are possible only if the model exhibits a
scale-invariant parameterization.

In mHM, the parameter estimation is provided
via the multiscale parameter regionalization technique [Samaniego
et al 2010 WRR] and a parallelized global search algorithm [DDS, Tolson
\&amp; Shoemaker 2007, WRR]. In this study, transfer-function parameters for mHM are
estimated with thousands of global streamflow time series (GRDC), FLUXNET
ET products, land surface temperature (MODIS) and the
terrestrial total water storage anomaly (GRACE).


</blurb>
    <EventParentName>MS17 HPC Simulation of the Hydrological Cycle</EventParentName>
    <external_id>67856-102007</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Novel Approaches to Optimize and Execute Task-Based, Irregular Applications on Extreme-Scale, Heterogeneous Systems using PaRSEC</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS18</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:00PM</starts_at>
    <ends_at>Feb 12 2020  4:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Thomas Herault</EventSpeakers>
    <EventSpeakerUniqueID>752105</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101953</EventHandoutURL>
    <blurb>In the context of the Epexa project, we research and test the hypothesis that a tight integration between the language, the compiler, and the runtime system enable developers to dramatically improve the performance of applications that target irregular problems on distributed memory, extreme-scale, and heterogeneous architectures. Specifically, we believe this can be achieved by through a software ecosystem that attacks the twin challenges of programmer productivity and portable performance for advanced scientific applications on massively-parallel, hybrid, many-core systems. The new ecosystem is focused upon high-performance implementations of irregular and dynamic computations that are poorly supported by current programming paradigms, while also enabling efficient execution of regular computations such as dense (multi)linear algebra.
</blurb>
    <EventParentName>MS18 Exploiting Task Parallelism in Exascale Computing Era</EventParentName>
    <external_id>67850-101953</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Space-Time Multilevel Monte Carlo with Application to Cardiac Electrophysiology</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS19</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Rolf Krause</EventSpeakers>
    <EventSpeakerUniqueID>739385</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102049</EventHandoutURL>
    <EventParentName>MS19 Parallel Adaptive Multigrid - Part II of II</EventParentName>
    <external_id>67819-102049</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Cutensor: A High-Performance Cuda Library for Tensor Primitives</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS20</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:00PM</starts_at>
    <ends_at>Feb 12 2020  4:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Paul Springer</EventSpeakers>
    <EventSpeakerUniqueID>790059</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101914</EventHandoutURL>
    <EventParentName>MS20 Frameworks/Libraries for High-Performance Tensor Computations - Part I of II</EventParentName>
    <external_id>67794-101914</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Mixed Precision Numerical Techniques Accelerated with Tensor Cores and its Impact on Today’s Scientific Computing and Implications for Tomorrow’s Hardware Design</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS21</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 12:10PM</starts_at>
    <ends_at>Feb 13 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Azzam Haidar</EventSpeakers>
    <EventSpeakerUniqueID>785958</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101569</EventHandoutURL>
    <blurb>Double-precision-floating-point has been the-de-facto standard for doing scientific simulation for several decades. Problem complexity and the sheer magnitude of data coming from various instruments and sensors motivate researchers to mix and match various approaches to optimize compute resources, including different levels of floating-point precision. In recent years, the big bang for machine learning has focused significant attention on half-precision.

We explored the possibility of using FP16/FP32-Tensor-Cores on NVIDIA-Volta-GPUs to accelerate one of the most common linear algebra routines without loss of accuracy. We achieved a 4x performance increase and 5x better energy efficiency versus the standard FP64 implementation while providing a solution with FP64 accuracy.

We studied a plasma fusion application that simulates the instabilities that occur inside a plasma inside the International-Thermonuclear-Experimental-Reactor (ITER). We show that using our mixed precision solver that harnesses the FP16/FP32-Tensor-cores in Volta GPUs, it is possible to simulate the instability between plasma beams 3.5x faster.

</blurb>
    <EventParentName>MS21 Advances in Algorithms Exploiting Low Precision Floating-Point Arithmetic - Part II of II</EventParentName>
    <external_id>67713-101569</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Communication Lower Bounds for Rectangular MTTKRPS</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS74</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:15PM</starts_at>
    <ends_at>Feb 15 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Grey Ballard</EventSpeakers>
    <EventSpeakerUniqueID>735519</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101645</EventHandoutURL>
    <blurb>Our goal is to establish lower bounds on the communication required to perform the Matricized-Tensor Times Khatri-Rao Product (MTTKRP) computation on a distributed-memory parallel machine.  MTTKRP is the bottleneck computation within algorithms for computing the CP tensor decomposition, which is an approximation by a sum of rank-one tensors and frequently used in multidimensional data analysis.

We will present a communication lower bound that generalizes previous results, tightening the bound so that it is attainable even when the tensor dimensions vary (the tensor is not cubical) and when the number of processors is small relative to the tensor dimensions.  We'll also describe the communication-optimal algorithm that attains the lower bound.  Finally, we'll give highlights from the proof, which includes a novel approach based on convex optimization.</blurb>
    <EventParentName>MS74 High-Performance Tensor Computation and Applications - Part III of III</EventParentName>
    <external_id>67758-101645</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Load Balancing Strategy of Parallel Performance Portable Sparse CP-APR Decomposition</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS74</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:40PM</starts_at>
    <ends_at>Feb 15 2020  3:00PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Keita Teranishi</EventSpeakers>
    <EventSpeakerUniqueID>89317</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101646</EventHandoutURL>
    <blurb>Sparse CP-APR tensor decomposition, extended from non-negative matrix factorization, enables effective analysis of large tensors, representing count (non-negative and discrete) data.  Despite the effectiveness, its parallel implementation poses several challenges because of the increasing diversity of parallel computing architecture and irregularity of sparsity patterns of real application data.  We have addressed these issues using Kokkos parallel programming model to implement sparse CP-APR code for multiple HPC platforms.   However, Kokkos’ default load balancing and dynamic scheduling capability is lacking some flexibility to mitigate the extreme irregularity found in the tensors from real applications.  In this work, we will present our load balancing strategies of our parallel sparse CP-APR decomposition and discuss the performance and the issues of high performance parallel programming model and library support for the emerging applications.
</blurb>
    <EventParentName>MS74 High-Performance Tensor Computation and Applications - Part III of III</EventParentName>
    <external_id>67758-101646</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Scalable Kohn-Sham Matrix Algebra Solutions with the ELSI Infrastructure</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS76</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:15PM</starts_at>
    <ends_at>Feb 15 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Volker Blum</EventSpeakers>
    <EventSpeakerUniqueID>762885</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101702</EventHandoutURL>
    <blurb>This talk describes the open-source infrastructure `ELSI'
(https:// wordpress.elsi-interchange.org/ and Ref. [1]), which provides simple
access to state-of-the art solutions to the Kohn-Sham 
eigenvalue problem for different codes and solvers using a single uniform
interface. ELSI provides solutions ranging from simple
serial to large-scale massively parallel execution, with efficient
matrix conversion between dense and sparse matrix formats. Supported
solvers include ELPA (massively parallel O(N$^3$) eigenproblem
solutions), PEXSI (O(N$^2$) density-matrix based solutions including
for metallic systems), NTPoly (O(N) density matrix purification), and
several further, specialized solvers. ELSI is a cross-code
development, now used in production versions of FHI-aims, Siesta, DFTB+,
and DGDFT; additionally, ELSI is part of the broader `Electronic
Structure Library' Bundle of open-source libraries for
electronic structure theory. Different solvers have different use
scenarios in terms of system size, system type and parallelism,
assessed in a comprehensive set of benchmarks in this talk. Finally,
we outline a new reverse communication interface (RCI) enabling the
facile, efficient implementation of different iterative solver
strategies aimed at plane wave basis sets, led by ELSI project members
Yingzhou Li and Jianfeng Lu (Duke University).

[1] V. Yu \emph{et al.}, Comput. Phys. Commun. \textbf{222}, 267 (2018).


</blurb>
    <EventParentName>MS76 Parallel Eigenvalue Algorithms for Physical Simulation - Part III of III</EventParentName>
    <external_id>67781-101702</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Modern Use of Checkpoint-Restart at Large Scale using VeloC</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS79</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Bogdan Nicolae</EventSpeakers>
    <EventSpeakerUniqueID>790007</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101716</EventHandoutURL>
    <blurb>This talk presents recent advances of VeloC, a multi-level checkpoint/restart runtime for high performance computing applications that takes advantage of and delivers high performance and scalability for modern, complex heterogeneous storage hierarchies without sacrificing ease of use and flexibility. First, the talk introduces the need for checkpointing at Exascale and associated challenges. Next, it highlights the key features of VeloC: (1) exposes a simple application-level API to checkpoint and restart HPC applications based on either protecting application data structures directly or managing application-defined checkpoint files; (2) hides the complexity of interacting with the storage hierarchy (burst buffers, etc.) of current and future HPC systems; (3) has a modular design that facilitates flexibility in choosing the resilience strategies and mode of operation (synchronous or asynchronous), while being highly customizable with additional post-processing modules; (3) supports use cases beyond fault tolerance: suspend-resume jobs over multiple reservations, revisit previous states (e.g. adjoint computations), etc. Furthermore, it provides a brief overview of the techniques introduces by VeloC to implements these features. Finally, the talk concludes with examples of and early results with HPC applications that aim for Exascale and use VeloC for checkpointing, as well as exploratory scenarios where checkpointing is of interest for large-scale deep learning.
</blurb>
    <EventParentName>MS79 Resilience and Fault Tolerance for Extreme Computing Systems - Part III of III</EventParentName>
    <external_id>67785-101716</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Silent-Error Detection, Local Recovery, and Failure Masking in MPI-Based Solvers</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS79</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:40PM</starts_at>
    <ends_at>Feb 15 2020  3:00PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jackson Mayo</EventSpeakers>
    <EventSpeakerUniqueID>730991</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101850</EventHandoutURL>
    <blurb>Efficient local detection of silent data corruption in parallel scientific computations, using ``physics-based checksums", enables straightforward recovery via checkpoint/restart. We discuss advantages of solver implementations that permit purely local recovery (restarting only processes with errors) using an extension of the Fenix fault tolerance library. These techniques can improve scalability and reduce failure delays, offering some of the benefits of task-based programming models within the familiar SPMD setting. [SNL is managed and operated by NTESS under DOE NNSA contract DE-NA0003525.]

</blurb>
    <EventParentName>MS79 Resilience and Fault Tolerance for Extreme Computing Systems - Part III of III</EventParentName>
    <external_id>67785-101850</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Multi-GPU Implementation of a Second Order Scheme to Simulate Landslide-Generated Tsunamis</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Marc de la Asuncion</EventSpeakers>
    <EventSpeakerUniqueID>789961</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101986</EventHandoutURL>
    <blurb>Landslide-HySEA is a model to simulate tsunamis generated by landslides. The evolution of the landslide, the propagation of the generated tsunami and the inundation of the coastal areas are performed in a single code. It uses a second order HLL well-balanced path-conservative MUSCL-Hancock scheme that only requires one time step to evolve in time. The numerical scheme is able to preserve the stationary solutions corresponding to water at rest and it is positive preserving with the usual 0.5 CFL condition. Landslide-HySEA has been implemented in multi-GPU, and a load balancing algorithm is applied taking into account the presence of water and granular material in the domain. It also supports time series, asynchronous writing of the resulting NetCDF files, and resuming a stored simulation. Additional techniques have been implemented to increase the efficiency, such as overlapping MPI communications with GPU computations, and asynchronous memory transfers between CPU and GPU memory. Several test cases are presented along with scaling measures.

Acknowledgements: This research has been partially supported by ChEESE project (EU Horizon 2020, grant agreement Nº 823844).</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-101986</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Scalable Geometric Search Algorithms in ArborX</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS41</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 12:10PM</starts_at>
    <ends_at>Feb 14 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Andrey Prokopenko</EventSpeakers>
    <EventSpeakerUniqueID>755869</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102061</EventHandoutURL>
    <blurb>A recently developed HPC spatial indexing library ArborX implements geometric search algorithms, such as k-nearest neighbors and radius search, using a bounding volume hierarchy (BVH). ArborX is implemented using MPI+Kokkos programming model. In this talk, we focus on the distributed component of the library, and  discuss the existing challenges and algorithms. We demonstrate the scalability of the library for scientific computing applications.</blurb>
    <EventParentName>MS41 Trilinos and Hardware Independent Computing (Kokkos)</EventParentName>
    <external_id>67852-102061</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Graphblas with Performance Semantics: Specification, Design, and Implementation</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Albert-Jan Yzelman</EventSpeakers>
    <EventSpeakerUniqueID>735266</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101999</EventHandoutURL>
    <blurb>We present a hybrid shared- and distributed-memory implementation of the GraphBLAS.
Different from other recently published work,
we assign performance semantics to each GraphBLAS primitive:
the maximum amount of work, intra-process data movement, inter-process communication, and any extra memory required.
Furthermore, we define only a small set of primitives is allowed to make system calls such as for memory allocations or I/O.
Such performance semantics must balance between allowing a back-end sufficient freedom to choose the best implementation on a given system,
while remaining useful enough for an algorithm designer to optimise her end of the design space.

We follow by introducing three back-ends that comply to the defined performance semantics:
a sequential reference implementation,
a shared-memory parallel one using OpenMP, and
a distributed-memory parallel one using the Lightweight Parallel Foundations (LPF).
The latter requires a secondary back-end for intra-node computations;
a composition with the OpenMP back-end thus naturally yields a fully hybrid GraphBLAS implementation.

We will demonstrate some of our C++ interface,
discuss key implementation choices,
and show attained performance of classical graph algorithms on scales varying from phones to clusters.
Finally,
through LPF and using Spark as an example,
we demonstrate how our hybrid back-end transparently integrates with auxiliary parallel Big Data framework in a low-maintenance fashion.</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-101999</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Simulating Quantum Circuits with Tensor Network States</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yuchen Pang</EventSpeakers>
    <EventSpeakerUniqueID>790171</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102030</EventHandoutURL>
    <blurb>One challenge in simulating quantum circuits is that the memory necessary to store the qubits grows exponentially in general as the number of qubits grows. On the other hand, tensor networks provide a compact way to approximately represent quantum states with specific entanglement patterns. We develop a parallel quantum circuit simulator based on tensor network states like Projected Entangled Pair States (PEPS), and compare it with the standard state vector approach. These simulators are implemented using Cyclops Tensor Framework (CTF) that supports efficient parallel numerical operations for tensors in distributed memory. The benchmark we use includes random quantum circuits, which are considered to be generally hard to simulate. We investigate various aspects of these two simulating methods such as speed, memory cost, accuracy, scaling, etc. and discuss the tradeoff between them.
</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-102030</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Massive Scaling of MASSIF: Algorithm Development for Hooke's Law Simulations on Distributed GPU Systems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Anuva Kulkarni</EventSpeakers>
    <EventSpeakerUniqueID>785157</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102050</EventHandoutURL>
    <blurb>Seven out of the top ten supercomputers are powered by Graphic Processing Units (GPUs) and can help solve computationally intensive problems in various fields. However, adapting legacy Fortran code with large working sets and low arithmetic intensity to GPUs with small on-chip memories is a challenge due to high memory requirements and communication patterns in these simulation algorithms traditionally designed to run on CPUs or CPU clusters. One such simulation is Micromechanical Analysis of Stress-Strain Inhomogeneities with Fourier transforms (MASSIF), an iterative Hooke's law partial differential equation solver. In this work, we describe algorithm development for porting MASSIF to heterogeneous platforms. We propose a domain decomposition method with multi-resolution octree-based adaptive sampling on domain-local results to enable computation within GPU memory constraints, and ease of accumulation of results over distributed GPUs. A first-order performance model of our method estimates that compression and multi-resolution sampling strategies can enable domain computation within GPU memory constraints for 3D grids larger than those simulated by the current state-of-the-art Fortran MPI implementation. Theoretical model evaluation on (1)Single node, multi-GPU (2)DGX2 workstation (3)Summit (ORNL) provides insight into design requirements for further scalability. We also discuss potential for cross-platform high performance implementations using emerging FFT APIs like FFTX.</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-102050</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>An Interface for Extreme-Scale Geometric Multigrid in PETSc</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Johann Rudi</EventSpeakers>
    <EventSpeakerUniqueID>752522</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102069</EventHandoutURL>
    <blurb>Multigrid methods have been effective iterative solvers for linear and
nonlinear systems in a wide range of large-scale applications, due to
exhibiting a computational complexity that scales linearly (or close to
linearly) with respect to the degrees of freedom.

While algebraic multigrid methods profit from being relatively simple to
incorporate into existing domain-specific programs, they come with challenges
regarding parallel scalability.  Geometric multigrid methods access
domain-specific knowledge directly and thereby avoid certain parallel scalability
bottlenecks of algebraic variants.  However, the programming effort required by
geometric methods can be prohibitive.  
Since, the PETSc library already provides scalable algorithms for a large
community of developers and domain-specialists, it is natural to extend PETSc
by geometric multigrid capabilities.  

We propose a multigrid interface in PETSc based on hybrid
spectral--geometric--algebraic multigrid methods that have demonstrated
scalability to 1.6M compute cores.  We focus on the geometric component
utilizing the adaptive mesh refinement library, p4est, where the parallel
distribution of adapted meshes is based on space-filling curves and an octree
topology; resulting in efficient parallel mesh coarsening, refinement, and
partitioning.  The interface to geometric multigrid aims to be accessible to
domain-specialists while providing parallel scalability to leadership-class
computing systems.</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-102069</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Ginkgo - a Node-Level Sparse Linear Algebra Library for High Performance Computing</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hartwig Anzt</EventSpeakers>
    <EventSpeakerUniqueID>763986</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104654</EventHandoutURL>
    <blurb>With the rise of manycore accelerators like GPUs, there exists an increasing demand for linear algebra libraries that can efficiently exploit the concurrency and performance available in a single compute node. At the same time, more and more application projects move towards an object-oriented software design based on C++ for both efficiency and ease of use. In the Ginkgo software effort, we design and develop a next-generation sparse linear algebra library able to run on multi- and manycore architectures. The library design is guided by combining ecosystem extensibility with heavy, architecture-specific kernel optimization using the platform-native languages CUDA (NVIDIA GPUs), HIP (AMD GPUs), or OpenMP (Intel/AMD multicore). </blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104654</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>High-Performance Implementation of Wavelet Transforms Using SIMD</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Camille Coti</EventSpeakers>
    <EventSpeakerUniqueID>768211</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104698</EventHandoutURL>
    <blurb>This poster presents an implementation of wavelet transforms taking advantage of vector registers featured by modern architectures.

We are presenting quickly the ideas behind wavelet transforms, and some leads on how they can be useful for high-performance computing. Then we are presenting how data is handled in the matrices and, consequently, in the memory. We present two algorithmic approaches and how register-blocking can minimize data movements while using a reasonable number of registers. 

We are comparing and analyzing the performance obtained by these approaches and the various options proposed by SIMD instruction sets on two wavelet transforms (Haar and Daubechies 4) on two recent CPUs.

We limited ourselves to these two transforms and these two CPUs in order to keep the poster readable. We chose these two transforms because they exhibit relevant and significant algorithmic features (folding with Db4 no folding with Haar). 

We conclude that the algorithmic optimizations allowed by the SIMD registers and associated instruction sets provide very significant performance improvements.
</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104698</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Tasktorrent: A Task-Based Distributed Runtime System</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yizhou Qian</EventSpeakers>
    <EventSpeakerUniqueID>792405</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104710</EventHandoutURL>
    <blurb>We propose Tasktorrent, a task-based distributed runtime system that emphasizes an asynchronous style of programming. Tasktorrent is lightweight and uses MPI as its backend for communication. Dependencies of tasks in Tasktorrent can be conveniently expressed with a parametrized task graph. We compare Tasktorrent with other runtime systems including Legion, Starpu and multithreaded Lapack on dense Cholesky factorization. We show the scalability of Tasktorrent as a function of number of cores. Tasktorrent delivers excellent performance and scalability while not being intrusive and allowing maximum reuse of the existing code base. It’s in particular best suited for porting legacy codes to modern parallel clusters.
	</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104710</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallelized Dual-Stage Energy Minimization</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Khaled Helal</EventSpeakers>
    <EventSpeakerUniqueID>792595</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104804</EventHandoutURL>
    <blurb>InSAR time-series derived from space-borne radar satellite data can be effectively used to identify potential geotechnical risks such as landslides and abnormal subsidence. Typically, time-series analysis at full or high resolution relies on phase unwrapping to generate accurate deformation maps over large regions. A popular formulation for unwrapping congruently is based on global optimization under the L1 norm. This formulation has the benefit of being robust to large scale errors at the cost of resource-intensive computation.

As the volume of available datasets grows, more potential geotechnical risks can be caught and addressed; however, this large volume of data requires processing, with globally optimal phase unwrapping as a potential bottleneck. Previous work on this subject has focused on solving this globally optimal formulation using primal methods. In this work, we present a scalable alternative by adopting a specialized primal-dual approach, in which the problem is subdivided and parallelized into smaller problems. We show careful experiments and benchmarks to show the efficiency of this approach.
</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104804</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Gemslr: a Multilevel Low-Rank Preconditioning and Solution Package</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tianshi Xu</EventSpeakers>
    <EventSpeakerUniqueID>792583</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104850</EventHandoutURL>
    <blurb>This poster summarizes the development and implementation of GeMSLR (Generalized Multilevel Schur complement Low-Rank), a distributed-memory preconditioner for the solution of large and sparse (non)symmetric linear systems of equations. The GeMSLR preconditioner is purely algebraic and is based on a multilevel reordering of the original set of equations/variables. The reordering is implemented by hierarchically ordering the interface degrees of freedom at each level and several reordering schemes are available. At each given level, GeMSLR decouples the solution of the current linear system into one associated with the interior variables and another associated with the interface ones. The first subproblem is block-diagonal and solved in parallel by applying some form of ILU preconditioning. The recursive nature of the preconditioner appears on the second subproblem where the Schur complement linear system is preconditioned by the interface coupling matrix. The latter is applied by descending to the next level until the last level is reached. In the latter case, the user can choose to use either Block Jacobi acceleration or redundantly solve the problem by (I)LU. Low-rank correction terms can be added at each level to further enhance robustness, and these are applied using the Woodbury formula. GeMSLR is implemented in MPI. We demonstrate the potential of GeMSLR by presenting numerical tests performed on several 2D and 3D problems, and both strong and weak scaling is discussed.</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104850</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Using GPUs in Chemical Kinetics - a Stochastic Solver Within the Framework of PySB</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS49</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:45AM</starts_at>
    <ends_at>Feb 14 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>701</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Martina Prugger</EventSpeakers>
    <EventSpeakerUniqueID>789945</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101528</EventHandoutURL>
    <blurb>Chemical kinetics is the field of study that focuses on modeling the dynamics of chemical reactions. This dynamics is described by the chemical master equation (CME). The computational effort of solving the full master equation is usually avoided by approximating the system using a set of ODE that supposedly captures the underlying dynamics of the system.

Recent technology enables us to study biological cells with single-cell resolution - a level of detail, where the influence of low concentration species can no longer be neglected and therefore the approximation of ODEs is no longer sufficient.

In this talk we introduce the implementation of a stochastic simulation algorithm (SSA) introduced by Gillespie to solve the CME using GPUs. This solver is part of the PySB framework - a Python based environment that enables the user to encode the chemical network using rules that resemble the standard notation of chemical reactions.</blurb>
    <EventParentName>MS49 GPU Computing for Solving Large Scale Scientific Problems</EventParentName>
    <external_id>67664-101528</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>GPU Computing for Solving Kinetic Equations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS49</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>701</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Lukas Einkemmer</EventSpeakers>
    <EventSpeakerUniqueID>771328</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101529</EventHandoutURL>
    <blurb>Graphic processing units (GPUs) have emerged as a viable platform to solve large scale scientific problems. However, they also pose significant challenges both in terms of designing suitable algorithms and in implementing them efficiently.

In this talk we will consider the solution of kinetic equations using the SLDG code. Kinetic models are extensively used in plasma physics. They are posed in an up to six-dimensional phase space, which makes their numerical solution extremely expensive. Traditional numerical methods have a number of downsides, such as the global data interchange inherent in these methods, that makes them ill-suited for GPUs.

We describe how GPU support was incorporated into the SLDG code. Both in terms of constructing suitable algorithms for the massively parallel systems targeted (in our case we use a semi-Lagrangian discontinuous Galerkin scheme) and how to implement them efficiently on modern computer hardware. We also touch on some software architecture aspects. In particular, we will discuss the trade-off between code specific to a given hardware platform (to increase performance) and the generic implementation of the algorithm (to facilitate code reuse).
</blurb>
    <EventParentName>MS49 GPU Computing for Solving Large Scale Scientific Problems</EventParentName>
    <external_id>67664-101529</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name> Recent Development of Multigrid Solvers in HYPRE on Modern Heterogeneous Computing Platforms</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS50</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:10PM</starts_at>
    <ends_at>Feb 14 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ruipeng Li</EventSpeakers>
    <EventSpeakerUniqueID>770972</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101757</EventHandoutURL>
    <blurb>Modern many-core processors such as the graphics processing units (GPUs) are becoming an integral part of many high performance computing systems nowadays. 
These processors yield enormous raw processing power in the form of massive SIMD parallelism. Accelerating multigrid methods on GPUs has drawn a lot of research attention in recent years. 
For instance, in recent releases of the HYPRE package, the structured multigrid solvers (SMG, PFMG) have full GPU-support for both the setup and the solve phases, whereas
the algebraic multigrid (AMG) solver, namely BoomerAMG, has only its solve phase been ported and the setup can still be computed on CPUs only. 
In this talk, we will provide an overview of the available GPU-acceleration in HYPRE and present our current work on the algorithms in the AMG setup that are suitable for GPUs including the parallel coarsening algorithms, the interpolation methods and the triple-matrix multiplications. 
The recent results as well as the future work will also be included.  

</blurb>
    <EventParentName>MS50 Advances in Parallel Sparse Linear System Solver Stacks for Exascale - Part I of II</EventParentName>
    <external_id>67788-101757</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Scientific computing in a changing landscape</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS51</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Laurent White</EventSpeakers>
    <EventSpeakerUniqueID>741940</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101887</EventHandoutURL>
    <blurb>In the early 2000s, due to constraints on economical heat dissipation, clock speeds of single-core CPUs could no longer be increased, which marked the adoption of multi-core CPUs, together with a paradigm shift to algorithms specifically designed for parallel architectures. About 15 years into this architectural cycle and on its way to exascale performance, the computing industry finds itself at the confluence of technical difficulties that cast doubt on its ability to sustain this architectural model beyond the exascale capability. These difficulties are driving the hardware industry and computational scientists to develop application-specific chips and to look beyond silicon-based technology (e.g., quantum computing, physical annealing, neuromorphics, etc.), with a continued emphasis on raw processing power while addressing concerns about energy efficiency. 

Hardware specialization will likely redefine the development of computational algorithms over the next two decades for a wide range of important applications: large-scale PDE-based problems, artificial intelligence, computational chemistry, and optimization problems, to name just a few. The pressure to decrease time to solution or improve simulation fidelity, or both, for these applications will continue unabated. Sitting between hardware designers and application users, the computational science &amp; engineering community will play a pivotal role to commercialize future computing technologies.
</blurb>
    <EventParentName>MS51 Novel Computational Algorithms for Future Computing Platforms - Part I of III</EventParentName>
    <external_id>67837-101887</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Wafer-Scale Chip and System for Deep Neural Networks</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS51</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:35PM</starts_at>
    <ends_at>Feb 14 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Rob Schreiber</EventSpeakers>
    <EventSpeakerUniqueID>791055</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=103231</EventHandoutURL>
    <blurb>The compute requirement for training neural networks is growing at a steep exponential pace that far exceeds the now-slowing growth of single chip performance.  We badly need more compute for AI.  So, what comes after Moore’s Law?  If we cannot endlessly pack more transistors into one square millimeter, perhaps we should pack more square millimeters into one chip.  Large chips are faster because on-chip memory is so fast it can alleviate or remove the memory wall, and on-chip interprocessor communication is so fast it can alleviate or remove network bandwidth and latency as performance limiters.
That idea, using the whole silicon wafer for a single processor, was tried in the 1980s, and it failed then.  That was then. Now, Cerebras Systems, a venture capital funded startup company, has built a system designed around a chip 56 times larger than the largest GPU: a wafer-scale chip.  The Cerebras Wafer-Scale Engine has an architecture optimized for neural network training and inference.  It enables unprecedented performance at greater power efficiency, and opens the door to scale and methods not practical on today's machines.  I will give an overview of the system and how it is used for training neural networks.
</blurb>
    <EventParentName>MS51 Novel Computational Algorithms for Future Computing Platforms - Part I of III</EventParentName>
    <external_id>67837-103231</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>FFTW++: A Hybrid OpenMP/MPI Implementation of FFTs and Implicitly Dealiased Convolutions</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS53</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:10PM</starts_at>
    <ends_at>Feb 14 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>John Bowman</EventSpeakers>
    <EventSpeakerUniqueID>709697</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101923</EventHandoutURL>
    <blurb>Originally developed as a C++ interface to FFTW, the FFTW++ library
has been extended to provide state-of-the-art implicit dealiasing
for efficiently computing Fourier-based convolutions on serial and
parallel architectures.

For 1D inputs, the memory usage of implicitly dealiased
and conventional zero-padded convolutions are identical.
However, for large problems, implicit dealiasing is faster.
In higher dimensions, the decoupling of work buffers
results in significant memory savings, yielding better
data locality and performance.
FFTW++ uses a general multithreaded implementation of
implicit dealiasing that accepts an arbitrary number of input and
output vectors. Thread-safe Hermitian convolutions are implemented
to avoid loop dependencies. An extended data layout enhances cache
efficiency.

Implicit dealiasing of higher-dimensional convolutions over
distributed memory benefits significantly from the reduction of
communication costs associated with its smaller memory footprint.
It provides a natural way of overlapping communication with FFT
computation. Our implementation relies on an adaptive matrix
transposition algorithm optimized for distributed networks of
multicore processors. Shared memory parallelism between the
cores of a single processor is become increasing advantageous as
the number of cores per processor rises. FFTW++ has been designed to
exploit hybrid OpenMP/MPI parallelism and general slab-like
and pencil-like decompositions.

</blurb>
    <EventParentName>MS53 Parallel Sparse and Conventional FFTs, Applications and Implementation - Part II of II</EventParentName>
    <external_id>67765-101923</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Cabana – a Co-Designed Library for Particle Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS54</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Stuart Slattery</EventSpeakers>
    <EventSpeakerUniqueID>762850</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101852</EventHandoutURL>
    <EventParentName>MS54 Particle Methods: Algorithms and Software Technology for Exascale - Part I of III</EventParentName>
    <external_id>67828-101852</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Efficient Tensor Operations via Sketching </name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS55</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:35PM</starts_at>
    <ends_at>Feb 14 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yang Shi</EventSpeakers>
    <EventSpeakerUniqueID>789997</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101639</EventHandoutURL>
    <blurb>Sketching is a randomized dimensionality-reduction method that aims to preserve relevant information in large-scale datasets. Count sketch is a simple popular sketch which uses a randomized hash function to achieve compression. In this talk, we propose a novel extension known as  Higher-order Count Sketch (HCS). While count sketch uses a single hash function, HCS uses multiple (smaller) hash functions for sketching.   HCS reshapes the input (vector) data into a higher-order tensor and employs a tensor product of the random hash functions to compute the sketch. This results in an exponential saving (with respect to the order of the tensor) in the memory requirements of the  hash functions, under certain conditions on the input data. Furthermore, when the input data itself has an underlying structure in the form of various tensor representations such as the Tucker decomposition, we obtain significant advantages. We derive efficient (approximate) computation of various tensor operations such as tensor products and tensor contractions directly on the sketched data. Thus, HCS is  the first sketch to fully exploit the multi-dimensional nature of higher-order tensors. We apply HCS to tensorized neural networks where we replace fully connected layers with sketched tensor operations. We achieve nearly state of the art accuracy with significant compression on the image classification benchmark.
</blurb>
    <EventParentName>MS55 High-Performance Tensor Computation and Applications - Part I of III</EventParentName>
    <external_id>67756-101639</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>HPC Acceleration of the GeoClaw Software for Modeling Geohazards</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS56</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:45PM</starts_at>
    <ends_at>Feb 14 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Randall LeVeque</EventSpeakers>
    <EventSpeakerUniqueID>759023</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101543</EventHandoutURL>
    <blurb>The open source GeoClaw software is widely used for modeling geophysical hazards, such as tsunamis, storm surge, and overland flooding, by solving the two dimensional shallow water equations (SWE).  Adaptive mesh refinement (AMR) is incorporated to efficiently handle a wide range of length scales, e.g., from tsunami propagation across the ocean to onshore inundation modeling at the scale of meters in a single simulation.  OpenMP is supported, and together with AMR allows rapid simulation of many real-world problems on shared memory machines. Large-scale problems can benefit from further acceleration of the code, and we will report on some recent improvements of the software.  GPU acceleration has been incorporated in order to rapidly integrate the SWE on each rectangular grid patch, while AMR management and communication between patches is performed on the CPU.  The use of AMR adds several challenges to the implementation, and we discuss some issues relating to accelerating AMR codes more generally.  GeoClaw has also been containerized using Docker to simplify use on cloud computing platforms.  We will report some efforts to select optimal hardware configurations in this setting to minimize expense for large runs or for ensembles of many runs, and will show some timing comparisons on large-scale simulations.
</blurb>
    <EventParentName>MS56 HPC Aspects of Tsunami Simulation</EventParentName>
    <external_id>67714-101543</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Low-Rank Stopping Criteria for Block Parallel SVD</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS57</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:45PM</starts_at>
    <ends_at>Feb 14 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Steven Goldenberg</EventSpeakers>
    <EventSpeakerUniqueID>784625</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101722</EventHandoutURL>
    <blurb>The singular value decomposition (SVD) is one of the most commonly used low rank approximation techniques due to its optimality for all Schatten p-norms. However, most SVD algorithms only provide a residual stopping criteria at best, and at worst, stop based purely on iterations. Additionally, they require practitioners to determine the ideal rank, often without prior spectral information. We propose new stopping criteria for parallel block Lanczos methods and show their performance in both synthetic and real-world applications.
</blurb>
    <EventParentName>MS57 Parallel Eigenvalue Algorithms for Physical Simulation - Part I of III</EventParentName>
    <external_id>67779-101722</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Matrix Powers Kernels for Thick-Restart Lanczos with Explicit External Deflation</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS57</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:10PM</starts_at>
    <ends_at>Feb 14 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ichitaro Yamazaki</EventSpeakers>
    <EventSpeakerUniqueID>790005</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101723</EventHandoutURL>
    <blurb>There are continual and compelling needs for computing many eigenpairs
of very large Hermitian matrix in physical simulations and data analysis.
Though the Lanczos method is effective for computing a few eigenvalues,
it can be expensive for computing a large number of eigenvalues. This
in fact is true for most of codes developed in the past even when vast
computing resources are available. To improve the performance of the
Lanczos method, we are developing a new TRLan eigensolver. It is
an s-step thick-restart Lanczos (s-step TRLan) combined with an explicit
external deflation (EED).  The s-step Lanczos method can achieve an
order of s reduction in data movement while the EED enables to compute
eigenpairs in batches along with a number of other advantages.

We will first present an overall design of the new eigensolver,
and then focus on a specialized matrix powers kernel (MPK) for s-step
Lanczos to reduce both communication and computation costs by taking
advantage of sparse-plus-low-rank property.  Performance results will be
presented to demonstrate the potential of the special MPK and the new
TRLan eigensolver.</blurb>
    <EventParentName>MS57 Parallel Eigenvalue Algorithms for Physical Simulation - Part I of III</EventParentName>
    <external_id>67779-101723</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Resilience for Large-Scale Iterative Linear Solvers</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS59</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:10PM</starts_at>
    <ends_at>Feb 14 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Wilfried Gansterer</EventSpeakers>
    <EventSpeakerUniqueID>76854</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101720</EventHandoutURL>
    <blurb>As HPC systems grow in scale to meet increased computational demands, the incidence of faults in a given window of time is expected to grow. This issue is addressed by the scientific community with research on solutions at every computational layer.

In this presentation, we focus on algorithm-based strategies for tolerating node failures in the preconditioned conjugate gradient (PCG) method for solving large sparse linear systems. Our approach is based on exact state reconstruction, so that the solver state can be fully reconstructed if a node fails unexpectedly. In particular, we show how to support recovery from \emph{multiple\/} simultaneous or overlapping failures of several nodes for general sparsity patterns of the system matrix and how to efficiently recover from node failures \emph{without\/} the use of extra spare nodes, i.\,e., without any overhead in terms of available hardware. We also investigate the influence of the preconditioner on a trade-off between load-balancing and communication cost in the recovery phase.

Our analyses and experimental evaluations of these fault tolerant PCG algorithms illustrate that the price to be paid for the strongly improved resilience is usually small and thus this approach turns out to be a promising way to make an important iterative linear solver fault tolerant. Generalizations to other solvers are possible.



</blurb>
    <EventParentName>MS59 Resilience and Fault Tolerance for Extreme Computing Systems - Part I of III</EventParentName>
    <external_id>67783-101720</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Incorporating Hierarchical Matrix Compression and Butterfly Factorizations in a Multifrontal Lu Solver</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS60</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Pieter Ghysels</EventSpeakers>
    <EventSpeakerUniqueID>760022</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101860</EventHandoutURL>
    <blurb>In this talk we discuss recent improvements to the fast direct solvers and rank structured preconditioners implemented in STRUMPACK, a parallel multifrontal LU solver. New frontal matrix compression schemes including HOD-LR (Hierarchically Off-Diagonal Low-Rank) and HOD-BF (Hierarchically Off-Diagonal Butterfly) have been implemented. For HOD-LR, a blocked and hierarchical version of the adaptive cross approximation algorithm is used which offers better performance, robustness and scalability. For HOD-BF, the off-diagonal blocks are compressed as butterfly decompositions, which decompose a matrix as a product of sparse factors. For 3D high frequency Helmholtz problems, HOD-BF can reduce the matrix ranks even taking into account near-field interactions and hence keeps the overall sparse solver quasi-linear. We also show how the robustness and efficiency of the compression can be drastically improved by using the graph of the original sparse matrix. The new algorithms, all purely algebraic and implemented in parallel using OpenMP and MPI, are benchmarked on some large scale test problems from DOE applications.</blurb>
    <EventParentName>MS60 Low-Rank Compression-Based Fast Sparse Direct Solvers</EventParentName>
    <external_id>67831-101860</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Fast $H^2$ Algorithms for Directly Solving General Sparse Matrices</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS60</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:10PM</starts_at>
    <ends_at>Feb 14 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Dan Jiao</EventSpeakers>
    <EventSpeakerUniqueID>740134</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101868</EventHandoutURL>
    <blurb>In this work, we take advantage of the sparse linear algebra, and meanwhile develop fast ${\cal H}^2$-algorithms to accelerate all the dense matrix computations incurred during the direct solution procedure. It is challenging to develop an ${\cal H}^2$-based direct solution for sparse matrices since to take advantage of the zeros in the original matrix, we cannot treat the entire sparse matrix as an ${\cal H}^2$-matrix and directly invert or factorize it. If we do so, many more fill-ins will be introduced in the L and U factors, which makes the resultant direct solver slower than prevailing direct sparse solvers such as multifrontal methods. However, leveraging the framework of the multifrontal solver and accelerating all the dense frontal matrix computations is difficult, since every node in the elimination tree has its own structure, and it is difficult to communicate between different ${\cal H}^2$-matrices while keeping the computation nested across the elimination tree. In this work, we successfully overcome this challenge and develop a series of fast  ${\cal H}^2$ algorithms to reduce the complexity of directly solving a sparse matrix. Numerical results will be shown to demonstrate its performance.

</blurb>
    <EventParentName>MS60 Low-Rank Compression-Based Fast Sparse Direct Solvers</EventParentName>
    <external_id>67831-101868</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Vendor-Optimized vs. Portable Performance: Approaches to Get Both</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS23</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Karl Rupp</EventSpeakers>
    <EventSpeakerUniqueID>752016</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101882</EventHandoutURL>
    <blurb>Scientific software should run at maximum performance on any hardware.
Reaching this noble ideal can be simplified by reusing well-tuned low-level routines provided by hardware vendors.
However, these vendor libraries only provide good performance on the vendor's hardware.
As such, scientific software has to either interface different vendor libraries, or provide portable performance by itself.
If no vendor-tuned implementations of a particular functionality exist, a strategy for achieving portable performance is required anyway.

On the one hand, this talk surveys available strategies and experience on providing portable performance.
On the other hand, a software library approach is presented that acts as an intermediate wrapper to provide a single, unified interface for several vendor-optimized linear algebra libraries.
This resolves the burden of interfacing many different vendor libraries; instead, only the intermediate wrapper needs to be interfaced, while preserving the full performance benefit of vendor libraries tuned for the particular hardware available.

</blurb>
    <EventParentName>MS23 Advances and Challenges in Solvers on GPGPU-based High-Performance Computing Architectures - Part I of II</EventParentName>
    <external_id>67833-101882</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Optimization of SpMV on GPU for Iterative Solvers in PETSc</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS23</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:45AM</starts_at>
    <ends_at>Feb 13 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hong Zhang</EventSpeakers>
    <EventSpeakerUniqueID>776638</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101885</EventHandoutURL>
    <blurb>Large sparse systems resulted from the discretization of partial differential
equations often need to be solved by iterative linear solvers due to scalability
and memory constraints. Typically the computation of sparse matrix-vector (SpMV)
products is the workhorse of iterative linear solvers. There are several
optimized general-purpose SpMV implementations available, such as Intel MKL,
CUSPARSE, CUSP, and ViennaCL. However, to use these libraries, explicit
conversion between different matrix format may be required, which hampers the
performance. Thus it is desirable and beneficial to have a native implementation
of SpMV in the solver that can be optimized to exploit the characteristics of a
hardware platform (e.g. memory bandwidth, thread-level parallelism) as well as
the characteristics of the application problem (e.g. the block structure,
sparsity pattern). In this talk, I will present our recent work on optimizing
the Sliced Ellpack (SELL) matrix class in PETSc on NVIDIA GPUs. SELL was
previously designed for multicore CPUs with long SIMD and tailored to the needs
of PETSc iterative solvers and preconditioners. I will show how we extend SELL
for better vectorization on GPU  and compare the performance of SELL with other
options supported by PETSc (ViennaCL and CUSPARSE).

</blurb>
    <EventParentName>MS23 Advances and Challenges in Solvers on GPGPU-based High-Performance Computing Architectures - Part I of II</EventParentName>
    <external_id>67833-101885</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Using the PETSc/TAO ADMM Methods on GPUs</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS23</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 12:10PM</starts_at>
    <ends_at>Feb 13 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Todd Munson</EventSpeakers>
    <EventSpeakerUniqueID>704203</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101886</EventHandoutURL>
    <blurb>The Alternating Direction Method of Multipliers (ADMM) is an algorithm for structured numerical optimization problems that solves a sequence of subproblems that update the variables and the multipliers.  The variables can typically be further subdivided and updated in parallel.  In this talk, we explore using the ADMM code in the Toolkit for Advanced Optimization for solving structured regression problems on GPUs and provide some initial performance results.</blurb>
    <EventParentName>MS23 Advances and Challenges in Solvers on GPGPU-based High-Performance Computing Architectures - Part I of II</EventParentName>
    <external_id>67833-101886</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Taxonomy of Distributed and Parallel Languages for High Performance Tasks-Based Multilevel Computing</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS25</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:45AM</starts_at>
    <ends_at>Feb 13 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Serge Petiton</EventSpeakers>
    <EventSpeakerUniqueID>789960</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101562</EventHandoutURL>
    <blurb>Extreme scale supercomputer programming often mix coarse-grained distributed programming and parallel programming. Task based programming paradigms based on graph of task/components are proposed to compute and exchange data on those large number of core machines. Scheduling these tasks, and associated communications, is critical to obtain performance and minimize energy consumptions. It is necessary to extract the most expertise from the users in order to optimize these criteria. Then, language describing these task graphs must be well-designed and adapted to end-user programming skills. Moreover, each task-component may have several levels of parallel programming itself. Therefore, we have to propose adapted interfaces between the task graph languages and the parallel languages used in the tasks. On the other hand, pre-existing parallel software may be used as tasks and tasks may be developed using different languages. 
In this talk, we present experiments and analyses of some block linear algebra methods using several languages proposed for such programming on a cluster of multi-core processors. We propose results with respect to several parameters such as : the number of tasks, the number of cores per tasks, the size of the matrices and the number of blocks. We propose a taxonomy tentative for such languages and we discuss how large applications may be developed using these programming paradigms, in particular in geoscience.


</blurb>
    <EventParentName>MS25 Progress and Challenges in Extreme Scale Computing and Big Data - Part I of II</EventParentName>
    <external_id>67709-101562</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>On Performance Portability - Application and Illustration on MC Neutron Transport Application</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS25</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:20AM</starts_at>
    <ends_at>Feb 13 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christophe Calvin</EventSpeakers>
    <EventSpeakerUniqueID>733175</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101563</EventHandoutURL>
    <blurb>Today and future supercomputer architecture are more an more complex and heterogeneous. It is now quite impossible to optimize a full application on all the different architectures. Moreover, having portions of code dedicated to specific architecture is a major concern regarding maintenance and portability of the full application.
Portability of performances is thus a way to explore for large scientific application, managing a tradeoff between code portability, maintenability and performances.
We illustrate this approach on a Monte Carlo Neutron Transport code developped at CEA. A partial offload version of the code has been developed via several programming models (OpenMP thread, OpenMP offload, OpenACC and CUDA) on different architectures (x86, Power and GPU).
Moreover we have tried to define and use preformance-portbaility metrics to compare the different implementations.
</blurb>
    <EventParentName>MS25 Progress and Challenges in Extreme Scale Computing and Big Data - Part I of II</EventParentName>
    <external_id>67709-101563</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Direct Parallel Visualization Using Forest-of-Octrees Meshes</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS27</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 12:10PM</starts_at>
    <ends_at>Feb 13 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Carsten Burstedde</EventSpeakers>
    <EventSpeakerUniqueID>723667</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101690</EventHandoutURL>
    <blurb>The visualization of simulation data is a primary scalability challenge arising in large-scale numerical simulation.  Writing data to disk for subsequent post-processing is slow and often entirely impracticable.  Interfacing and linking to third-party visualization libraries has become fairly popular, but usually requires to duplicate and reformat the simulation data for the library, often losing the advantage of native acceleration data structures.  We are thus investigating parallel algorithms that natively support fast mesh refinement and partitioning as well as efficient in-situ visualization, in our case based on a forest-of-octrees design, and present the current state of our research.</blurb>
    <EventParentName>MS27 Challenges in Parallel Adaptive Mesh Refinement - Part I of III</EventParentName>
    <external_id>67770-101690</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>NUMO: A Non-Hydrostatic Unified Model of the Ocean</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS28</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Michal Kopera</EventSpeakers>
    <EventSpeakerUniqueID>786024</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101836</EventHandoutURL>
    <blurb>Ice-sheet/ocean interaction in Greenland is one of the key outstanding challenges in climate modeling, yet present-day climate models are not able to resolve fine-scale processes in the fjords. This is due to orders of magnitude difference in spatial scales between the open ocean (1000km) and fjord (1km) as well as small-scale processes at the glacier terminus (1m), complicated bathymetry and coastline. The most recent computational studies of fjord circulation focused on simplified box-like domains (e.g., [Kimura et al., \textit{The effect of meltwater plumes on the melting of a vertical glacier face}, 2014]).
\vspace{10pt}

We develop the Non-hydrostatic Unified Model of the Ocean (NUMO) to address the need for hi-resolution simulations of ocean circulation in Greenland fjords which take into account complex geometry and the range of scales of motion. The model uses fully three-dimensional unstructured mesh and high-order element-based Galerkin methods to discretize the Boussinesq approximation of incompressible Navier-Stokes equations.
\vspace{10pt}

In this talk, we will discuss the validation of the model on a range of test cases with increasing complexity and present recent developments and challenges. We will focus on the parallel performance of the model.





</blurb>
    <EventParentName>MS28 High-Performance Numerics and Model Development for Geophysical Systems - Part I of II</EventParentName>
    <external_id>67822-101836</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Recent Developments in Hybridisation Techniques for Finite Element Problems in Numerical Weather Prediction</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS28</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:45AM</starts_at>
    <ends_at>Feb 13 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jack Betteridge</EventSpeakers>
    <EventSpeakerUniqueID>790020</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101838</EventHandoutURL>
    <blurb>For problems in Numerical Weather Prediction (NWP), time to solution is a critical factor. Semi-implicit time-stepping methods can speed up geophysical fluid dynamics simulations by taking larger time-steps than explicit methods. This is possible because they treat the fast waves implicitly, and the time-step size is not restricted by the CFL condition for these waves. This method requires an expensive linear solve that must be performed at every time-step, however, an effective preconditioner can significantly reduce the computational cost of this solve, making a semi-implicit scheme faster overall.

Finite element methods are often difficult to precondition due to the large number of coupled degrees of freedom. Hybridisation methods can eliminate this coupling and instead couple the equations to a smaller global system on the trace space, which is easier to precondition.  This is achieved by considering variables which only lie on the facets of the mesh. The resultant trace system can be solved using multigrid instead of directly, allowing us to run high resolution simulations. Hybridisation is possible in both a conforming finite element setting and in a discontinuous Galerkin setting.

We demonstrate the effectiveness of a multigrid preconditioner for a semi-implicit IMEX time-stepper. The method is implemented in the Slate language, which is part of the Firedrake project. Firedrake is a Python framework for solving finite element problems via code generation.</blurb>
    <EventParentName>MS28 High-Performance Numerics and Model Development for Geophysical Systems - Part I of II</EventParentName>
    <external_id>67822-101838</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Extreme-Scale Task-Based Cholesky Factorization Toward Climate and Weather Prediction Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS30</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hatem Ltaief</EventSpeakers>
    <EventSpeakerUniqueID>735518</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101870</EventHandoutURL>
    <blurb>Climate and weather can be predicted statistically via geospatial
Maximum Likelihood Estimates (MLE), as an alternative to running
large ensembles of forward models. The MLE-based iterative
optimization procedure requires the solving of large-scale linear
systems that performs a Cholesky factorization on a symmetric
positive-definite covariance matrix—a demanding dense factorization
in terms of memory footprint and computation. We propose
a novel solution to this problem: at the mathematical level, we reduce
the computational requirement by exploiting the data sparsity
structure of the matrix off-diagonal tiles by means of low-rank
approximations; and, at the programming-paradigm level, we integrate
PaRSEC, a dynamic, task-based runtime to reach unparalleled
levels of efficiency for solving extreme-scale linear algebra matrix
operations. The resulting solution leverages fine-grained computations
to facilitate asynchronous execution while providing a flexible
data distribution to mitigate load imbalance. Performance results
are reported using 3D synthetic datasets up to 42M geospatial locations
on 130, 000 cores, which represent a cornerstone toward fast
and accurate predictions of environmental applications.
</blurb>
    <EventParentName>MS30 Accelerating Data Sparse Applications on Massively Parallel Systems - Part I of III</EventParentName>
    <external_id>67825-101870</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Clustering Techniques and Hierarchical Matrix Formats for Scalable Kernel Ridge Regression</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS30</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:45AM</starts_at>
    <ends_at>Feb 13 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Xiaoye  Li</EventSpeakers>
    <EventSpeakerUniqueID>65840</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101872</EventHandoutURL>
    <blurb>We present scalable and memory-efficient algorithm to approximate kernel
methods for machining learning. Namely, we exploit the sub-block
rank deficiency of the kernel matrices to build fast direct
solvers to compute the regression weights during the training stage of
the kernel methods. Our method results in optimal $O(r^2 n)$ training time
using hierarchical matrix algebra combined with the geometry information
of the data, where $r$ is the maximum off-diagonal rank.
The accuracy of our method matches the
state-of-the-art non-approximated kernel regression, and our parallel
code can effectively work with datasets of millions of data points.

</blurb>
    <EventParentName>MS30 Accelerating Data Sparse Applications on Massively Parallel Systems - Part I of III</EventParentName>
    <external_id>67825-101872</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Structured Abstractions in MLIR: High-Level Infrastructure for Optimizing Matrix/Tensor Computations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS31</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:45AM</starts_at>
    <ends_at>Feb 13 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mahesh Ravishankar</EventSpeakers>
    <EventSpeakerUniqueID>792497</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101920</EventHandoutURL>
    <blurb>MLIR is a new SSA-based compiler infrastructure designed for extensibility.

It allows one to easily define intermediate representations at various levels of abstraction from tensorflow ops operating on tensor values all the way down to control-flow graphs operating scalar and vector SSA values.

This talk proposes a composable and structured abstraction for code generation in MLIR. This abstraction aims at building tractable end-to-end flows for high-performance optimization of matrix and tensor operations. </blurb>
    <EventParentName>MS31 Frameworks/Libraries for High-Performance Tensor Computations - Part II of II</EventParentName>
    <external_id>67795-101920</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Next Generation Cyberinfrastructure for Science: Cyberinfrastructure Center of Excellence Pilot for Large Facilities</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS32</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:10PM</starts_at>
    <ends_at>Feb 13 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>4000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ewa Deelman</EventSpeakers>
    <EventSpeakerUniqueID>774696</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101753</EventHandoutURL>
    <EventParentName>MS32 Transparency, Reproducibility, Sustainability, and Security: The Four Pillars of the Next Generation Scientific Software Stack</EventParentName>
    <external_id>67792-101753</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Quantum Computing -- Overview &amp; Potential Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS62</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 11:00AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Gilad Ben-Shach</EventSpeakers>
    <EventSpeakerUniqueID>790051</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101892</EventHandoutURL>
    <blurb>Quantum Computing leverages the physics of quantum mechanics to enable a new approach to computation. Recently, the technology has moved from academic research labs to the cloud, where people can access real quantum hardware from around the globe. Although quantum computers are not useful for business applications today, there is hope that continued research will enable them to help solve complex problems in the future. This talk will cover an introduction to what quantum computing is (and isn't), an exploration of potential applications, and an overview of what IBM is doing to help drive the field forward.</blurb>
    <EventParentName>MS62 Novel Computational Algorithms for Future Computing Platforms - Part II of III</EventParentName>
    <external_id>67838-101892</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Quantum Circuit Synthesis using Linear Algebra and Optimization Algorithms</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS62</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:30AM</starts_at>
    <ends_at>Feb 15 2020 11:50AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Marc Baboulin</EventSpeakers>
    <EventSpeakerUniqueID>770172</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102012</EventHandoutURL>
    <blurb>Programming a quantum co-processor relies on the description of a quantum circuit which performs a series of elementary operations acting on the quantum memory to generate the final desired state.
We propose two methods that optimize the synthesis of quantum circuits in two ways.
The first method is based on Householder transformations and minimizes the classical resources required (flops, time) for the synthesis.
The second method minimizes for some type of circuits the quantum resources (number of gates, number of qubits) using a numerical optimization algorithm.
We present experimental results in the simulation of circuit synthesis using current CPU and GPU devices and we address the tradeoff between the two types of resources optimization.
	
</blurb>
    <EventParentName>MS62 Novel Computational Algorithms for Future Computing Platforms - Part II of III</EventParentName>
    <external_id>67838-102012</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Data-Centric Operating Systems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS63</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:05AM</starts_at>
    <ends_at>Feb 15 2020 11:25AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>801</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Arthur Mccabe</EventSpeakers>
    <EventSpeakerUniqueID>780335</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101764</EventHandoutURL>
    <EventParentName>MS63 Data-Centric Operating Systems and Runtimes</EventParentName>
    <external_id>67775-101764</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Aml: Building Blocks for Memory Management</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS63</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:30AM</starts_at>
    <ends_at>Feb 15 2020 11:50AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>801</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Swann Perarnau</EventSpeakers>
    <EventSpeakerUniqueID>790017</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101765</EventHandoutURL>
    <EventParentName>MS63 Data-Centric Operating Systems and Runtimes</EventParentName>
    <external_id>67775-101765</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MFIX-Exa: An Exascale CFD-DEM Model for Reactor Design Engineering</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS64</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:05AM</starts_at>
    <ends_at>Feb 15 2020 11:25AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jordan Musser</EventSpeakers>
    <EventSpeakerUniqueID>760910</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101858</EventHandoutURL>
    <blurb>Commercial deployment of gas-solids reactors requires an understanding of how to scale laboratory designs of multiphase systems to industrial sizes. However, the direct scale up of such reactors is known to be unreliable, and the current approach necessitates building and testing physical systems at increasingly larger intermediate scales. CFD-DEM (computational fluid dynamics – discrete element method) offers an accurate way to model gas-solids flows and could reduce the number of physical devices that need to be built and tested thereby helping control costs and reduce risk.\Because CFD-DEM tracks individual particles and resolves particle-particle-wall collisions, simulations are computationally expensive, limiting applications to small reactors containing, at most, tens of millions of particles. To address the computational and data management challenges of modeling industrially relevant devices with CFD-DEM, a new code called MFIX-Exa is being developed by migrating the core hydrodynamic models from the widely used, open-source code MFIX into the AMReX framework. Parallel performance of the code has been improved by employing a dual grid approach for fluid and particle load balancing, modernizing the fluid solution algorithm, and extending kernels to run on GPU. To handle complex reactor geometries, AMReX embedded boundary (EB) data structures and iterators were incorporated and an algorithm to address particle collisions with EB walls was implemented.
</blurb>
    <EventParentName>MS64 Particle Methods: Algorithms and Software Technology for Exascale - Part II of III</EventParentName>
    <external_id>67829-101858</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Lagrangian Particle Methods for Exascale Simulation of  Dilute Sprays in Combustion Systems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS64</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:30AM</starts_at>
    <ends_at>Feb 15 2020 11:50AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Wenjun Ge</EventSpeakers>
    <EventSpeakerUniqueID>790040</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101859</EventHandoutURL>
    <blurb>The Lagrangian-Eulerian (LE) method is widely used for simulations of fuel sprays in turbulent combustion because of its discrete treatment of the spray droplets. We present a performance-portable library, Grit, that has been developed to simulate dilute spray evolution in preparation for exascale capable computers. The tracking of the evaporating fuel droplets represented by the Lagrangian particles are coupled to direct numerical simulations (DNS) of turbulent flows on Eulerian meshes. Grit employs the Message Passing Interface (MPI) for distributed memory parallelism and Kokkos programming model for on-node shared memory parallelism. We present details of the software implementation for the Lagrangian physics kernels and numerical operators, along with performance on the pre-exascale Summit system. 

</blurb>
    <EventParentName>MS64 Particle Methods: Algorithms and Software Technology for Exascale - Part II of III</EventParentName>
    <external_id>67829-101859</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Exascale Molecular Dynamics with Cabana: from Lennard-Jones to Neural Network Potentials</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS64</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:55AM</starts_at>
    <ends_at>Feb 15 2020 12:15PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Samuel Reeve</EventSpeakers>
    <EventSpeakerUniqueID>790041</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101861</EventHandoutURL>
    <blurb>In order to effectively drive scientific discovery with tools like classical molecular dynamics in the current high-performance computing era, performance portable solutions are required. To achieve this, we use the Co-design center for Particle Applications (CoPA) Cabana particle library which, i) is built on Kokkos for on-node parallelism on various hardware, ii) provides performant particle-centric functionality for both MPI communication and neighbor search, and iii) enables optimization of data structure for a given architecture through arrays-of-structs-of-arrays (AoSoA), intermediate between arrays-of-structs and structs-of-arrays. To explore the performance of Cabana we develop the CabanaMD proxy app and test it with both the Lennard-Jones model, the near 100 year-old benchmark kernel, and a newly developed neural network potential, a kernel with near-quantum level accuracy and significantly higher computational cost. With these substantially different kernels we focus on the impact of algorithmic, data layout/access, and communication decisions on performance and scaling across hardware, including many-core CPU, among them ARM and AMD, and NVIDIA GPU. 

Work performed under the auspices of the U.S. DOE by LLNL under contracts DE-AC52-07NA27344 and supported by the Exascale Computing Project (17-SC-20-SC), a collaborative effort of the U.S. DOE Office of Science and the NNSA.</blurb>
    <EventParentName>MS64 Particle Methods: Algorithms and Software Technology for Exascale - Part II of III</EventParentName>
    <external_id>67829-101861</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Autotuning Exascale Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS66</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 11:00AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Osni Marques</EventSpeakers>
    <EventSpeakerUniqueID>75645</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101599</EventHandoutURL>
    <blurb>This presentation summarizes the main features of GPTune, an autotuning framework that relies on multitask and transfer learning to find optimal performance parameters of a kernel or an application that are treated as a black-box functions. We show that the framework can be more suitable than some state-of-the-art frameworks for the tuning of any general application and, in particular, expensive large-scale applications. To illustrate, we provide comparative results with state-of-the-art autotuning techniques, for a set of solvers and linear algebra computations used by a range of applications. GPTune is part of the xSDK effort supported by DOE's Exascale Computing Project (ECP).

</blurb>
    <EventParentName>MS66 New Approaches for Software Auto-Tuning and Accuracy Assurance - Part I of II</EventParentName>
    <external_id>67742-101599</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallel Computation of Many Eigenvalues by S-Step Thick-Restart Lanczos Algorithm with Explicit External Deflation - II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS67</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 11:00AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Zhaojun Bai</EventSpeakers>
    <EventSpeakerUniqueID>700481</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101697</EventHandoutURL>
    <blurb>
There are continual and compelling needs for computing many eigenpairs
of very large Hermitian matrix in physical simulations and data analysis.
Though the Lanczos method is effective for computing a few eigenvalues,
it can be expensive for computing a large number of eigenvalues. This
in fact is true for most of codes developed in the past even when vast
computing resources are available. To improve the performance of the
Lanczos method, we are developing a new TRLan eigensolver. It is
an s-step thick-restart Lanczos (s-step TRLan) combined with an explicit
external deflation (EED).  The s-step Lanczos method can achieve an order
of s reduction in data movement while the EED enables to compute eigenpairs
in batches along with a number of other advantages.

In this talk, we will focus on stability analysis of the new TRLan
eigensolver and show the parallel computation of many eigenvalues
for physical simulations in electronic structure calculations and
dynamics analysis of supramolecular systems.

</blurb>
    <EventParentName>MS67 Parallel Eigenvalue Algorithms for Physical Simulation - Part II of III</EventParentName>
    <external_id>67780-101697</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Library to Accelerate Stencil Codes on Vector Processors</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS68</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:55AM</starts_at>
    <ends_at>Feb 15 2020 12:15PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>1900</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Arihiro Yoshida</EventSpeakers>
    <EventSpeakerUniqueID>790085</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101974</EventHandoutURL>
    <blurb>One of most costly computational kernels in seismic imaging is "stencil code", which iteratively updates each element in a multidimensional grid by referring to the neighbor elements. Since it frequently appears also in other wide variety of scientific simulations, image processing, deep learning, and so on, it is quite an important issue to make stencil codes perform faster. As a solution, we developed a library that can accelerate stencil codes on vector processors, which is one of most suitable hardware architectures for processing a massive amount of data with high performance. Stencil codes load a value of each element several times while they store a new value once. Hence, for gaining performance, it is required to reduce memory load access. We optimized stencil code computation by a combination of two techniques. The first one is loop blocking, which is a well-known technique to utilize a cache efficiently. The second one is a unique technique using vector registers like a high-speed cache by elaborate loop structure deformation. Every vector register can have hundreds of element values and can be accessed much faster than a cache. The library running on vector processors shows significant performance compared to stencil codes running on commonly used scalar processors.</blurb>
    <EventParentName>MS68 Advanced HPC Trends Oil and Gas Applications - Part I of II</EventParentName>
    <external_id>67846-101974</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Teaching Parallel and Distributed Computing Concepts in Simulation</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS69</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:05AM</starts_at>
    <ends_at>Feb 15 2020 11:25AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Henri Casanova</EventSpeakers>
    <EventSpeakerUniqueID>735541</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101739</EventHandoutURL>
    <blurb>Teaching topics related to high performance, parallel and/or distributed
computing in a hands-on manner is challenging, especially at introductory,
undergraduate levels. There is a participation challenge due to the need to
secure access to a platform on which students can learn via hands-on
activities, which is not always possible. There are also pedagogic
challenges as particular platforms provided to students impose constraints
on which learning objectives can be achieved hands-on.  These challenges
become steeper as the topics being taught target more heterogeneous, more
distributed, and/or larger platforms, as needed to prepare students for
using and developing Cyberinfrastructure.

In this presentation we will elaborate on the above challenges, and report
on successful experiences with using simulation as a pedagogic tool in the
classroom for teaching the above topics.  These experiences relied on
hands-on, simulation-driven, pedagogic modules developed with the SimGrid
and the WRENCH simulation frameworks.  These modules can be integrated
piecemeal into existing university courses, and we will explain how they
have contributed to students achieving learning objectives in specific
undergraduate and graduate computer science courses taught at the
University of Hawai`i at Manoa.</blurb>
    <EventParentName>MS69 The Many Faces of Simulation for HPC - Part I of II</EventParentName>
    <external_id>67786-101739</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name> Extreme-Scale Scientific Software Stack (E4S)</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS71</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sameer Shende</EventSpeakers>
    <EventSpeakerUniqueID>716126</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101698</EventHandoutURL>
    <blurb>The DOE Exascale Computing Project (ECP) Software Technology focus area is developing an HPC software ecosystem that will enable the efficient and performant execution of exascale applications. Through the Extreme-scale Scientific Software Stack (E4S) [https://e4s.io], it is developing a comprehensive and coherent software stack that will enable application developers to productively write highly parallel applications that can portably target diverse exascale architectures. E4S provides both source builds through the Spack platform and a set of containers that feature a broad collection of HPC software packages. E4S exists to accelerate the development, deployment, and use of HPC software, lowering the barriers for HPC users. It provides container images, build manifests, and turn-key, from-source builds of popular HPC software packages developed as Software Development Kits (SDKs). This effort includes a broad range of areas including programming models and runtimes (MPICH, Kokkos, RAJA, OpenMPI), development tools (TAU, HPCToolkit, PAPI), math libraries (PETSc, Trilinos), data and visualization tools (Adios, HDF5, Paraview), and compilers (LLVM), all available through the Spack package manager. It will describe the community engagements and interactions that led to the many artifacts produced by E4S. It will introduce the E4S containers are being deployed at the HPC systems at DOE national laboratories using Singularity, Shifter, and Charliecloud container runtimes.</blurb>
    <EventParentName>MS71 Toward Efficient Software Integration and Deployment</EventParentName>
    <external_id>67782-101698</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Adaptive Local Timestepping and its Parallelization</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS72</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:15PM</starts_at>
    <ends_at>Feb 15 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Max Bremer</EventSpeakers>
    <EventSpeakerUniqueID>790055</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101901</EventHandoutURL>
    <blurb>Systems of conservation laws model a large cross section of scientific applications of interest. Explicit timestepping schemes must satisfy the famous CFL stability condition. However, in the presence of large variations of wavespeed or mesh size, existing timestepping schemes can be overly conservative. Local timestepping is an adaptive timestepping technique, whereby regions of the mesh stably advance with varying timestep sizes. Verifying that correctness is enforced in a distributed context requires evaluating a task graph that cannot be determined statically. We propose a novel adaptive local timestepping scheme that has been formulated as a discrete event simulation that guarantees the CFL constraint is satisfied. Important features of this work are (1) a formal correctness proof using loop invariants that guarantees the correctness of this algorithm in a highly asynchronous execution context, and (2) the use of an optimistic (i.e. speculative) discrete event simulator to efficiently parallelize the algorithm. Preliminary scaling results will outline the performance compared to a flat MPI implementation using synchronous timestepping. We expect the techniques used to develop this timestepping scheme to have broad crossover to algorithm design in highly asynchronous settings.</blurb>
    <EventParentName>MS72 Novel Computational Algorithms for Future Computing Platforms - Part III of III</EventParentName>
    <external_id>67839-101901</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Stable Automatic Tuning Method for Performance Fluctuation</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS75</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:40PM</starts_at>
    <ends_at>Feb 15 2020  3:00PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Naoto Seki</EventSpeakers>
    <EventSpeakerUniqueID>790094</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101605</EventHandoutURL>
    <blurb>Automatic tuning (AT) is a technique to search for optimum parameter value settings of a user program. The execution time of a program depends on the value of a given parameter, but execution time for a single parameter setting can vary from one run to the next. We call this fluctuation. In our tests, the fluctuations were found to depend on the execution condition of the computing environment. The bad influence of fluctuation on parameter optimization must be mitigated. Thus, our goal is set to consider fluctuations and realize a stable AT.

We researched the iterative one-dimensional (1D) search that can search good parameter with a little search cost. The iterative 1D search is based on an approximation function (d-Spline). The d-Spline function was chosen because it flexibly follows the measured data with small calculation cost.

To realize a stable AT, we extend the conventional iterative 1D search. The proposed method calculates the improvement rate of the estimated best parameter after each 1D parameter search, and starts the multiple-time measurements of a single parameter value after the estimated best parameter is expected to nearly optimum. Moreover, after the iterative 1D search is ended, it keeps and updates average execution time of each parameter setting, and choses the parameter setting with shortest execution time.

In our numerical evaluation, the proposed method significantly reduced the total execution time in comparison with conventional methods.</blurb>
    <EventParentName>MS75 New Approaches for Software Auto-Tuning and Accuracy Assurance - Part II of II</EventParentName>
    <external_id>67743-101605</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Parallel Strategy for Kohn-Sham Solver with GPU-Accelerated Nodes</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS76</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jean-Luc Fattebert</EventSpeakers>
    <EventSpeakerUniqueID>712771</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101701</EventHandoutURL>
    <blurb>We consider the problem of solving the Kohn-Sham eigenvalue problem using a very accurate numerical discretization such as finite differences, finite elements or plane waves (pseudo-spectral) on a distributed memory architecture with very fast nodes, such as GPU-accelerated nodes. Traditionally, wave functions would be distributed among nodes, while matrices expressing operators in the subspace spanned by trial wave functions would be distributed using a library such as ScaLapack. 
We investigate how the increasing performance gap between floating point operations and off-node communications affects this strategy. We use a proxy-app to investigate performance and develop an effective strategy for strong scaling of problems with O(500-10,000) wave functions on the Summit supercomputer at the Oak Ridge Leadership Computing Facility (OLCF).

Research sponsored by the Laboratory Directed Research and Development Program of Oak Ridge National Laboratory (ORNL), managed by UT-Battelle, LLC for the U. S. Department of Energy under Contract No. De-AC05-00OR22725. This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725. 
</blurb>
    <EventParentName>MS76 Parallel Eigenvalue Algorithms for Physical Simulation - Part III of III</EventParentName>
    <external_id>67781-101701</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallel Randomized Pivoted QR Factorizations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS35</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:10PM</starts_at>
    <ends_at>Feb 13 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jianlin Xia</EventSpeakers>
    <EventSpeakerUniqueID>732378</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101946</EventHandoutURL>
    <blurb>We show the design of a parallel low-rank compression algorithm based on randomized pivoting and rank-revealing QR factorization. Randomized pivoting ensures both the efficiency and the reliability of the rank-revealing factorization. A hierarchical strategy is used to enhance both the scalability and stability. This is joint work with Xin Ye.</blurb>
    <EventParentName>MS35 Parallel Matrix Factorization Algorithms - Part II of III</EventParentName>
    <external_id>67848-101946</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Tasks Unlimited : Lightweight Task Offloading and Replication for Parallel Adaptive Mesh Refinement</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS37</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:10PM</starts_at>
    <ends_at>Feb 13 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Philipp Samfass</EventSpeakers>
    <EventSpeakerUniqueID>780382</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101683</EventHandoutURL>
    <blurb>As we move towards exascale, new challenges arise: performance becomes less predictable due to variability at runtime and hard faults are more and more likely to occur. Besides, numerical algorithms such as the ADER-DG method pose challenges with respect to load balancing. In this work --- in an effort to prepare AMR frameworks to future exascale systems --- we propose to unleash tasks into distributed memory: i.e., tasks are no longer bound to an MPI process but they can dynamically migrate between processes. We discuss how this relaxed distributed tasking can be exploited for load balancing and fault tolerance.
</blurb>
    <EventParentName>MS37 Challenges in Parallel Adaptive Mesh Refinement - Part II of III</EventParentName>
    <external_id>67771-101683</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Quantum Approximate Optimization with a Trapped-Ion Quantum Simulator</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS39</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:10PM</starts_at>
    <ends_at>Feb 13 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Guido Pagano</EventSpeakers>
    <EventSpeakerUniqueID>789999</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101657</EventHandoutURL>
    <blurb>Quantum computers and simulators may offer significant advantages over their classical counterparts, providing insights into quantum many-body systems and possibly solving exponentially hard problems, such as optimization and satisfiability. We report the first implementation of a shallow-depth Quantum Approximate Optimization Algorithm (QAOA) using an analog quantum simulator to estimate the ground state energy of the transverse field Ising model with tunable long-range interactions. First, we exhaustively search the variational control parameters to approximate the ground state energy with up to 40 trapped-ion qubits. We then interface the quantum simulator with a classical algorithm to more efficiently find the optimal set of parameters that minimizes the resulting energy of the system. We finally sample from the full probability distribution of the QAOA output with single-shot and efficient measurements of every qubit.</blurb>
    <EventParentName>MS39 Recent Advances and Trends in Hybrid Quantum-Classical Algorithms - Part II of II</EventParentName>
    <external_id>67763-101657</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Fast quantum subroutines for the simplex method</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS39</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:35PM</starts_at>
    <ends_at>Feb 13 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Giacomo Nannicini</EventSpeakers>
    <EventSpeakerUniqueID>736504</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101658</EventHandoutURL>
    <blurb>We propose quantum subroutines for the simplex method that avoid 
classical computation of the basis inverse. For a well-conditioned 
$m \times n$ constraint matrix with at most $d_c$ nonzero elements 
per column, at most $d$ nonzero elements per column or row of the 
basis, and optimality tolerance $\epsilon$, we show that pricing can 
be performed in time $\tilde{O}(\frac{1}{\epsilon}\sqrt{n}(d_c n + 
d^2 m))$, where the $\tilde{O}$ notation hides polylogarithmic 
factors. If the ratio $n/m$ is larger than a certain threshold, the 
running time of the quantum subroutine can be reduced to 
$\tilde{O}(\frac{1}{\epsilon}d \sqrt{d_c} n \sqrt{m})$. Classically, 
pricing would require $O(d_c^{0.7} m^{1.9} + m^{2 + o(1)} + d_c n)$ 
in the worst case. We also show that the ratio test can be 
performed in time $\tilde{O}(\frac{t}{\delta} d^2 m^{1.5})$, where 
$t, \delta$ determine a feasibility tolerance; classically, this 
requires $O(m^2)$ in the worst case. For well-conditioned sparse 
problems the quantum subroutines scale better in $m$ and $n$, and 
may therefore have a worst-case asymptotic advantage. 
This asymptotic speedup does not depend 
on the data being available in some ``quantum form': the input of 
our quantum subroutines is the natural classical description of the 
problem, and the output is the index of the variables that should 
leave or enter the basis.</blurb>
    <EventParentName>MS39 Recent Advances and Trends in Hybrid Quantum-Classical Algorithms - Part II of II</EventParentName>
    <external_id>67763-101658</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Fast Algorithms for Hierarchical Matrices</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS40</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:45PM</starts_at>
    <ends_at>Feb 13 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>George Biros</EventSpeakers>
    <EventSpeakerUniqueID>731287</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101875</EventHandoutURL>
    <EventParentName>MS40 Accelerating Data Sparse Applications on Massively Parallel Systems - Part II of III</EventParentName>
    <external_id>67826-101875</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Hierarchical Matrix Algorithms on Manycore Architectures</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS40</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:10PM</starts_at>
    <ends_at>Feb 13 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>George  Turkiyyah</EventSpeakers>
    <EventSpeakerUniqueID>769524</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101876</EventHandoutURL>
    <blurb>In this talk we describe parallel algorithms of core linear algebra operations on hierarchical matrices optimized for GPUs and multi GPU systems. We consider hierarchical matrix-vector multiplication operations, low rank updates and recompression, construction of a hierarchical matrix approximation of matrices that may be available only via matrix-vector products, as well matrix multiplication through randomized sampling. Our algorithms rely on flattening the trees of the hierarchical representations to allow level by level distribution and processing, and the effective use of batched dense linear algebra kernels QR and randomized SVD kernels that we have also developed.Results on single and multi GPU systems show the high performance of our implementations on both compute bound and memory bound operations.

</blurb>
    <EventParentName>MS40 Accelerating Data Sparse Applications on Massively Parallel Systems - Part II of III</EventParentName>
    <external_id>67826-101876</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Solving Acoustic Boundary Integral Equations using High Performance Tile Low-Rank LU Factorization</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS40</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:35PM</starts_at>
    <ends_at>Feb 13 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Rabab Omairy</EventSpeakers>
    <EventSpeakerUniqueID>785466</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=103614</EventHandoutURL>
    <blurb>We design and develop a new numerical solver for non-symmetric matrices based on a fast direct LU factorization on parallel systems.  The LU factorization is the first and most time-consuming step toward solving systems of linear equations in the context of analyzing acoustic scattering from large 3D objects.  The matrix equation is obtained by discretizing the boundary integral of the exterior Helmholtz problem using a higher-order Nystr\”{o}m scheme.  The main idea is to exploit the inherent data sparsity of the matrix operator by performing local approximation while capturing the most significant information.  In particular, the proposed LU-based solver leverages the Tile Low-Rank
(TLR) data compression format as implemented in the Hierarchical Computations on Manycore Architectures (HiCMA) library to decrease the cubic complexity of ``classical' dense direct solvers. This requires to taskify the underlying boundary integral kernels to expose fine-grained computations. We then employ the dynamic runtime system StarPU to orchestrate the computational tasks onto the underlying hardware resources. The resulting asynchronous execution permit to weaken the artifactual synchronization points, while mitigating the overhead of data motion, especially when tackling large-scale problems. The new TLR HiCMA LU factorization outperforms the state-of-the-art dense factorizations on various parallel systems.


</blurb>
    <EventParentName>MS40 Accelerating Data Sparse Applications on Massively Parallel Systems - Part II of III</EventParentName>
    <external_id>67826-103614</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>State of the Tpetra Linear Solver Stack</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS41</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:45AM</starts_at>
    <ends_at>Feb 14 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christopher Siefert</EventSpeakers>
    <EventSpeakerUniqueID>723261</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102056</EventHandoutURL>
    <blurb>The Trilinos/Tpetra linear solver stack, including MueLu, Ifpack2, Belos and Amesos2 offers the potential to efficiently solve large, sparse linear systems on both conventional (e.g. CPU systems) and advanced (e.g. many-core or GPU systems) architectures.  Built using components from Kokkos and KokkosKernels, offers portability across a wide range of architectures.  This talk will give an overview of the capabilities available in the Tpetra linear solver stack and present performance results on both CPU and GPU architectures.</blurb>
    <EventParentName>MS41 Trilinos and Hardware Independent Computing (Kokkos)</EventParentName>
    <external_id>67852-102056</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Views on Software Sustainability from a Computing Facility Perspective</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS42</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Judith Hill</EventSpeakers>
    <EventSpeakerUniqueID>734253</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101781</EventHandoutURL>
    <blurb>As large-scale computing resources become more complex in many aspects---including the node architecture, the memory hierarchy, the interconnect, and the tiered storage systems---software portability between architectures at different facilities and between generations of machines within a facility can be a challenge for application developers.  Since the transition from homogeneous to heterogeneous node architectures, the Oak Ridge Leadership Computing Facility (OLCF) has been collaborating with application developers to ensure that their developers employ best practices for portability and maximize the application developer productivity to ensure the sustainability of software applications.  
\linebreak

During this talk, I will discuss the productivity lessons learned and best practices developed by OLCF computational scientists and application developers over the past ten years.  Additionally, I will also discuss the broader DOE Exascale Computing Project (ECP) productivity project, IDEAS-ECP, as the DOE applications community looks ahead to the sustainability challenges in the exascale era.



</blurb>
    <EventParentName>MS42 Improving Productivity and Sustainability for Parallel Computing Software - Part I of II</EventParentName>
    <external_id>67772-101781</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The Design of FFTX</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS43</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Franz Franchetti</EventSpeakers>
    <EventSpeakerUniqueID>726040</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101660</EventHandoutURL>
    <blurb>We present the design of FFTX and provide a first look at SpectralPack. These software packages are developed as part of the DOE ExaScale effort by LBL, Carnegie Mellon University, and SpiralGen, Inc. We aim at translating the LAPACK/BLAS approach from the numerical linear algebra world to the spectral algorithm domain. FFTX is extending and updating FFTW for the exascale era and beyond while providing backwards compatibility. SpectralPack captures higher level spectral algorithms and their variants, including convolutions, Poisson solvers, correlations, and numerical differentiation approaches that translate to FFT calls. The SPIRAL code generation and autotuning system--now available as open source under a BSD/Apache license--underpins the effort to provide performance portability.
 </blurb>
    <EventParentName>MS43 Parallel Sparse and Conventional FFTs, Applications and Implementation - Part I of II</EventParentName>
    <external_id>67764-101660</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Hierarchical Algorithms on Hierarchical Architectures</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS44</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:45AM</starts_at>
    <ends_at>Feb 14 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>David Keyes</EventSpeakers>
    <EventSpeakerUniqueID>8363</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101983</EventHandoutURL>
    <blurb>Some algorithms achieve optimal arithmetic complexity but have low arithmetic intensity. Others possess high arithmetic intensity but lack optimal complexity. A special group of algorithms, Fast Multipole and its H-matrix generalizations, realizes a combination of optimal complexity and high intensity. Hierarchically low-rank linear algebra is bringing about a renaissance in linear algebra, offering data sparsity to problems formally defined as dense, and thus significantly increasing the range of problem sizes that can be accommodated in (among others) integral equations, covariance matrices in statistics, and Hessians in optimization. Implemented with task-based dynamic runtime systems, these hierarchical methods also have potential for relaxed synchrony, which is important for future energy-austere architectures, since there may be significant nonuniformity in processing rates of different cores even if task sizes can be controlled. 

We describe modules of KAUST's Hierarchical Computations on Manycore Architectures (HiCMA) software toolkit that illustrate these features and are intended as building blocks of more sophisticated applications, such as matrix-free higher-order methods in optimization. HiCMA's target is hierarchical algorithms on emerging architectures, which have hierarchies of their own that generally do not align with those of the algorithm. Some modules of this open source project have been adopted in the software libraries of major vendors. </blurb>
    <EventParentName>MS44 Parallel Matrix Factorization Algorithms - Part III of III</EventParentName>
    <external_id>67849-101983</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>AMReX: A Block-Structured AMR Software Framework for the Exascale</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS46</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ann Almgren</EventSpeakers>
    <EventSpeakerUniqueID>713700</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102008</EventHandoutURL>
    <blurb>AMReX is a software framework for the development of block-structured AMR  algorithms on current and future architectures.  AMR reduces the computational cost and memory footprint compared to a uniform mesh while preserving the essentially local descriptions of different physical processes in complex multiphysics algorithms. AMReX supports a number of different time-stepping strategies and spatial discretizations, and incorporates data containers and iterators for mesh-based fields, particle data and irregular embedded boundary (cut cell) representations of complex geometries. Current AMReX applications include accelerator design, additive manufacturing, astrophysics, combustion, cosmology, microfluidics, materials science and multiphase flow. In this talk I will focus on AMReX's strategy for balancing readability, usability, maintainability and performance across multiple applications and architectures.




</blurb>
    <EventParentName>MS46 Challenges in Parallel Adaptive Mesh Refinement - Part III of III</EventParentName>
    <external_id>67857-102008</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>An Asynchronous Algorithm for 2:1 Octree Balance</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS46</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 12:10PM</starts_at>
    <ends_at>Feb 14 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hansol David Suh</EventSpeakers>
    <EventSpeakerUniqueID>790138</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102054</EventHandoutURL>
    <blurb>Adaptive mesh refinement (AMR) is a key technology in many simulations, allowing for locally increased resolution without the cost of a globally refined regular mesh.  The p4est library, which takes a forest-of-quadtrees (2D) or forest-of-octrees (3D) approach to AMR, has been demonstrated to be highly scalable for large scale applications.  In particular, its implementation of the mesh adaptivity cycle---all steps between marking cells for coarsening or refinement and readiness to resume computation---has demonstrated parallel scalability beyond 100,000 MPI processes.Previous studies of the performance of p4est have foregrounded weak scalability, with a large number of octree leaves per MPI process.  This work focuses on the strong scalability of the mesh adaptivity cycle.  Improving the strong scalability and reducing the latency of the mesh adaptivity cycle reduces the time-to-solution of existing simulations and allows for more frequent adaptivity within runtime constraints.We have identified the communication pattern of p4est's \emph{2:1-balance} algorithm---which enforces a size constraint between adjacent leaves in a partitioned octree by refining those that are too coarse---as a latency bottleneck.  We compare the existing "two round" algorithm for this problem to a new ``local-sort' algorithm and a simple global sort algorithm, comparing on the TACC Stampede2 supercomputer.</blurb>
    <EventParentName>MS46 Challenges in Parallel Adaptive Mesh Refinement - Part III of III</EventParentName>
    <external_id>67857-102054</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Revisiting the Jacobi Method for Eigen Problems in Computational Chemistry</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS76</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  3:05PM</starts_at>
    <ends_at>Feb 15 2020  3:25PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hua Huang</EventSpeakers>
    <EventSpeakerUniqueID>790006</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101727</EventHandoutURL>
    <blurb>Jacobi methods for symmetric eigenvalue problems have attracted attention because of their inherent parallelism. In this work, we study the application of Jacobi methods in electronic structure calculations.  In particular, we consider how the number of Jacobi sweeps could be reduced in certain situations.  We also discuss the potential regimes for which the Jacobi algorithm could be preferred over the QR algorithm, in terms of problem size and number of processing units.</blurb>
    <EventParentName>MS76 Parallel Eigenvalue Algorithms for Physical Simulation - Part III of III</EventParentName>
    <external_id>67781-101727</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Faithful Performance Prediction of a Dynamic Task-Based Runtime System, an Opportunity for Task Graph Scheduling</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS78</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Samuel Thibault</EventSpeakers>
    <EventSpeakerUniqueID>752486</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101735</EventHandoutURL>
    <blurb>Obtaining the maximum performance of heterogeneous machines is challenging as it requires to carefully offload computations and manage data movements between different processing units. Task-based runtime systems allow to abstract the question and rely on opportunistic scheduling algorithms. The problem then gets shifted to choosing a task granularity and task graph structure, and optimizing scheduling strategies.

Trying different combinations of these different alternatives is however a challenge by itself. Indeed, getting accurate measurements requires reserving the target system for the whole duration of experiments. Furthermore, observations are limited to the few available systems at hand and may be difficult to generalize.

In this talk, I will describe how we have crafted a coarse-grain hybrid simulation/emulation of StarPU, a dynamic runtime for hybrid architectures, over SimGrid, a versatile simulator for distributed systems. This approach allows to obtain performance predictions of classical dense linear algebra kernels accurate within a few percents and in a matter of seconds, which allows both runtime and application designers to quickly decide which optimization to enable or whether it is worth investing in higher-end GPUs or not. Additionally, it allows to conduct robust and extensive scheduling studies in a controlled environment whose characteristics are very close to real platforms while having reproducible behavior.
</blurb>
    <EventParentName>MS78 The Many Faces of Simulation for HPC - Part II of II</EventParentName>
    <external_id>67787-101735</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Application-Simulation Co-Design for Performance and Correctness Evaluation</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS78</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:40PM</starts_at>
    <ends_at>Feb 15 2020  3:00PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Luigi Genovese</EventSpeakers>
    <EventSpeakerUniqueID>758374</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101796</EventHandoutURL>
    <blurb>For large-scale HPC applications, the search space of possible optimization techniques is usually vast. It is common to try several algorithmic alternatives to relax synchronizations to allow applications to opportunistically use resources, or to rely on automatic scheduling and load-balancing techniques. However, (1) the respective efficiency of such approaches is difficult to assess and highly depends on the target system; and (2) the corresponding implementations are quite complex and thus error-prone. It is not uncommon in dynamic and highly optimized approaches to obtain efficient implementations that suffer from rare non-deterministic deadlocks or failstop errors which are extremely difficult to narrow and debug. Additionally, direct experiments provide only limited experimental control, hindering the reproducibility of experiments.

Simulation is an appealing alternative to study the performance and the correctness of such systems. In this talk, we explain how partially porting the BigDFT application on top of SimGrid simulation toolkit led us to very promising results. We present the co-design approach in which modifications were done to the scientific application and SimGrid to enable and ease its simulation in various experimental conditions. If successful, this approach will represent a radical epistemic shift; the developer will be able to test and validate the performance of an exascale run without the need for a huge amount of computational resources.</blurb>
    <EventParentName>MS78 The Many Faces of Simulation for HPC - Part II of II</EventParentName>
    <external_id>67787-101796</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Space-Efficient Reed-Solomon Encoding to Detect and Correct Pointer Corruption</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS79</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  3:05PM</starts_at>
    <ends_at>Feb 15 2020  3:25PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Scott Levy</EventSpeakers>
    <EventSpeakerUniqueID>785489</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101728</EventHandoutURL>
    <blurb>Concern about memory errors has been widespread in HPC for decades. These concerns have led to significant research on detecting and correctingmemory errors to improve performance and to provide strong guarantees about the correctness of the memory contents of scientific simulations.  However, power concerns and changes in memory architectures threaten the continued 
viability of current approaches to protecting memory (e.g., Chipkill). Returning to a less protective error-correcting code, e.g., SECDED, may increase the frequency of memory errors, including silent data corruption (SDC).  SDC has the potential to silently cause applications to produce incorrect results and mislead domain scientists.


In this presentation, we provide a detailed description of how we can exploit unnecessary pointer bits to store Reed-Solomon parity symbols.  We evaluate the performance impacts of this approach and examine the effectiveness of the approach against corruption.  Our results demonstrate that encoding and decoding is fast (less than 45 us per event) and that the protection it provides is robust (the rate of miscorrection is less than 5\% even for significant corruption).


</blurb>
    <EventParentName>MS79 Resilience and Fault Tolerance for Extreme Computing Systems - Part III of III</EventParentName>
    <external_id>67785-101728</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Combinatorial Optimization on Quantum Computers </name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MT1</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  2:40PM</ends_at>
    <EventFilter>PP20|Minitutorial</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605 </location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ilya Safro</EventSpeakers>
    <EventSpeakerUniqueID>726604</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101807</EventHandoutURL>
    <blurb>Quantum computing has potential to efficiently solve combinatorial optimization problems. Recent advances in both hardware and algorithm development have made it possible to solve small problems on modern quantum computers. Combinatorial optimization problems (especially NP-hard problems) are of particular interest, since for many of these problems efficient classical algorithms are not known. One such problem is MaxCut on graphs. In this minitutorial, we will introduce the MaxCut problem and explain how it can be solved on IBM quantum computers available on the cloud today using the Qiskit framework. Our presentation will assume little to no prior knowledge of quantum computation. Moreover, we will provide examples of how more complicated problems can be solved using the QAOA (Quantum Approximate Optimization Algorithm). The tutorial will be interactive and will use Jupyter notebooks to explain step by step how to formulate and implement algorithms in the Ising Hamiltonian form.

	</blurb>
    <EventParentName>MT1 Combinatorial Optimization on Quantum Computers</EventParentName>
    <external_id>67813-101807</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>To Be Defined</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS78</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  3:05PM</starts_at>
    <ends_at>Feb 15 2020  3:25PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>  TBD</EventSpeakers>
    <EventSpeakerUniqueID>770766</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101797</EventHandoutURL>
    <blurb>TBD</blurb>
    <EventParentName>MS78 The Many Faces of Simulation for HPC - Part II of II</EventParentName>
    <external_id>67787-101797</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallel Fenics Implementation of Block Solvers</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Innokentiy Protasov</EventSpeakers>
    <EventSpeakerUniqueID>786703</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101635</EventHandoutURL>
    <blurb>The FEniCS Project is a popular and sophisticated open-source software library for conducting general-purpose finite element simulations. Once the variational formulations are assembled, FEniCS relies on other scientific libraries like PETSc to solve the discretized linear and nonlinear systems of equations. However, when it comes to mixed finite element formulations, neither parallel direct solvers nor simple implementations of Krylov methods with standard preconditioning techniques will be sufficient to solve large-scale problems. Instead, one needs to split them into blocks and apply said preconditioning techniques to them individually. This paper presents an overview of an open-source software library called pFibs: a parallel FEniCS implementation of Block Solvers, which provides a FEniCS interface to PETSc's composable block solver capabilities. The software enables users to perform nested block preconditioning, provide their own variational form for the preconditioner, and interface directly with Dolfin-Adjoint. We outline the essential pieces of information PETSc requires from the FEniCS Project, give examples on how to utilize each feature in pFibs, and present some scalability results. Our examples have shown that pFibs is indeed a viable way to solve mixed finite element formulations constructed from the FEniCS Project.
</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-101635</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Fast Generation of Extreme-Scale Matrices with Preassigned 2-Norm Condition Number</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Massimiliano Fasi</EventSpeakers>
    <EventSpeakerUniqueID>767240</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102011</EventHandoutURL>
    <blurb>Generating benchmark problems to test numerical linear algebra algorithms at scale presents new challenges. The test problems must be inexpensive to generate, yet large enough to saturate the computational capabilities of the supercomputer being tested. In many cases, being able to tune some parameters of the test problem may also be required. When assessing the performance of iterative solvers for linear systems, for instance, one typically desires to generate square matrices with a preassigned 2-norm condition number. For small-scale problems, such a matrix can be constructed from its singular value decomposition, as it is well known that the 2-norm condition number is the ratio of the largest to the smallest singular value. This approach, however, requires a dense matrix-matrix multiplication, an operation that has cubic cost and is communication-intensive in a distributed setting, and can quickly become too costly as the size of the problem grows. We propose a new method to generate extremely large matrices with preassigned 2-norm condition number. In order to produce a matrix of order $n$, our technique requires only $\mathcal O(n^2)$ floating point operations and minimal communication between the nodes in the cluster.
</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-102011</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Massively-Parallel Computing of Multi-Channel 2D Convolution</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Stanislav Sedukhin</EventSpeakers>
    <EventSpeakerUniqueID>729537</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102033</EventHandoutURL>
    <blurb>Convolutional neural networks (CNNs) are among the most powerful and widely used algorithms for computer vision applications. The primary computationally-demanding parts of CNNs are the convolutional layers, which convolve multi-channel images with multiple kernels. As a result, the different acceleration techniques for fast and efficient CNN computing are rapidly growing in popularity. In this work, the extreme acceleration technique for massively-parallel fine-grained computing of multi-channel 2D convolutions is proposed and evaluated. This technique uses a 4D processing space to speed-up an implementation of multi-dimensional convolution which is originally represented in the  6D index/iteration space. Unlike other algorithms in the existing CNN accelerators, the proposed algorithm computes the required convolution by parallel execution on each time-step all the permissible multiply-accumulate (MAC) operations combined with a systolic-like massively-parallel data movement to resolve data dependencies. A required by multi-channel convolution data reduction is computed by proper “chaining” of MAC operations. Our implementation radically reduces the number of time-steps needed to compute a convolutional layer.  Under real-time constrains to execute a given CNN model, such deep reduction of the time-steps  can be effectively used for lowering operating frequency in the algorithm’s hardware implementation and, as a consequence, diminishing the power and energy consumption. 
	</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-102033</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Hardware Agnostic Implementation of Cosmo-Eulag Dynamical Core for Regional Numerical Weather Prediction using GridTools Framework</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Zbigniew Piotrowski</EventSpeakers>
    <EventSpeakerUniqueID>740566</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=103350</EventHandoutURL>
    <blurb>One of the major computational challenges for the developers of weather prediction models is to efficiently exploit modern supercomputing architectures. This opportunity is particularly attractive due to the higher memory bandwidth and lower total cost of ownership of an energy-efficient cluster. While implementations of particular components of the weather frameworks for manycore architectures were reported decade ago, it is uncommon to exploit these ports in the operational service. A notable example is the COSMO framework for regional weather prediction in Europe. In this case, operational computations have been performed for several years using fat-node GPU solution. This was enabled thanks to the rewritten or adapted codebase using family of DSLs embedded in C++, namely: STELLA and GridTools to compute atmospheric dynamics, as well as OpenACC directives to accelerate physics.
To address numerical difficulties influencing operational COSMO high resolution runs over topography of the Alps, novel dynamical core based on EULAG research model for multiscale flows was implemented within the COSMO. To enable efficient COSMO-EULAG computations on GPUs and multicore processors, EULAG components were ported into the recently announced initial stable Gridtools release. Within this poster presentation we document the process of porting of legacy Fortran 2003 codebase to the C++/GridTools, along with software engineering, energetic and performance considerations on CPU and GPU.</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-103350</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Additively Damped AFAC Variants for High Order Discretisations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Charles Murray</EventSpeakers>
    <EventSpeakerUniqueID>791933</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104243</EventHandoutURL>
    <blurb>Multigrid algorithms are among the best solvers for elliptic partial differential equations. However, small coarse grid problems create a bottleneck that limits multiplicative multigrid's concurrency. Additive multigrid decouples coarse grid solves from those on finer levels. This increases the concurrency but may reduce stability as the number of grid levels grows.
Duplicated corrections across levels tend to make iterations overshoot. Instead of damping coarse grids - which harms convergence - we follow up on the idea of AFACx and introduce an additional, level-specific damping equation that approximates corrections from the next coarsest level to reduce overcorrection. Notably, this additional correction space uses smoothed transfer operators. Stability is further increased through BoxMG transfer operators and arbitrary dynamic AMR is achieved through the use of HTMG and FAS. We increase the arithmetic load slightly yet obtain a stabilised additive, i.e. parallelisable multigrid code. Time-to-solution is improved further as we don't assemble the full matrix and instead hold approximations to the matrix within the mesh. The multigrid scheme kicks off with low accuracy stencils guiding the smoother in the right direction, while tasks in the background improve stencil quality. This scheme makes the solver of relevance for complex material configurations or high order discretisations where the construction of proper stencils, in particular in an AMR context, is not cheap.</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104243</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Stable Automatic Tuning Method for Performance Fluctuation and Evaluation</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Naoto Seki</EventSpeakers>
    <EventSpeakerUniqueID>790094</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104665</EventHandoutURL>
    <blurb>Automatic tuning (AT) is a technique to search for optimum parameter value settings of a user program. The execution time for a single parameter setting can vary from one run to the next. We call this fluctuation. In our tests, the fluctuations were found to depend on the execution condition of the computing environment. The bad influence of fluctuation on parameter optimization must be mitigated. Thus, our goal is set to consider fluctuations and realize a stable AT. 

We researched the iterative one-dimensional (1D) search that can search good parameter with a little search cost. The iterative 1D search is based on an approximation function (d-Spline). The d-Spline function was chosen because it flexibly follows the measured data with small calculation cost.
To realize a stable AT, we extend the conventional iterative 1D search. The proposed method calculates the improvement rate of the estimated best parameter after each 1D parameter search, and starts the multiple-time measurements of a single parameter value after the estimated best parameter is expected to nearly optimum. Moreover, after the iterative 1D search is ended, it keeps and updates average execution time of each parameter setting, and choses the parameter setting with shortest execution time. 

In our numerical evaluation, the proposed method significantly reduced the total execution time in comparison with conventional methods.
The contents of this poster will be presented at MS75.



</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104665</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>EquationBC: Solving a Different PDE on Boundary</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Koki Sagiyama</EventSpeakers>
    <EventSpeakerUniqueID>792342</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104680</EventHandoutURL>
    <blurb>Firedrake (https://www.firedrakeproject.org/download.html) is a finite element analysis software for solutions of partial differential equations that takes a high-level mathematical representation of a problem and generates/compiles efficient low-level codes that run in parallel.

In this poster presentation we introduce a recently added feature of Firedrake to apply boundary conditions in equation form using \emph{EquationBC} class.
Designed to be a generalisation of DirichletBC class in Firedrake for applying Dirichlet boundary conditions, EquationBC class allows users to force degrees of freedom on boundary to satisfy some partial differential equations.
This generalisation becomes useful when, e.g., solving problems that have distinct physics in the domain and on the boundary.
Examples include ocean wave modelling in which we solve the Navier-Stokes equation in the domain and the wave equation on the boundary. Here, we will illustrate the concept and the use of EquationBC using simple examples.</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104680</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Construction of High-Order Multirate Imex Integrators for Large-Scale Complex Multiphysics Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Rujeko Chinomona</EventSpeakers>
    <EventSpeakerUniqueID>784953</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104701</EventHandoutURL>
    <blurb>Multiphysics simulations for example in models of the early universe or in climate modeling tend to be large-scale and involve complex dynamics. Part of these complex dynamics can be disparities in temporal scales for different physical processes. In addition to robust and high performing frameworks, such multiphysics simulations can greatly benefit from high-order multirate schemes in terms of accuracy and efficiency. The focus of this study is to construct (the first of their kind) high-order multirate time integrators that evolve the slow dynamics with an implicit-explicit (IMEX) scheme and the fast dynamics with either an implicit or explicit time integrator. These new multirate integrators are designed for problems that have a structure similar to advection-diffusion-reaction systems. The slow dynamics (advection and diffusion) which also tend to be the most communication intensive are evolved with a small number of large time steps. Furthermore, the advection is treated in an explicit fashion and the diffusion in an implicit fashion. The fast dynamics (reactions) are evolved with a smaller time step. Our hypothesis is that our high-order multirate integrators will achieve the same accuracy as other integration schemes faster while taking large time steps for the slow dynamics. Computation and communication costs are therefore greatly reduced for the slow dynamics.

	</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104701</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>SIAG Best Paper Prize:  The BLIS Framework: Experiments in Portability</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>SP1</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  8:30AM</starts_at>
    <ends_at>Feb 14 2020  9:15AM</ends_at>
    <EventFilter>PP20|Prize Speaker</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Robert van de Geijn</EventSpeakers>
    <EventSpeakerUniqueID>712582</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=103314</EventHandoutURL>
    <blurb>Over the past decade, the BLAS-like Library Instantiation Software
(BLIS) project has carefully revisited past progress on how to structure the implementation of the level-3 BLAS-like operations (matrix-matrix
computations) in particular and all basic linear algebra operations in general. The paper ``The BLIS Framework: Experiments in Portability" 
demonstrates how a refactoring of prior approaches yields a more flexible, more easily maintained, highly portable, yet high-performing and scalable software library.

Casting level-3 BLAS functionality in terms of multiplication with submatrices was proposed in the works of Kågström, Ling, and Van Loan. 
This led to efforts to auto-generate and tune such as PHiPAC and ATLAS. 
A dramatic breakthrough came circa 2000 when Goto proposed ``Goto's algorithm" (now at the heart of most high-performance BLAS) for implementing matrix-matrix multiplication.

BLIS casts Goto's algorithm in terms of five portable loops (written in
C99) around a ``microkernel" that updates a small submatrix of C that fits in registers. It is only this microkernel that needs to be customized for a new architecture when implementing matrix multiplication. The refactoring exposed in BLIS drastically reduced the size, complexity, and number of assembly kernels necessary for supporting high-performance across all datatypes and level-3 operations.

The prize-winning paper was coauthored with Tyler Smith, Bryan Marker, Tze Meng Low, Francisco Igual, Mikhael Smelyanskiy, Xianyi Zhang, Michael Kistler, Vernon Austel, John Gunnels, and Lee Killough. </blurb>
    <EventParentName>SP1 SIAG Best Paper Prize:  The BLIS Framework: Experiments in Portability</EventParentName>
    <external_id>68266-103314</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T08:30:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Applications of Parareal to Geo/astrophysical Fluid Dynamics: Convection and Magnetic Field Generation</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS47</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 12:10PM</starts_at>
    <ends_at>Feb 14 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Andrew Clarke</EventSpeakers>
    <EventSpeakerUniqueID>784886</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101772</EventHandoutURL>
    <blurb>
The precise mechanisms responsible for the natural dynamos in the Earth and Sun are still not fully understood. Numerical simulations which couple the flow of a conducting fluid with magnetic effects, using the equations of magnetohydrodynamics (MHD), are used to investigate the dynamo effect. These simulations are extremely computationally intensive, and are carried out in parameter regimes many orders of magnitude away from real conditions. 

Parallelization in space is a common strategy to speed up simulations on high performance computers (HPCs), but eventually a scaling limit is reached due to increasing overheads from communication. Additional directions of parallelization are desirable to improve utilisation of HPCs.

We have used the spectral code Dedalus to study the performance of the parallel-in-time algorithm Parareal when applied to simulations of geo/astrophysical fluid dynamics. Specifically, we look at the kinematic dynamo problem, which concentrates on magnetic field generation, and Rayleigh-B\’enard convection, in which the characteristics of the fluid flow are investigated.

Results for kinematic dynamo studies show that parareal offers increased performance for both the Roberts and Galloway-Proctor dynamos, when compared to parallel in space methods alone. Galloway-Proctor simulations showed speed ups of 300 when using 1600 cores. Early Rayleigh B\’enard convection results shows speed ups are possible up to at least $Ra=10^6$</blurb>
    <EventParentName>MS47 Parallel-in-Time Integration Methods - Part I of II</EventParentName>
    <external_id>67796-101772</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Data Movement Orchestration in Accelerator-Rich Systems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS51</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:45PM</starts_at>
    <ends_at>Feb 14 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Andreas Gerstlauer</EventSpeakers>
    <EventSpeakerUniqueID>790049</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101889</EventHandoutURL>
    <blurb>With the end of traditional technology scaling, accelerator-rich computer architectures have emerged as a primary means to achieve continued performance gains. A wide variety of specialized accelerators have been developed for a range of applications, including linear algebra, scientific computing and machine learning. While it is relatively well understood how to accelerate computations, integration of a large number of heterogeneous accelerators into an overall system architecture has become the main challenge. Specifically, if not carefully orchestrated, the costs of offloading computations, feeding accelerators and moving data between different computational elements on or off chip can quickly outweigh the benefits of acceleration. In this talk, we will discuss data movement challenges as well various software and hardware solutions to minimize offloading and data movement costs. This includes an outlook on how future large-scale system architectures that incorporate near-memory processing to exploit data and compute locality will have to be treated as distributed systems both from a programming and runtime management perspective.


</blurb>
    <EventParentName>MS51 Novel Computational Algorithms for Future Computing Platforms - Part I of III</EventParentName>
    <external_id>67837-101889</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Experiences with Productivity and Software Sustainability on LCF Machines</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS52</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:35PM</starts_at>
    <ends_at>Feb 14 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Katherine Riley</EventSpeakers>
    <EventSpeakerUniqueID>790004</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101788</EventHandoutURL>
    <blurb>The Argonne Leadership Computing Facility was created 13 years ago to provide access to computational and data resources that high-impact science and engineering researchers would not be able to access anywhere else.  To perform this mission, the ALCF has deployed four supercomputers that were among the largest systems of their time.  Readiness to use these platforms is challenging.  Application teams need the agility to adapt to new bleeding-edge hardware, develop new science capability, and manage large science campaigns that generate complex datasets.  
\linebreak
 
Through these 13 years, there have been patterns for the approach of some of the most success research groups and their application codes.  I will discuss how projects have sustained their applications and evolved their software engineering and management practices, and I will touch on practices that were not as effective.  Practices are also evolving as hardware complexity is increasing.  I will discuss how projects are collaborating with the ALCF to improve their software engineering practices and how the ALCF is changing to better support these needs.







</blurb>
    <EventParentName>MS52 Improving Productivity and Sustainability for Parallel Computing Software - Part II of II</EventParentName>
    <external_id>67773-101788</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Challenges and Best Practices in the Computational Molecular Sciences</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS52</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:10PM</starts_at>
    <ends_at>Feb 14 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Benjamin Pritchard</EventSpeakers>
    <EventSpeakerUniqueID>790003</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101955</EventHandoutURL>
    <blurb>The area of computational molecular science (CMS) encompasses a diverse range of fields, including quantum chemistry, molecular dynamics, and materials science. All of these fields rely on software and computing resources to carry out simulations.
\linebreak
 
Like many scientific fields, CMS faces many challenges related to software development. Some challenges arise from the mismatch of incentives between the desire for novel, career-progressing research and the need for reliable, well-written software. Other obstacles are purely technical -- not only does CMS contain large legacy codebases that are still widely used, there also exists a sizable but fragmented software ecosystem.
\linebreak
 
While many of these difficulties are similar to those found in other scientific fields, CMS has its own unique challenges. As the HPC community rapidly moves to heterogeneous architectures, some fields of CMS risk being left behind due to the inability to adapt not only code but algorithms and methods to the new HPC resources.
\linebreak
 
As part of an investigation into the HPC field, the Molecular Sciences Software Institute (MolSSI) has conducted many interviews with researchers, developers, and users of HPC resources. This talk will review the results of these interviews, as well as what we consider to be the overarching themes we see in the community. What we consider to be best practices, and how these relate to HPC software development, will be also be discussed.

</blurb>
    <EventParentName>MS52 Improving Productivity and Sustainability for Parallel Computing Software - Part II of II</EventParentName>
    <external_id>67773-101955</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Implementing some Strategies to Reduce Communication Time of FFT Algorithm</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS53</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Samar Aseeri</EventSpeakers>
    <EventSpeakerUniqueID>747261</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101662</EventHandoutURL>
    <blurb>This work evaluates the impact of job placement, grid ordering and Slurm scheduling techniques to the performance of FFT Kampur library on a Dragonfly network cluster, i.e. Shaheen II. The evaluations demonstrate that configuration case when all processors on different blades which have physical all-to-all connections are utilized for execution the total time reduces to half. Further configurations were tested as well and will be demonstrated in this talk. 






</blurb>
    <EventParentName>MS53 Parallel Sparse and Conventional FFTs, Applications and Implementation - Part II of II</EventParentName>
    <external_id>67765-101662</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Rendezvous Algorithms for Large-Scale Particle Simulations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS54</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:35PM</starts_at>
    <ends_at>Feb 14 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Steve Plimpton</EventSpeakers>
    <EventSpeakerUniqueID>32856</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101856</EventHandoutURL>
    <blurb>Rendezvous algorithms are useful communication patterns when you have
data to send to processors who need it, but the senders don't know who
the receivers are, or vice versa.  The idea is to define an
intermediate decomposition where data from various processors can
"rendezvous", and the owning rendezvous processors can be identified
by both senders and receivers.

Originally designed for interpolating between overlaid grids with
independent decompositions [Plimpton, Hendrickson, Stewart, "A Parallel
Rendezvous Algorithm for Interpolation Between Multiple Grids", JPDC,
64, 266-276 (2004)], we've recently realized they are also useful for
operations needed at large scale in two of our particle codes: LAMMPS
for molecular dynamics and SPARTA for DSMC modeling.  They are
particularly useful for infrequent setup operations which require data
to move around the machine in arbitrary ways, but can leverage large
bisection bandwidths.

Brute-force alternatives – e.g. circulating data in a ring to all
processors so they can find what they need – often work fine when
processor counts are small, but can become significant bottlenecks at scale, for billions of particles and/or millions of MPI tasks.

We describe how the rendezvous algorithm works, how it can be
implemented simply as a flexible tool, and illustrate some dramatic
performance improvements it offers.  We also point out similarities
with the MapReduce paradigm popularized by Google and Hadoop.
</blurb>
    <EventParentName>MS54 Particle Methods: Algorithms and Software Technology for Exascale - Part I of III</EventParentName>
    <external_id>67828-101856</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Sketching-Based Streaming Tensor Decompositions and Detection of Concept Drift in Unsupervised Exploratory Analysis</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS55</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:45PM</starts_at>
    <ends_at>Feb 14 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Evangelos Papalexakis</EventSpeakers>
    <EventSpeakerUniqueID>779284</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101637</EventHandoutURL>
    <blurb>Tensors and tensor decompositions have been very popular and effective tools for analyzing multi-aspect data in a wide variety of fields, ranging from Psychology to Chemometrics, and from Signal Processing to Data Mining and Machine Learning.  Using tensors in the era of big data presents us with a rich variety of applications, but also poses great challenges, especially when it comes to scalability and efficiency. 

In this talk, we touch upon the problem of streaming tensor decomposition, where new data "slices" are added to our existing data. An instance of this problem is the case of a time-evolving graph, where each time snapshot, a new adjacency matrix is appended to our tensor. We discuss algorithmic approaches that leverage sketching for updating the already-computed decomposition with the new data, and we present novel results on capturing and alleviating concept drift in such streaming cases.</blurb>
    <EventParentName>MS55 High-Performance Tensor Computation and Applications - Part I of III</EventParentName>
    <external_id>67756-101637</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>PASTA: A Parallel Sparse Tensor Algorithm Benchmark Suite</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS55</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:10PM</starts_at>
    <ends_at>Feb 14 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jiajia Li</EventSpeakers>
    <EventSpeakerUniqueID>768149</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101638</EventHandoutURL>
    <blurb>Tensor computations present significant performance challenges that impact a wide spectrum of applications ranging from machine learning, healthcare analytics, social network analysis, data mining to quantum chemistry and signal processing. Efforts to improve the performance of tensor computations include exploring data layout, execution scheduling, and parallelism in common tensor kernels. This work presents a benchmark suite for arbitrary-order sparse tensor kernels using state-of-the-art tensor formats: coordinate (COO) and hierarchical coordinate (HiCOO) on CPUs and GPUs. It presents a set of reference tensor kernel implementations that are compatible with real-world tensors and power law tensors extended from synthetic graph generation techniques. We also propose Roofline performance models for these kernels to provide insights of computer platforms from sparse tensor view. This benchmark suite is publicly available: https://gitlab.com/tensorworld/pasta.
</blurb>
    <EventParentName>MS55 High-Performance Tensor Computation and Applications - Part I of III</EventParentName>
    <external_id>67756-101638</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Solving Large Eigenvalue Problems with the Parallel EVSL Package</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS57</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yousef Saad</EventSpeakers>
    <EventSpeakerUniqueID>774768</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101696</EventHandoutURL>
    <blurb>This talk will discuss how filtering techniques for eigenvalue
problems can be put to work to implement `spectrum slicing'
strategies, i.e., strategies that extract slices of the spectrum
independently. The presentation will begin with an overview of
filtering strategies, whether polynomial or rational.  Polynomial
filtering can be particularly efficient for standard problems and in situations where the matrix-vector product operation is inexpensive and a large number of eigenvalues is sought.  Rational filtering can be competitive in other situations and when a good parallel direct
solver is available.  We will also present practical implementation issues in a scalar and parallel environment and provide some details on the related EVSL package.  Finally, we will discuss a collaborative effort that involves the use of EVSL to solve large eigenvalue problems that arise when computing the normal modes of the Earth.
</blurb>
    <EventParentName>MS57 Parallel Eigenvalue Algorithms for Physical Simulation - Part I of III</EventParentName>
    <external_id>67779-101696</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Performance Analysis and Benchmarking for pySDC</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS58</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Robert Speck</EventSpeakers>
    <EventSpeakerUniqueID>747124</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101768</EventHandoutURL>
    <blurb>The parallel full approximation scheme in space and time (PFASST) allows to integrate multiple time-steps simultaneously. Based on iterative spectral deferred correction (SDC) methods, PFASST uses a space-time hierarchy with various coarsening strategies to maximize parallel efficiency. In numerous studies, this approach has been used on up to 448K cores and coupled to space-parallel solvers which use finite differences, spectral methods or even particles for discretization in space. However, since the integration of SDC or PFASST into an existing application code is not straightforward and the potential gain is typically uncertain, we have developed the Python prototyping framework pySDC. While it allows to rapidly test new ideas and to implement first toy problems more easily, it can also be used to run space-time parallel tests and applications using mpi4py. In this talk, we examine pySDC’s performance on an HPC cluster and demonstrate the application of the "Scalable Performance Measurement Infrastructure for Parallel Codes" (Score-P) for analyzing the performance of our code. We highlight Python-, MPI- and PinT-specific aspects of our results and show the benefits of a structured benchmarking workflow.</blurb>
    <EventParentName>MS58 Parallel-in-Time Integration Methods - Part II of II</EventParentName>
    <external_id>67797-101768</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Resilient Computation Patterns in Scientific Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS59</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:45PM</starts_at>
    <ends_at>Feb 14 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ignacio Laguna</EventSpeakers>
    <EventSpeakerUniqueID>790014</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101730</EventHandoutURL>
    <blurb>As HPC systems scale in size and computational power, the danger of
silent errors grows dramatically. Consequently, applications running on
HPC systems need to exhibit resilience to such errors. Previous work has
found that, for certain codes, this resilience can come for free, i.e.,
some applications are naturally resilient, but few studies have shown
the code patterns (i.e., combinations or sequences of computations) that
make an application naturally resilient. In this talk, we present
FlipTracker, a framework designed to extract these patterns using
fine-grained tracking of error propagation and resilience properties.
Using this framework, we present a set of computation patterns that are
responsible for making representative HPC applications naturally
resilient to errors. This not only enables a deeper understanding of
resilience properties of these codes, but also can guide future
application designs towards patterns with natural resilience.</blurb>
    <EventParentName>MS59 Resilience and Fault Tolerance for Extreme Computing Systems - Part I of III</EventParentName>
    <external_id>67783-101730</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A General Parallel Sparsified Nested Dissection Algorithm using a Task-Based  Runtime System</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS61</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:30AM</starts_at>
    <ends_at>Feb 15 2020 11:50AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Leopold Cambier</EventSpeakers>
    <EventSpeakerUniqueID>783669</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101817</EventHandoutURL>
    <blurb>We propose a new algorithm for the fast solution of large, sparse linear systems. It is based on nested dissection, sparsification and low-rank compression. After eliminating all interiors at a given level of the elimination tree, the algorithm sparsifies all separator. This reduces the size of the separators but without introducing any fill-in, at the expense of a small and controllable approximation error.
We present the algorithm as well as a parallel version using TaskTorrent. TaskTorrent is a task-based distributed runtime system. It is lightweight, portable (using C++ threads and MPI) and requires only minor changes to the user code.
We then demonstrate that a version using orthogonal factorization and block-diagonal scaling takes fewer CG iterations to converge than previous similar algorithms on various kinds of problems. Furthermore, this algorithm is provably guaranteed to never break down and the matrix stays symmetric positive-definite throughout the process. The factorization time is roughly O(N) and the number of iterations grows slowly with N.
We finally show the scalability of the algorithm as a function of the number of cores. In particular, we show that the sparsification step increases the concurrency of the algorithm. By moving most of the computation towards the leaves in the sparsification step, the algorithm avoids the typical bottleneck of direct methods at the root of the tree. This significantly speeds up solving large sparse linear systems.


</blurb>
    <EventParentName>MS61 Advances in Parallel Sparse Linear System Solver Stacks for Exascale - Part II of II</EventParentName>
    <external_id>67789-101817</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name> One-Sync CGS2 Algorithm in the Context of QR Factorization and Arnoldi Process</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS61</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:55AM</starts_at>
    <ends_at>Feb 15 2020 12:15PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daniel Bielich</EventSpeakers>
    <EventSpeakerUniqueID>790029</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101822</EventHandoutURL>
    <blurb>The number of global reductions is an important metric for the parallel scalability of Krylov iterative methods. We focus on the Arnoldi-QR algorithm for nonsymmetric matrices. The underlying orthogonalization scheme is ``left-looking" and ``sees" columns one at a time. Thus, at least one global reduction is required per iteration. A stable method for orthogonalizing the Krylov vectors during the Arnoldi process is the classical Gram Schmidt algorithm with reorthogonalization (CGS2), requiring three reductions per step. A new variant of Arnoldi-CGS2 that requires only one reduce has been derived.  Stability and strong-scaling results are presented for finding eigenvalue-pairs of a nonsymmetric matrix. A preliminary attempt to derive a similar algorithm (one reduction per Arnoldi iteration with a robust orthogonalization scheme) was presented by Hernandez et al. 2007 [1] but their method lacks numerically stability; while our new method, after extensive experiments, is much more stable and accurate.  Our algorithm can also be implemented in the context of a QR factorization (as opposed to Arnoldi), and we explain the method in this context as well.

[1] V. Hernandez, J.E. Roman, A. Tomas, Parallel Arnoldi eigensolvers with enhanced scalability via global communications rearrangement. Parallel Computing 33 (2007) 521–540.




</blurb>
    <EventParentName>MS61 Advances in Parallel Sparse Linear System Solver Stacks for Exascale - Part II of II</EventParentName>
    <external_id>67789-101822</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Minimal-Precision Computing for High-Performance, Energy-Efficient, and Reliable Computations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS66</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:55AM</starts_at>
    <ends_at>Feb 15 2020 12:15PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daichi Mukunoki</EventSpeakers>
    <EventSpeakerUniqueID>753661</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101602</EventHandoutURL>
    <blurb>We introduce a joint research project with RIKEN, Sorbonne University, and University of Tsukuba for developing a minima-precision computing scheme -- to obtain the floating-point result with the precision requested by users with the minimal-precision use --  for high-performance (speed and energy) and reliability (accuracy and reproducibility). This is a system-level proposal involving both hardware and software stack by combining (1) a precision-tuning method through numerical validation by Discrete Stochastic Arithmetic (DSA), (2) arbitrary-precision arithmetic libraries, (3) fast and accurate numerical libraries, and (4) Field-Programmable Gate Array (FPGA) with high-level synthesis. As a result, it achieves the following goals: (a) Reliable: the precision of the result is validated by DSA. It meets the demands for accurate and reproducible computation. (b) General: it is applicable for any floating-point computations. It contributes to low development cost and sustainability. (c) Comprehensive: our system covers from the precision-tuning to the execution and combines heterogeneous hardware and hierarchal software stack. (d) High-performance: it utilizes fast numerical libraries and hardware acceleration with FPGA and GPU. (e) Realistic: it can be realized by combining available technologies. Although it is still in development, this talk introduces that the system could be constructed by combining already available technologies and that many of them are developed by us. </blurb>
    <EventParentName>MS66 New Approaches for Software Auto-Tuning and Accuracy Assurance - Part I of II</EventParentName>
    <external_id>67742-101602</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Scalable Matrix-Free Eigensolver for Studying Many-Body Localization</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS67</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:55AM</starts_at>
    <ends_at>Feb 15 2020 12:15PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Roel Van Beeumen</EventSpeakers>
    <EventSpeakerUniqueID>760440</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101853</EventHandoutURL>
    <blurb>We present a scalable and matrix-free eigensolver for studying nearest-neighbor Heisenberg spin chain plus random on-site disorder models that undergo a many-body localization (MBL) transition. This type of problem is computationally challenging because its dimension grows exponentially with the physical system size, and the solve must be iterated many times to average over different configurations of the random disorder. For each eigenvalue problem, eigenvalues from different regions of the spectrum and their corresponding eigenvectors need to be computed. Traditionally, the interior eigenstates for a single eigenvalue problem are computed via the shift-and-invert Lanczos algorithm. Due to the extremely high memory footprint of the LU factorizations, this technique is not well suited for large number of spins $L$, e.g., one needs thousands of compute nodes on modern high performance computing infrastructures to go beyond $L = 24$. We propose a new matrix-free approach that does not suffer from this memory bottleneck and even allows for simulating spin chains up to $L = 24$ spins on a single compute node. We discuss the OpenMP and hybrid MPI--OpenMP implementations of matrix-free block matrix-vector operations that are the key components of the new approach. The efficiency and effectiveness of the proposed algorithm is demonstrated by computing eigenstates in a massively parallel fashion, and analyzing their entanglement entropy to gain insight into the MBL transition.</blurb>
    <EventParentName>MS67 Parallel Eigenvalue Algorithms for Physical Simulation - Part II of III</EventParentName>
    <external_id>67780-101853</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Devito - DSL for Generating MPI and OpenMP Parallel Finite Difference Operators for Seismic Imaging</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS68</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:05AM</starts_at>
    <ends_at>Feb 15 2020 11:25AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>1900</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Gerard J Gorman</EventSpeakers>
    <EventSpeakerUniqueID>730755</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101911</EventHandoutURL>
    <blurb>Devito is an innovative Python project based on domain-specific languages (DSL)
and compiler technology for rapid implementation of high-performance,
structured-grid solvers for partial differential equations
(PDEs). In FWI/RTM for example, Devito is used to implement
efficient, full-fledged production-grade wave propagators. In this article, we introduce
Distributed-memory parallelism (DMP) in Devito. The key innovations are the abstractions
provided to the user and the compiler-based implementation
approach, which we consider invaluable for long-term sustainable software to
replace (partly or fully) obsolete, impenetrable, hardly extendable and often
inefficient legacy code. Our approach allows users to think as sequentially as
possible — all major aspects of DMP are handled by Devito, including domain
decomposition, data accessing (through distributed NumPy arrays), and optimized
communications. We provide compelling evidence demonstrating the efficiency of our
technology at all levels of abstraction. In particular, we show the results of strong
scaling experiments on single-node multi-socket as well as multi-node systems,
using production-level seismic inversion propagators employed in FWI/RTM.

</blurb>
    <EventParentName>MS68 Advanced HPC Trends Oil and Gas Applications - Part I of II</EventParentName>
    <external_id>67846-101911</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Fast and Faithful Performance Prediction of MPI Applications: the HPL Case Study</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS69</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:30AM</starts_at>
    <ends_at>Feb 15 2020 11:50AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tom Cornebize</EventSpeakers>
    <EventSpeakerUniqueID>790015</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101740</EventHandoutURL>
    <blurb>Finely tuning MPI applications (number of processes, granularity, collective
operation algorithms, topology and process placement) is critical to obtain good performance on
supercomputers.  With a rising cost of modern supercomputers, running parallel
applications at scale solely to optimize their performance is extremely
expensive. Having inexpensive but faithful predictions of expected performance
could be a great help for researchers and system administrators.
The methodology we propose captures the complexity of adaptive applications by
emulating the MPI code while skipping insignificant parts. We demonstrate its
capability with High Performance Linpack (HPL), the benchmark used to rank
supercomputers in the TOP500 and which requires a careful tuning.  We explain
(1) how we both extended the SimGrid's SMPI simulator and slightly modified the
open-source version of HPL to allow a fast emulation on a single commodity
server at the scale of a supercomputer and (2) how to model the different
components (network, BLAS, ...) of the system.  We show that a careful
modeling of both spatial and temporal node variability allows us to obtain
predictions within a few percents of real experiments.
</blurb>
    <EventParentName>MS69 The Many Faces of Simulation for HPC - Part I of II</EventParentName>
    <external_id>67786-101740</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Composing Asynchrony, Communication and Resilience</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS70</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:30AM</starts_at>
    <ends_at>Feb 15 2020 11:50AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sri Raj Paul</EventSpeakers>
    <EventSpeakerUniqueID>790009</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101718</EventHandoutURL>
    <blurb>Resilience is an imminent issue for next-generation systems due to projected increases in soft/transient failures. Much of the work for resilience has been focused on traditional bulk-synchronous parallel programming models. We believe that Asynchronous Many-Task (AMT) models are better suited to enabling resilience since they provide explicit abstractions of data and tasks which contribute to more scalable and portable approaches. In this talk, we introduce a comprehensive
approach to enabling resiliency in AMT programming models, along with integrating the capability to perform asynchronous inter-node communication.
Specifically, we make the following contributions:
\begin{enumerate}
\item programming model extensions to enable resilience techniques from past work (task replay, task replication, ABFT) to be applied to AMT applications along with asynchronous communication
\item unified execution framework that supports arbitrary composition of these extensions and non-resilient tasks
\item implementation of our approach as extensions to the Habanero C/C++ library
\end{enumerate} 
Our results show that we can enable resilience in AMT programs with low overheads, and we demonstrate the ability to combine different resilience schemes along with communication.





</blurb>
    <EventParentName>MS70 Resilience and Fault Tolerance for Extreme Computing Systems - Part II of III</EventParentName>
    <external_id>67784-101718</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Towards Continuous Benchmarking: An Automated Performance Evaluation Framework for High Performance Software</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS71</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:40PM</starts_at>
    <ends_at>Feb 15 2020  3:00PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Terry Cojean</EventSpeakers>
    <EventSpeakerUniqueID>785177</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101712</EventHandoutURL>
    <blurb>We present an automated performance evaluation framework that enables an automated workflow for testing and performance evaluation of software libraries. The Continuous Integration (CI) framework employed for the Ginkgo library is extended to a Continuous Benchmarking framework (CB) which automatically runs benchmark tests on predetermined HPC systems, stores the state of the machine and the environment along with the compiled binaries, and collect results in a publicly accessible performance data repository. The Ginkgo performance explorer (GPE) can be used to retrieve the performance data from the repository, and visualizes it in a web browser. The combination of Continuous Integration, Continuous Benchmarking, and the Ginkgo Performance Explorer creates a workflow which enables performance reproducibility and software sustainability. </blurb>
    <EventParentName>MS71 Toward Efficient Software Integration and Deployment</EventParentName>
    <external_id>67782-101712</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The Rogues Gallery as a Testbed for Novel Algorithm Design for Future Architectures</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS72</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:40PM</starts_at>
    <ends_at>Feb 15 2020  3:00PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jeffrey Young</EventSpeakers>
    <EventSpeakerUniqueID>780682</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101903</EventHandoutURL>
    <blurb>The Rogues Gallery is a new testbed deployment at Georgia Tech for understanding next-generation hardware with a focus on unorthodox and uncommon technologies for what is commonly referred to as post-Moore computing. This testbed project was originally initiated in 2017 in response to IEEE Rebooting Computing efforts and initiatives for new hardware designs. The Gallery's focus is to acquire new and unique hardware (the rogues) from vendors, research labs, and start-ups and to make this hardware widely available to students, faculty, and industry collaborators within a managed data center environment. By exposing students and researchers to this set of unique hardware, we hope to foster cross-cutting discussions about hardware and software design that will drive future performance improvements in computing long after the Moore's Law era of cheap transistors ends. 

We present highlights of the first two years of post-Moore era research with the Rogues Gallery. Specifically, we focus on how the Rogues Gallery has supported new algorithm and application development efforts in the areas of near-memory, neuromorphic, and quantum computing with some brief lessons learned on how we can better support future research in these unique research areas. 




</blurb>
    <EventParentName>MS72 Novel Computational Algorithms for Future Computing Platforms - Part III of III</EventParentName>
    <external_id>67839-101903</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Design of New Streaming and Graph Analytics Algorithms for the Strider Architecture</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS72</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  3:05PM</starts_at>
    <ends_at>Feb 15 2020  3:25PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sriseshan Srikanth</EventSpeakers>
    <EventSpeakerUniqueID>790056</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101905</EventHandoutURL>
    <blurb>A vertex-centric, push-based implementation of a graph analytics algorithm is equivalent to a sparse reduction problem, where the algorithm decides the associative reduction operator. Obtaining high performance from these problems is particularly challenging as they suffer from minimal spatio-temporal locality, depending upon the input. By automatically and intelligently marshaling an input sparse data stream using specialized hardware, the Strider architecture is able to maximize bandwidth utilization from on-chip as well as off-chip memory subsystems, while also performing parallel reduction. As a result, Strider is able to significantly and efficiently accelerate SpMV and SpGEMM based graph analytics problems. Furthermore, as Strider treats the input as a dynamic sparse data stream with no pre-ordering assumed, it is able to handle both static and evolving graphs. This talk presents an overview of the Strider architecture, and the design of graph analytics algorithms to best leverage such an architecture. </blurb>
    <EventParentName>MS72 Novel Computational Algorithms for Future Computing Platforms - Part III of III</EventParentName>
    <external_id>67839-101905</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Distributed Memory Generalized Sparse Tensor Decomposition</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS74</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  3:05PM</starts_at>
    <ends_at>Feb 15 2020  3:25PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Karen Devine</EventSpeakers>
    <EventSpeakerUniqueID>62039</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101647</EventHandoutURL>
    <EventParentName>MS74 High-Performance Tensor Computation and Applications - Part III of III</EventParentName>
    <external_id>67758-101647</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Real-Space Parallel Eigensolvers for Electronic Structure Calculations: The Future</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS76</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:40PM</starts_at>
    <ends_at>Feb 15 2020  3:00PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>James Chelikowsky</EventSpeakers>
    <EventSpeakerUniqueID>763738</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101726</EventHandoutURL>
    <blurb>Two physical ingredients, pseudopotentials and density functional theory, are widely used and accepted in electronic structure computations for a wide variety of materials applications.  If we wish to address large, complex systems, the implementation of these ingredients on high performance computational platforms is vital.  Real space grid methods offer a compelling vehicle for such computations.  These methods are mathematically robust, very accurate and well suited for modern, massively parallel computing resources.   I will illustrate the utility of these methods as implemented in the PARSEC code.  Key algorithms in this code include subspace filtering based on Chebyshev polynomials, spectrum slicing for added level of parallelism, Cholesky QR algorithms to improve the performance of orthogonalization, and a 2D partition of the wave functions for efficient matrix-vector operations.  Applications will be illustrated for nanostructures containing tens of thousands of atoms. </blurb>
    <EventParentName>MS76 Parallel Eigenvalue Algorithms for Physical Simulation - Part III of III</EventParentName>
    <external_id>67781-101726</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Revisiting Seismic Data Management with Distributed Asynchronous Object Storage (DAOS)</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS77</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:15PM</starts_at>
    <ends_at>Feb 15 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>1900</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Essam Algizawy</EventSpeakers>
    <EventSpeakerUniqueID>786136</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101973</EventHandoutURL>
    <EventParentName>MS77 Advanced HPC Trends for Oil and Gas Applications - Part II of II</EventParentName>
    <external_id>67853-101973</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Distributed ML-Based Applications in Oil &amp; Gas</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS77</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  3:05PM</starts_at>
    <ends_at>Feb 15 2020  3:25PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>1900</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mauricio Araya-Polo</EventSpeakers>
    <EventSpeakerUniqueID>790337</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102126</EventHandoutURL>
    <blurb>ML-based applications in Oil &amp; Gas are maturing fast, and with that the need to access and train on large datasets is becoming a must. We will introduce example applications, distributing learning frameworks and how these frameworks help our applications to scale and solve large datasets. The examples will range from core plugs to reservoir level.
</blurb>
    <EventParentName>MS77 Advanced HPC Trends for Oil and Gas Applications - Part II of II</EventParentName>
    <external_id>67853-102126</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Robust Dynamic Load Balancing of Scientific Applications against System Variability and Failures</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS79</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:15PM</starts_at>
    <ends_at>Feb 15 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Florina Ciorba</EventSpeakers>
    <EventSpeakerUniqueID>790013</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101742</EventHandoutURL>
    <blurb>Exploiting the full computational power of HPC systems for extreme-scale scientific applications is a complex challenge due to load imbalance, system variability, fail-stop failures and silent data corruptions (SDCs). Dynamic load balancing (DLB) improves the performance of such applications, typically via the use of self-scheduling-based techniques. However, the robustness of self-scheduling-based techniques against variability in and failures of system components remains an open problem. Existing fault-tolerant self-scheduling techniques need fault-detection mechanisms to trigger the rescheduling of tasks whose execution failed. In this talk, we present a \emph{robust dynamic load balancing} (rDLB) approach for the robust self-scheduling of scientific applications with independent tasks on HPC systems under system variability and/or failures. We also present a \emph{selective particle replication} (SPR) approach for detecting SDCs in particle-based scientific applications. The rDLB proactively reschedules already allocated tasks,  requires no detection of system variability or failures, and is integrated into an MPI-based DLB library. The experiments show that rDLB boosts application robustness against system variability and fail-stop failures by up to 30\% compared to the case without rDLB. The SPR replicates the computation and data of a few carefully selected particles and achieves detection rates of 91-99.9\%, no false-positives, at an overhead of 1-10\%. 
</blurb>
    <EventParentName>MS79 Resilience and Fault Tolerance for Extreme Computing Systems - Part III of III</EventParentName>
    <external_id>67785-101742</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Combinatorial Optimization on Quantum Computers</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MT1</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  2:40PM</ends_at>
    <EventFilter>PP20|Minitutorial</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605 </location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yuri Alexeev</EventSpeakers>
    <EventSpeakerUniqueID>760911</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101798</EventHandoutURL>
    <blurb>Quantum computing has potential to efficiently solve combinatorial optimization problems. Recent advances in both hardware and algorithm development have made it possible to solve small problems on modern quantum computers. Combinatorial optimization problems (especially NP-hard problems) are of particular interest, since for many of these problems efficient classical algorithms are not known. One such problem is MaxCut on graphs. In this minitutorial, we will introduce the MaxCut problem and explain how it can be solved on IBM quantum computers available on the cloud today using the Qiskit framework. Our presentation will assume little to no prior knowledge of quantum computation. Moreover, we will provide examples of how more complicated problems can be solved using the QAOA (Quantum Approximate Optimization Algorithm). The tutorial will be interactive and will use Jupyter notebooks to explain step by step how to formulate and implement algorithms in the Ising Hamiltonian form.</blurb>
    <EventParentName>MT1 Combinatorial Optimization on Quantum Computers</EventParentName>
    <external_id>67813-101798</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Application of Projectile Physics and Variable Drag Implications in Determining Market Price Movements for Futures Derivatives</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Leonard Mushunje</EventSpeakers>
    <EventSpeakerUniqueID>789708</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101592</EventHandoutURL>
    <blurb>This particular study took an econophysics route to explain the market behaviour for futures contracts in terms of prices and its market life span. We used projectile motion models defined under two distinct conditions (perfect/horizontal and imperfect/ drag implication) based on Newton’s and Galileo’s laws of motion. Despite that it was more theoretical we managed to derive the futures price functions and the results showed that futures prices depends largely on market forces of demand and supply and underlying assets price behavior. Also, we managed to find the terminal prices for the securities given the initial prices, which is a worrying matter to the trading parties. From the performance comparison of the two models used, results suggested that futures price function from a drag variable is more powerful in modelling the price behavior for options than the one sorely controlled by market demand and supply forces. However, it should not be carelessly taken that the projectile models used are much good at price motions/movements within the market from time to time with a stunted ability to capture in other facts of interest such as volatility coefficients which paves a research way for other scholars.</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-101592</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Planning Robot Manipulation Using Parallel-in-Time Integration</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daniel Ruprecht</EventSpeakers>
    <EventSpeakerUniqueID>748212</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101956</EventHandoutURL>
    <blurb>Planning actions performed by a robot to manipulate objects requires many rapid solves of systems of ordinary differential equations. In many cases, these planning times can be substantial and thus the robot "freezes", sometimes for minutes, while it "thinks". We explore the feasibility of using parallel-in-time integration algorithms to reduce planning times. Examples will include both simulations and experiments with a real robot pushing an object around an obstacle into a target zone.

	</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-101956</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Greedy Algorithms for Neural Network Architecture Optimization</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Massimiliano Lupo Pasini</EventSpeakers>
    <EventSpeakerUniqueID>762851</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=103170</EventHandoutURL>
    <blurb>In this project we aim to develop a new algorithm for the optimization of neural network architectures. The motivation at the core of this project is twofold. First, there is a need from domain scientists to easily interpret predictions returned by the statistical model and this tends to be unpractical when neural networks with complex structures are deployed. Secondly, there is a need to facilitate the use of neural networks in situations of compute/memory limitations. We address these demands by identifying a neural network that attains a prescribed accuracy with a minimal complexity. We propose a novel approach that is iterative in nature and that searches over sets of neural networks with incremental complexity. At each iteration the number of hidden layers is fixed and a random search is performed over all the hyper parameters other than the number of hidden layers. This method has appealing properties for algorithmic and computational scalability. Moreover, the random nature of the algorithm at each iteration still guarantees a thorough exploration of the hyper parameter space. Numerical experiments are shown to compare our method with other hyper parameter optimization algorithms. Preliminary results show that our approach can significantly improve the accuracy attained by the selected model as well as it can reduce the computational time to complete the optimization search.

	

</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-103170</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Discrete Element Method using Triangulated Particles</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Peter Noble</EventSpeakers>
    <EventSpeakerUniqueID>791858</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104163</EventHandoutURL>
    <blurb>Discrete Element Methods (DEM) simulate the interaction of large numbers of rigid, incompressible objects with each other. 
Mainstream DEM codes focus on analytical shapes to streamline the identification of contacts between objects.  This step dominates the simulation time.  We manage to support triangulated particles with a wide variety of sizes in an efficient DEM code due to a combination of several new algorithmic ideas.

First, we sieve the objects into a cascade of finer and finer adaptive Cartesian grids, which provide natural cut-off radii to search for collisions.  Intelligent inter-grid transfer operations and neighbouring cell searches allow us to reduce the number of geometric checks and to parallelise both within one grid resolution and between meshes.

Second, we model objects as hierarchy of shapes (level of detail). Collision tests are done with as low detail and cost as possible before we fall back to high resolution comparisons.

Third, we run all comparisons as combination of geometric checks and a distance minimisation algorithm. The latter yields high flop rates yet breaks down for ill-posed geometric constellations. Only in this latter case, we fall back to robust geometric checks, which do not vectorise.

Finally, we phrase our calculations in different floating point precisions and gradually step from smaller float memory footprints with high vector concurrency to high precision yet slow calculations.</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104163</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>SEMANTICMODELS.JL: A Framework for Automatic Composition of Scientific Models Across Domains</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Micah Halter</EventSpeakers>
    <EventSpeakerUniqueID>791879</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104175</EventHandoutURL>
    <blurb>Scientific progress comes from adapting and extending models from prior work to address new problems. However, this ideal workflow is difficult primarily because of the informal or inconsistent representation of models. This problem is exacerbated in fields outside of computer science where scientific models are formulated in informal languages. We propose \texttt{SemanticModels.jl}, a category theory-based framework for defining meta-modeling tasks such as model augmentation and model selection along with semantic information extraction. We illustrate the major features of \texttt{SemanticModels.jl} including representating models as wiring diagrams, extending and composing models with algebraic operations, and generating executable code of the resulting models. These features are demonstrated by constructing a model of mosquito borne illness in humans along with predator-prey dynamics between mosquitos and birds to study the effects of predator species on the control of mosquito borne illnesses. Through the careful application of category theoretic ideas to scientific computing, general patterns in scientific modeling languages and frameworks can be axiomatized and these formalizations can be exploited to improve scientific computing research and development processes.</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104175</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Proxy App for Mesh Relaxation via Machine Learning on Advanced Hardware</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kristofer Zieb</EventSpeakers>
    <EventSpeakerUniqueID>791915</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104233</EventHandoutURL>
    <blurb>With the doubling of transistors no longer a reality for CPUs, the advent of application specific hardware is becoming the new standard. As it is unreasonable to port large codes to new hardware with no promise of performance gain, the idea of a mini/proxy app is essential. Numerous mini/proxy apps have been developed that capture the approximate behavior of specific portions of the code essential for radiation transport and hydrodynamics. Whether that be memory impact specifically, say for cross section retrieval (Quicksilver or XSBench), or the behavior of a full hydrocode, but on a much smaller scale (LULESH). As machine learning (ML) comes into vogue, along with hardware specific to its application, the development of a mini/proxy app capturing its potential changes to program performance becomes necessary to evaluate memory, accuracy, and runtime needs of an application.
This work presents early results developed to detect mesh tangling in hydrodynamic simulations through the use of machine learning. Portability is the key focus of this application, as such, performance on GPUs, as well as new ML-specific hardware is presented.</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104233</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Optimization of GPU Kernels for Sparse Matrix Computations in HYPRE</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Chaoyu Zhang</EventSpeakers>
    <EventSpeakerUniqueID>791895</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104518</EventHandoutURL>
    <blurb>The acceleration of sparse matrix computations on GPUs can significantly enhance the performance of iterative methods for solving linear systems. In this work, we consider the kernels of Sparse Matrix Vector Multiplications (SpMV), Sparse Triangular Matrix Solves (SpTrSv) and Sparse Matrix Matrix Multiplications (SpMM), which are often demanded by Algebraic Multigrid (AMG) solvers. With the CUDA and the hardware support of the Volta GPUs on Sierra, the existing kernels should be further optimized to fully take the advantage of the new hardware, and the optimizations have shown significant performance improvement. The presented kernels have been put in HYPRE for solving large scale linear systems on HPC equipped with GPUs. These shared-memory kernels for single GPU are the building blocks of distributed matrix operations required by the solver across multiple GPUs and compute nodes. The implementations of these kernels in Hypre and the code optimizations will be discussed.

	
</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104518</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>PCPATCH: Software for the Topological Construction of Multigrid Relaxation Methods</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Lawrence Mitchell</EventSpeakers>
    <EventSpeakerUniqueID>785392</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104635</EventHandoutURL>
    <blurb>  Effective relaxation methods are necessary for good multigrid convergence.
  For many equations, standard Jacobi and Gau\ss--Seidel are inadequate, and
  more sophisticated space decompositions are required; examples include
  problems with semidefinite terms or saddle point structure. In this
  poster we present a unifying software abstraction, PCPATCH, for the topological
  construction of space decompositions for multigrid relaxation methods.
  Space decompositions are specified by collecting topological
  entities in a mesh (such as all vertices or facets) and applying a construction rule (such
  as taking all degrees of freedom in the cells around each entity). The
  software is implemented in PETSc and facilitates the elegant expression of a
  wide range of schemes merely by varying solver options at runtime. In turn,
  this allows for the very rapid development of fast solvers for difficult problems.

</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104635</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Comparison of a Randomized and a Deterministic Low-Rank Approximation Algorithm</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Robert Ernstbrunner</EventSpeakers>
    <EventSpeakerUniqueID>792440</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104721</EventHandoutURL>
    <blurb>Low-rank approximations of large sparse matrices are important in many scientific applications. We compared a deterministic algorithm for computing a truncated LU factorization with tournament pivoting, proposed by Grigori et al. (2018), with the adaptive randomized range finder, proposed by Halko et al. (2011). The latter is a randomized algorithm which computes an approximate basis for the range of the input matrix.

We developed optimized implementations of these algorithms in order to evaluate their accuracy and runtime behavior. Moreover, we compared parallelization properties of randomized low-rank approximations to deterministic algorithms by using shared-memory parallelization. For the adaptive randomized range finder, we implemented a blocked version that utilizes sparse embedding matrices to reduce communication. Moreover, we 
used the Block Classical Gram-Schmidt algorithm in combination with Tall Skinny QR factorization for orthogonalization, which is known to be communication-optimal.

For the randomized algorithm the runtime can be estimated well based on the problem size due to the inevitable use of dense linear algebra, since the resulting low-rank approximation is usually quite dense. In the deterministic algorithm, after each iteration the input matrix is updated by removing the portion approximated so far. This potentially produces fill-in on the matrix used in successive iterations, which can have large impact on the runtime but is hard to predict.
</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104721</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>SIAG/Supercomputing Early Career Prize: Scalable Algorithms for Tensor Computations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>SP2</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  9:25AM</starts_at>
    <ends_at>Feb 14 2020  9:55AM</ends_at>
    <EventFilter>PP20|Prize Speaker</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Edgar Solomonik</EventSpeakers>
    <EventSpeakerUniqueID>767223</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=103316</EventHandoutURL>
    <blurb>We give a high-level overview of a few recent algorithmic advances in the design of efficient algorithms for tensor computations. We highlight a communication-avoiding parallel algorithm for dense symmetric eigenvalue problems, which is part of a broader family of new matrix factorization algorithms with costs that attain communication lower bounds. One recently developed practical variant of these algorithms, a 3D parallel algorithm for CholeskyQR2, achieves speed-ups of up to 3.3X over the best existing libraries. These algorithmic innovations are extended to tensor operations and deployed as part of the Cyclops library. We describe novel parallel implementations of tensor decomposition and tensor completion methods using Cyclops. Finally, we introduce a new algorithm for tensor decomposition, pairwise perturbation, which approximates the alternating least squares procedure with asymptotically less cost.</blurb>
    <EventParentName>SP2 SIAG/Supercomputing Early Career Prize: Scalable Algorithms for Tensor Computations</EventParentName>
    <external_id>68268-103316</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T09:25:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Scalable Explicit Finite Element Solver for Cardiovascular Models with Uncertain Material Properties</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP1</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:50PM</starts_at>
    <ends_at>Feb 12 2020  2:10PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Xue Li</EventSpeakers>
    <EventSpeakerUniqueID>790198</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102055</EventHandoutURL>
    <blurb>Simulation of cardiovascular flow under uncertainty is an extremely intensive computational task, requiring large meshes and hundreds to thousands of high-fidelity model solutions. In this context, we propose a novel approach for ensemble simulation, and demonstrate it to the segregated, explicit-in-time solution of blood flow in the thoracic aorta with random material properties, focusing on the implementation of fast matrix-vector products on CPUs and GPUs. Distributed CPU storage is achieved through METIS partitioning and using a sparse compressed row storage (CRS) format with dense blocks containing multiple material property realizations for a three-d.o.f.s shell finite element. We developed optimized Cython code for the sparse matrix-vector multiplication using MPI+openMP and compared it to the \emph{mkl\_cspblas\_dcsrgemv} routine provided through the Intel MKL library. Our implementation achieves better performance on a wide range of mesh sizes, number of cores, and with/without multithreading. Our openCL-based GPU matrix-vector product achieves instead a 10-fold speed-up with respect to a na\"ive implementation, by using separate command-queues, overlapping the CPU to GPU data transfer with GPU kernel execution, and using page-locked CPU memory. Additional improvements are obtained by direct computation of local element matrices on the GPU. Ongoing work focuses on coupling our explicit structural solver with a variational multiscale finite element fluid solver.
	</blurb>
    <EventParentName>CP1 UQ and Stochastic Processes</EventParentName>
    <external_id>68191-102055</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Scalable Parallel Contact Algorithm Based on Dynamic Ghost Reconstruction for Lagrangian Hydrodynamic Application</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP3</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:25PM</starts_at>
    <ends_at>Feb 12 2020  4:45PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Li Liao</EventSpeakers>
    <EventSpeakerUniqueID>763480</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102002</EventHandoutURL>
    <blurb>Parallel simulation of contact problem is very challenging due to the extremely complex behavior of the contact line motion on the boundary. A data decomposition based parallel algorithm is imposed to deal with contact with large elastic deformations including time dependent responses. Entity set technology and patch based slide line data structure are combined to support the parallel contact definition and continuously varying relation management. Dynamic ghost region reconstruction technique is designed to support patch based parallel contact search and computation. Check point technique is used to reduce communication overhead caused by reconstruction process.  The algorithm is designed and implemented on JAUMIN which is a component based domain specific parallel programming framework. The implementation of the algorithm in a two dimentional Lagrangian elastic-plastic fluid mechanics application proves that it effectively balances the load of internal and contact force calculation to achieve good parallel efficiency.

	</blurb>
    <EventParentName>CP3 Application - Part I of III</EventParentName>
    <external_id>68193-102002</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Multiscale Multiphysics Coupling of Hall-Effect Thruster, Plume, and Surface Charging Models for Spacecraft Integration</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP5</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:45PM</starts_at>
    <ends_at>Feb 13 2020  4:05PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Samuel Araki</EventSpeakers>
    <EventSpeakerUniqueID>780242</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101598</EventHandoutURL>
    <blurb>Assessment of spacecraft integration is particularly important when designing spacecraft, as exhaust gases from propulsion devices can damage and contaminate important components of spacecraft such as the payload and solar array. When using a Hall-Effect Thruster (HET), ions originating from the thruster and created in the plume through a charge-exchange process can contribute to sputtering of spacecraft components even in regions with no direct line of sight of the thruster channel. The assessment of spacecraft is typically done with combinations of numerical models that simulate a HET device, plume region, and spacecraft surface charging. These models involve multiple time-scales that are governed by ions, electrons, grid resolution, and electron motion within materials, and therefore coupling of the models has to be done with care for stable convergence of numerical solution. In addition, it is difficult to achieve good performance from naive coupling of different simulations due to communication. To overcome the difficulty, a unified framework across the three numerical models is currently under development, which will simplify data exchange and enable strong coupling between models.</blurb>
    <EventParentName>CP5 Application - Part II of III</EventParentName>
    <external_id>68194-101598</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Efficient Parallel Algorithms and Implementations for Sparse Triangular Solves on GPUs</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP10</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:45AM</starts_at>
    <ends_at>Feb 14 2020 12:05PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Chaoyu Zhang</EventSpeakers>
    <EventSpeakerUniqueID>791895</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104192</EventHandoutURL>
    <blurb>The sparse triangular matrix solve (SpTrSV) is an important computation kernel that is demanded by a variety of numerical methods such as the Gauss-Seidel iterations. However, developing efficient parallel algorithms for SpTrSV that are suitable for GPUs remains a challenging task due to the inherently sequential nature in the solve. In this paper, we revisit this problem by reviewing several parallel algorithms based on different task scheduling  and different sparse matrix storage schemes, proposing modifications to the existing methods that can greatly improve the performance, and describing the implementations in details. 
Numerical results of Gauss-Seidel iterations with structured and unstructured  matrices make evident  the superiority of the proposed algorithms and implementations comparing with state-of-the-art methods in the literature.


</blurb>
    <EventParentName>CP10 Proceedings Papers - Part III of III</EventParentName>
    <external_id>68615-104192</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Accelerating Jacobi Iteration on GPUs using Shared Memory</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP13</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  2:10PM</ends_at>
    <EventFilter>PP20|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mohammad Islam</EventSpeakers>
    <EventSpeakerUniqueID>790204</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102046</EventHandoutURL>
    <blurb>Scientific simulations modeling important physical phenomena typically require solving linear systems of equations. This step often takes a vast amount of computational time to complete, and therefore presents a bottleneck in simulation work. Solving these linear systems efficiently requires the use of highly parallel hardware with high computational throughput, as well as the development of algorithms which respect the memory hierarchy of these hardware architectures. 

In this talk, we present an algorithm to accelerate Jacobi iteration for solving linear systems arising from structured problems, on graphics processing units (GPUs). Our algorithm takes advantage of the quick to access shared memory which resides in each streaming multiprocessor (SM). This is done using a domain decomposition approach, in which we partition our problem domain into subdomains whose information is stored in the shared memory of each SM. Jacobi iterations are performed internally within each SM's shared memory, avoiding the need to perform expensive global memory accesses every step. We test our algorithm on the linear systems arising from discretization of Poisson’s equation in 1D and 2D, and observe speedup in convergence using our shared memory approach compared to a traditional Jacobi implementation which only uses global memory on the GPU.


</blurb>
    <EventParentName>CP13 Accelerating Software with GPUs</EventParentName>
    <external_id>68200-102046</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Accelerated-Node-Enabled Computational and Data Science: It’s not just for Exascale</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  8:30AM</starts_at>
    <ends_at>Feb 13 2020  9:10PM</ends_at>
    <EventFilter>PP20|Invited Speaker</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Douglas Kothe</EventSpeakers>
    <EventSpeakerUniqueID>721075</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101779</EventHandoutURL>
    <blurb>In just the past decade, heterogeneous node-based computing hardware and software architectures have moved from novelty to mainstream. Prototypical examples are the now ubiquitous “accelerators”, or GPU-based architectures designed to accelerate certain operations on data arranged in certain ways. This heterogeneity will soon be more extreme, where computing nodes will not just have GPU accelerators, but other application-specific accelerators such as those for key computational motifs, AI, and even quantum-based optimization. GPU-based accelerators on supercomputers in the US Department of Energy (DOE) began with the Roadrunner and Titan systems, then on to the current Summit and Sierra and planned Perlmutter systems, and finally for the first three US exascale systems (Aurora, Frontier, El Capitan). Accelerated node hardware and software architectures are not only here and now but represent our collective future. The US scientific community must be able to effectively exploit these architectures to address urgent problems of National importance. A key response by the DOE to this “call to arms” was the launching of the Exascale Computing Project in 2016, which is an aggressive RD&amp;D project focused on delivery of mission critical applications, an integrated software stack, exascale hardware technology advances and all within the context of an accelerated-node co-design software and algorithm paradigm that is not just for exascale but here to stay for the foreseeable future.</blurb>
    <EventParentName>IP2 Accelerated-Node-Enabled Computational and Data Science: It’s not just for Exascale</EventParentName>
    <external_id>67803-101779</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T08:30:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Development of an Eigen-Analysis Engine for Large-Scale Simulation and Big Data Analysis</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP3</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  1:45PM</starts_at>
    <ends_at>Feb 13 2020  2:25PM</ends_at>
    <EventFilter>PP20|Invited Speaker</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tetsuya Sakurai</EventSpeakers>
    <EventSpeakerUniqueID>719692</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101666</EventHandoutURL>
    <blurb>Large-scale eigenvalue problems arise in a wide variety of scientific and engineering applications such as nano-scale materials simulation, vibration analysis of automobile components, data analysis, graph analysis, etc. In such computations, high-performance parallel eigensolvers are required to exploit distributed parallel computing environments.

In this talk, we present a parallel eigensolver, the Sakurai-Sugiura method (SSM), for large-scale interior eigenvalue problems. This method is derived using numerical quadrature and has good parallel scalability. We also show a software package "z-Pares," which enables users to utilize a large number of computational resources because of its hierarchical parallel structure.

While the presented technology is already well-established, its applications are yet to be fully explored. Hence this talk will also give an overview of challenging issues on the intersection of Big Data analysis and simulation that could be tackled with a scalable eigensolver.
</blurb>
    <EventParentName>IP3 Development of an Eigen-Analysis Engine for Large-Scale Simulation and Big Data Analysis</EventParentName>
    <external_id>67766-101666</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T14:05:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Cognitive Discovery: Pushing the Frontier of Technical R&amp;D with AI</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP6</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  9:25AM</starts_at>
    <ends_at>Feb 15 2020 10:05AM</ends_at>
    <EventFilter>PP20|Invited Speaker</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Costas Bekas</EventSpeakers>
    <EventSpeakerUniqueID>726311</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101776</EventHandoutURL>
    <blurb>Cognitive Discovery is an overarching framework that uses AI to achieve scientific knowledge extraction and representation, to intelligently design and guide simulations, in order to drastically accelerate the pace of scientific discovery. Cognitive Discovery targets to accelerate scientific workflows in technical disciplines and provide a new generation of tools. The workflows follow the cycle: a) Massive literature review in order to understand the problem at hand. Literature refers to all aspects such as mathematical modelling, solution methods, actual computer models and HPC deployment. b) Enrichment of literature data with experimental data and formation of hypotheses. c) Running simulations to test hypotheses and generate new knowledge in order to close any knowledge gaps. All three phases suffer today major disruptions. Simply put: the volume of new literature is exploding (e.g. roughly 450K new publications in materials science are published every year, tens of thousands of papers in numerical and HPC methods need to be reviewed). IoT advances as well as advances in measuring all aspects of HPC systems create an explosion of data. High fidelity models lead to massive configuration spaces the complexity of which clearly outpaces our capability to scale and efficiently run modern HPC systems. We will showcase how AI can help dramatically improve this setting and lead to a massive acceleration for scientific discovery.
</blurb>
    <EventParentName>IP6 Cognitive Discovery: Pushing the Frontier of Technical R&amp;D with AI</EventParentName>
    <external_id>67800-101776</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T09:25:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Intel Software Solutions for Diverse Computing Architectures</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS1</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  1:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sarah Knepper</EventSpeakers>
    <EventSpeakerUniqueID>760197</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101847</EventHandoutURL>
    <blurb>Today's applications are more diverse than ever, as is the hardware used to run and accelerate them. This causes a challenge for developers, as they must juggle multiple code bases, tools, programming languages, and workflows. The oneAPI initiative from Intel is designed to help overcome these challenges, by offering a unified programming model that allows acceleration of the code on various processing architectures. This new standards-based programming model will allow developers to run their workload on existing Intel® Xeon® systems and accelerate portions of it to specialized processors, including GPUs, FPGAs, and other accelerators. Included in the Intel oneAPI portfolio are powerful developer tools like Intel® oneAPI Math Kernel Library.








</blurb>
    <EventParentName>MS1 Industrial Mathematical Software</EventParentName>
    <external_id>67824-101847</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Evaluation of Large Scale Systems with Focus on Application Performance: the Benchmarking Perspective</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS2</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:25PM</starts_at>
    <ends_at>Feb 12 2020  1:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Piotr Luszczek</EventSpeakers>
    <EventSpeakerUniqueID>739657</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101575</EventHandoutURL>
    <blurb>With benchmark-driven performance evaluation, informed by a broad field
experience of involvement with 4 widely known benchmarks in circulation, an
opportunity arises to obtain meaningful measurements of a number of HPC metrics
that relate to applications' behavior.  The scope of these past benchmarks
spans a number of computational patterns that are often directly observed in
scientific codes on both past and modern hardware platforms. This may then
advance the progress of co-design even for early hardware but still with
meaningful insight into the performance space. This was often observed when
careful consideration is given to how the benchmark metrics relate to the
essential aspects of applications' rates of execution.  The extensive adoption
and longevity of these efforts combined with a large volume of results' data
will provide a relevant background on the community process involved in
benchmarking: from basic implementation, through the performance engineering,
deployment efforts, and the eventual adoption for use in applications for proxy
use or to be leveraged for code reuse in critical performance kernels. The
process ends up being beneficial as benchmarks and applications provide
feedback to each other as the early hardware platforms mature to production and
the porting of software on the delivered system is tagged for general use.</blurb>
    <EventParentName>MS2 Meaningful Performance Indicators for Scientific Computing</EventParentName>
    <external_id>67722-101575</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>How to Monitor the Performance Evolution of a Large HPC System : a System and Application Points of View</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS2</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:50PM</starts_at>
    <ends_at>Feb 12 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jean-Christophe Weill</EventSpeakers>
    <EventSpeakerUniqueID>789962</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101576</EventHandoutURL>
    <blurb>To measure the initial performance of a new HPC system, it is usual to use standard benchmarks, like TOP500, Graph500, on the whole system.  During its lifetime, the system is evolving, the hardware is changing, and software, including firmware and processor microcode, also is evolving to correct bugs, security problems and to offer new features. So, inevitably HPC systems' performance is moving. While on one side, it is hardly the case that we can run the standard benchmarks again because it requires monopolizing the system for a too long time. On the other side, the standard benchmark does not represent very well the applications that are running on the system: users' feeling is very different from the benchmarkers' one. 

In this talk, we will introduce the solutions we are implementing that combine the system and the application points of view. 

For the system side, we will discuss benchmarks that enable us to quickly assess the proper functioning and performances of both individual nodes as well as the entire HPC system. We will describe the way those benchmarks are integrated into our large HPC cluster components lifecycle to capture potential issues before they are raised into production systems. 

For the application side, we will discuss how to choose performance indicators that are meaningful from an application user's point of view, and how to choose the right use cases. We will describe our solution to monitor the performance evolution.


</blurb>
    <EventParentName>MS2 Meaningful Performance Indicators for Scientific Computing</EventParentName>
    <external_id>67722-101576</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>High-Resolution Visualization of In Vivo Blood Flow from Low-Resolution MRI Scans using Computational Fluid Dynamics and Optimization</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS4</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  1:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Matthew Zahr</EventSpeakers>
    <EventSpeakerUniqueID>789165</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101623</EventHandoutURL>
    <blurb>Detailed in vivo imaging of the human body using magnetic resonance imaging (MRI) holds great potential for scientific discovery and impact in health care. However, current MRI scanners and flow reconstruction techniques are limited by a fundamental trade-off between resolution, image quality, and scan time. We propose a new method of flow reconstruction that aims to overcome this limitation by integrating computational fluid dynamics (CFD), numerical optimization, and uncertainty quantification into the MRI workflow. Our approach defines an optimization problem that aims to define the boundary conditions and material properties of a high-order CFD simulation that best describes data from low-resolution (fast) MRI scans. The resulting data-certified simulation is subsequently used for flow visualization and to accurately compute clinical biomarkers. To quantify the uncertainty in the reconstruction due to noise in the measurements, we adopt a Bayesian setting and estimate the posterior distribution using implicit sampling. The optimization and sampling procedures are accelerated using adaptive projection-based reduced-order models. We demonstrate the method reconstructs both external and in vivo flows more accurately than standard 4D flow MRI techniques.
</blurb>
    <EventParentName>MS4 Physical-Based Modeling and Machine Learning for Biological and Environmental Sciences - Part I of II</EventParentName>
    <external_id>67751-101623</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Enlarged Krylov Methods and 2-Level Preconditionner for the Map-Making Problem in CMB Data Analysis</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS6</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  2:15PM</starts_at>
    <ends_at>Feb 12 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Thibault Cimic</EventSpeakers>
    <EventSpeakerUniqueID>792594</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101618</EventHandoutURL>
    <blurb>The Cosmological Microwave Background (CMB) data analysis in astro-
physics consists in filtering different noises in the electromagnetic radiation
as a remnant from an early stage of the universe in Big Bang cosmology. In
order to do so and due to a constant need for better sensitivity, the size of
the data sets used to solve this problem grows at Moore's rate. Therefore
suitable, accurate, massively paralellisable algorithms are needed.
We focus at one particular problem for the CMB data analysis, called the
map-making problem, where one standardly seeks for a maximum likelihood
estimator of the signal by solving a generalized least square problem via the
normal equation. Our aim is to develop numerical tools to solve this problem
which includes a study on Krylov methods and preconditioning techniques.
This work fits in the Big Bang from Big Data of the Cosmic Microwave
Background project (B3DCMB).
</blurb>
    <EventParentName>MS6 High Performance Krylov Subspace Methods: Theory, Implementation, and Application - Part I of III</EventParentName>
    <external_id>67748-101618</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Layer-Parallel Approach for Training Deep Neural Networks</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS7</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:50PM</starts_at>
    <ends_at>Feb 12 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Eric Cyr</EventSpeakers>
    <EventSpeakerUniqueID>735268</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101910</EventHandoutURL>
    <blurb>Deep neural networks are a powerful machine learning tool with the capacity to “learn” complex nonlinear relationships described by large data sets. Despite their success training these models remains a challenging and computationally intensive undertaking. In this talk we will present a new layer-parallel training algorithm that exploits a multigrid scheme to accelerate both forward and backward propagation. Introducing a parallel decomposition between layers requires inexact propagation of the neural network. The multigrid method used in this approach stiches these subdomains together with sufficient accuracy to ensure rapid convergence. We demonstrate an order of magnitude wall-clock time speedup over the serial approach, opening a new avenue for parallelism that is complementary to existing approaches. 





</blurb>
    <EventParentName>MS7 Advances in Parallel-in-Time Integration Methods</EventParentName>
    <external_id>67845-101910</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Multigrid Reduction in Time for the Shallow Water Equations using Asymptotic Techniques</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS7</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  2:15PM</starts_at>
    <ends_at>Feb 12 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Nicholas Abel</EventSpeakers>
    <EventSpeakerUniqueID>790058</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101913</EventHandoutURL>
    <EventParentName>MS7 Advances in Parallel-in-Time Integration Methods</EventParentName>
    <external_id>67845-101913</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The Last Centimeter: Trials, Tribulations and Bottlenecks in Getting Data onto the Wire</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS8</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  1:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ian Karlin</EventSpeakers>
    <EventSpeakerUniqueID>752049</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101570</EventHandoutURL>
    <blurb>GPU accelerators are increasing node complexity.  They increase the number of locations data can be sent from between nodes.  New technologies, including GPUDirect and GPUDirect async increase the options of how to send messages.  In this talk, we investigate how performance varies with messaging location, and the performance and coding tradeoffs presented by new technologies.  We show proper configuration and usage are important and messaging bottlenecks on GPUs occur for different reasons than on CPUs.</blurb>
    <EventParentName>MS8 Co-Design of Networking for Scientific HPC Applications</EventParentName>
    <external_id>67721-101570</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>GPU Communication Considerations for a Modern Multi Physics Code</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS8</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  2:15PM</starts_at>
    <ends_at>Feb 12 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Michael Lang</EventSpeakers>
    <EventSpeakerUniqueID>735569</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101573</EventHandoutURL>
    <EventParentName>MS8 Co-Design of Networking for Scientific HPC Applications</EventParentName>
    <external_id>67721-101573</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Smooth Subdivision Multigrid and Large Scale Simulations in Life Science</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS9</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:25PM</starts_at>
    <ends_at>Feb 12 2020  1:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Gillian Queisser</EventSpeakers>
    <EventSpeakerUniqueID>729722</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101826</EventHandoutURL>
    <blurb>We introduce a smooth subdivision theory-based geometric multigrid method. While theory and efficiency of geometric multigrid methods rely on grid regularity, this requirement is often not directly fulfilled in applications where partial differential equations are defined on complex geometries. Instead of generating multigrid hierarchies with classical linear refinement, we propose the use of smooth subdivision theory for automatic grid hierarchy regularization within a geometric multigrid solver. This subdivision multigrid method is compared to the classical geometric multigrid method for two benchmark problems. Numerical tests show significant improvement factors for iteration numbers and solve times when comparing subdivision to classical multigrid. A second study focusses on the regularizing effects of surface subdivision refinement, using the Poisson-Nernst-Planck equations as a model problem. These techniques can be coupled to novel dimension-switching multigrid methods. Various large-scale life-science applications are presented.</blurb>
    <EventParentName>MS9 Parallel Adaptive Multigrid - Part I of II</EventParentName>
    <external_id>67818-101826</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Performance and Accuracy of Mixed-Precision Matrix Factorizations with GPU Tensor Cores</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS10</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:35PM</starts_at>
    <ends_at>Feb 12 2020  3:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Theo Mary</EventSpeakers>
    <EventSpeakerUniqueID>784823</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101535</EventHandoutURL>
    <blurb>The emergence of new hardware supporting low precision floating-point formats, such as half precision, presents great potential for scientific computing. However, it also presents new challenges as algorithms must be adapted to exploit the hardware: in particular, the NVIDIA GPU Tensor Cores are a new type of mixed-precision, block fused multiply-add units.  We propose several matrix factorization algorithms exploiting such units and compare them in terms of performance and accuracy, both theoretically (via their rounding error analysis) and experimentally. Our numerical results using the GPU Tensor Cores give new insights into how to best implement matrix factorizations in a mixed precision setting.</blurb>
    <EventParentName>MS10 Advances in Algorithms Exploiting Low Precision Floating-Point Arithmetic - Part I of II</EventParentName>
    <external_id>67712-101535</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Progress on the Development of Mesh-Based PIC for Fusion Codes</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS12</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:00PM</starts_at>
    <ends_at>Feb 12 2020  4:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Cameron  Smith</EventSpeakers>
    <EventSpeakerUniqueID>731156</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101749</EventHandoutURL>
    <blurb>An important class of multiscale simulation is the Particle-in-Cell (PIC) method in which particle motion is coupled to fields described by PDEs that are discretized over meshes of the domain of interest. The planned exascale computers will provide the computational power required for the PIC method to be effectively applied to a wide range of scientific and engineering problems. For example, PIC methods are central to many ITER fusion tokamak simulations. Due to their ability to deal with very general geometries and support general anisotropic mesh gradations, these simulation codes are increasingly employing unstructured mesh discretizations of the simulation complex reactor domains. The price that has to be paid when using unstructured meshes is the need to employ irregular data structures and core mesh level operations. The need to attain performance on GPUs has only increased the complexity of developing performant unstructured mesh methods. The presentation will give an overview of efforts underway for a set of structures and methods that will support the developers of unstructured mesh PIC codes. The status of the development of the versions of the two fusion plasma PIC codes, XGC edge plasma and GITR for impurity transport will be given. 
</blurb>
    <EventParentName>MS12 Experiences in Developing GPU Support for Department of Energy Math Libraries - Part II of II</EventParentName>
    <external_id>67791-101749</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Portable Performance for AMR on GPUs: The Proto Approach</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS12</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:25PM</starts_at>
    <ends_at>Feb 12 2020  4:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Brian Van Straalen</EventSpeakers>
    <EventSpeakerUniqueID>88922</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101750</EventHandoutURL>
    <blurb>The Chombo Team has built and kept up a structured adaptive mesh refinement C++ package for 20 years now and we have made a good long run with MPI+OpenMP parallelism and a simple patch-based parallelism and ChomboFortran.   Future exascale computing platforms are placing much more emphasis on accelerators.  The programming environments for these heterogeneous computing platforms are still in a great state of flux so the Chombo team has been working on a new abstraction layer Proto to help provide portable high-performance AMR applications across a variety of computer architectures.  The key abstractions are pointwise tensor functions and scalar stencil operations with optimized implementations across multiple programming models.
</blurb>
    <EventParentName>MS12 Experiences in Developing GPU Support for Department of Energy Math Libraries - Part II of II</EventParentName>
    <external_id>67791-101750</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Satellite Precipitation Estimation at Uci Chrs: Algorithm Development &amp; Challenges</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS13</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:00PM</starts_at>
    <ends_at>Feb 12 2020  4:20PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Phu Nguyen</EventSpeakers>
    <EventSpeakerUniqueID>789989</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101629</EventHandoutURL>
    <blurb>Precipitation is a key variable in hydrological processes and varies within time and space. Understanding and monitoring precipitation is crucially important in the human society. Over the past two decades, Precipitation Estimation from Remotely Sensed Information using Artificial Neural Networks (PERSIANN) products, including PERSIANN, PERSIANN-CCS and PERSIANN-CDR, have been produced and incorporated in a wide range of studies and applications.  
While algorithms that exclusively use satellite infrared data as input are attractive owing to their rich spatiotemporal resolution and near-instantaneous availability, their sole reliance on cloud-top brightness temperature (Tb) readings causes over-predictions in wet regions and under-predictions in dry regions—this is especially evident over the Western Contiguous United States (CONUS). We introduce a new algorithm, the Precipitation Estimations from Remotely Sensed Information using Artificial Neural Networks (PERSIANN) Dynamic-Infrared Rain rate model (PDIR), which utilizes climatological data to construct a dynamic (e.g. laterally shifting) Tb-rain rate relationship that better estimates precipitation totals and distributions, notably over the Western CONUS. 
This presentation also introduces the recently developed data and information systems by CHRS including RainSphere, iRain, and DataPortal.


</blurb>
    <EventParentName>MS13 Physical-Based Modeling and Machine Learning for Biological and Environmental Sciences - Part II of II</EventParentName>
    <external_id>67752-101629</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Hardware Architecture Independent Adaptive Mesh Refinement Solver using Trilinos</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS41</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Matthias Frey</EventSpeakers>
    <EventSpeakerUniqueID>790129</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102000</EventHandoutURL>
    <blurb>In this talk we present a hardware architecture independent implementation of an adaptive mesh refinement Poisson solver that is integrated into the
electrostatic Particle-In-Cell beam dynamics code OPAL. The Poisson solver is solely based on second generation Trilinos packages to
ensure the desired hardware portability. We show CPU-benchmarks and report our experience running the code on GPUs.
</blurb>
    <EventParentName>MS41 Trilinos and Hardware Independent Computing (Kokkos)</EventParentName>
    <external_id>67852-102000</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Application of Information Theory in Understanding Hydrological Interactions and Model Diagnostics</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS13</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:25PM</starts_at>
    <ends_at>Feb 12 2020  4:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Edom Moges</EventSpeakers>
    <EventSpeakerUniqueID>789990</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101630</EventHandoutURL>
    <blurb>Watershed model calibration tunes model parameters to information content found in inputs to fit observations. This tuning has mostly been done using different goodness of fit performance measures (e.g. NSCE, percent bias, etc.). However, such tuning is limited in quantifying and providing insights on how information flows among the various input and output variables and thus whether observed processes are adequately represented within the model. By considering output from uncalibrated and calibrated parameterizations of the National Hydrologic Model run with the Precipitation Runoff Modeling System (NHM-PRMS), this study demonstrated the utility of information theory (IT) in characterizing information flow during model calibration. The results in the model diagnostic suggest that the calibration process overfits the observed streamflow by poorly extracting the information content of input precipitation. On the other hand, by applying transfer entropy, a concept from information theory, the strength, memory length and factors controlling the interaction between streamflow and precipitation are demonstrated across 671 watersheds over the conterminous US. Here, it is found that precipitation can explain up to 50% of the dynamics in streamflow. 
</blurb>
    <EventParentName>MS13 Physical-Based Modeling and Machine Learning for Biological and Environmental Sciences - Part II of II</EventParentName>
    <external_id>67752-101630</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Immersive Data Exploration and Analysis of Large Scale CFD Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS14</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:30AM</starts_at>
    <ends_at>Feb 15 2020 11:50AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>701</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ralf-Peter Mundani</EventSpeakers>
    <EventSpeakerUniqueID>726271</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101650</EventHandoutURL>
    <blurb>Due to ever increasing advances in hardware, today's HPC systems allow for an interactive treatment of even complex problems stemming from domains like engineering, medicine, or geo-sciences. Such an approach not only bridges the gap between (often batch-oriented) HPC applications and real-time user interaction, it also paves the way for new types of interactive applications in order to obtain insight -- not numbers! While those in situ approaches are on the rise, very often they suffer from hardware and/or algorithmic limitations hindering fast and efficient visual exploration in order to experience phenomena that would not be possible or accessible in reality.

Within our research, various aspects of different CFD applications (e.g. floods, HVAC) are in the focus of interest. Those applications are often of large scale and, thus, forbid an interactive visual analysis due to their huge data advent. Key feature of our approach is a data exploration technique called sliding window that allows for online/offline visualisations even of huge data sets (up to hundreds of billions of unknowns) during run time. For better visual comprehension of the computed results an immersive virtual reality facility (CAVE) is coupled to the running application. While standing in the CAVE, users have the possibility moving around and zooming into the data, moving forward/backward in time, and manipulating simulation parameters (e.g. boundary conditions) to evaluate different scenarios.
</blurb>
    <EventParentName>MS14 Advanced Visualisation, Analysis, and Parallelisation Concepts for Multi-Scale CFD Simulations in Science and Engineering</EventParentName>
    <external_id>67759-101650</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Asynchronous Programming in Modern C++: What Is An AMT and Why Do You Want One for Christmas?</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS18</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:25PM</starts_at>
    <ends_at>Feb 12 2020  4:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hartmut Kaiser</EventSpeakers>
    <EventSpeakerUniqueID>780269</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101952</EventHandoutURL>
    <blurb>With the advent of modern computer architectures characterized by many-core nodes, deep and complex memory hierarchies, heterogeneous subsystems, and power-aware components, it is becoming increasingly difficult to achieve best possible application scalability and satisfactory parallel efficiency. The community is experimenting with new programming models that rely on finer-grain parallelism, flexible and lightweight synchronization, combined with work-queue-based, message-driven computation. The recently growing interest in the C++ programming language increases the demand for libraries implementing those programming models for the language. We present a new asynchronous C++ parallel programming model that is built around lightweight tasks and mechanisms to orchestrate massively parallel and distributed execution. This model uses the concept of Futures to make data dependencies explicit, employs explicit and implicit asynchrony to hide latencies and to improve utilization, and manages finer-grain parallelism with a work-stealing scheduling system enabling automatic load balancing of tasks. We have implemented such a model as a C++ library exposing a higher-level parallelism API that is fully conforming to the existing C++11/14/17 standards and is aligned with the ongoing standardization work. This API and programming model has shown to enable writing highly efficient parallel applications for heterogeneous resources with excellent performance and scaling characteristics. </blurb>
    <EventParentName>MS18 Exploiting Task Parallelism in Exascale Computing Era</EventParentName>
    <external_id>67850-101952</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Enriched Finite Volume Methods - Taylored Test Spaces for Interface Problems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS19</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:35PM</starts_at>
    <ends_at>Feb 12 2020  3:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Gabriel Wittum</EventSpeakers>
    <EventSpeakerUniqueID>781099</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101828</EventHandoutURL>
    <EventParentName>MS19 Parallel Adaptive Multigrid - Part II of II</EventParentName>
    <external_id>67819-101828</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Model-Driven Tile Optimization for Tensor Contractions</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS20</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ponnuswamy Sadayappan</EventSpeakers>
    <EventSpeakerUniqueID>785697</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101907</EventHandoutURL>
    <blurb>Data movement between the processor and memory hierarchy is a fundamental bottleneck that limits the performance of many applications on modern computer architectures. Tiling and loop permutation are key techniques for reducing data movement in the memory hierarchy. However, selecting effective tile-sizes and permutations of tiling loops is particularly challenging for tensor contractions due to the large number of loops. We describe an analytical model-driven approach to multi-level tile size optimization and permutation selection for tensor contractions.

</blurb>
    <EventParentName>MS20 Frameworks/Libraries for High-Performance Tensor Computations - Part I of II</EventParentName>
    <external_id>67794-101907</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>TAMM: Tensor Algebra for Many-Body Methods</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS20</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:35PM</starts_at>
    <ends_at>Feb 12 2020  3:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sriram Krishnamoorthy</EventSpeakers>
    <EventSpeakerUniqueID>758447</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101909</EventHandoutURL>
    <blurb>Tensor contractions constitute a computationally critical part of many quantum many-body methods. To enable performance-portable implementation of electronic structure methods on the upcoming exascale computers, we have been developing the Tensor Algebra Framework for Many-body Methods (TAMM) runtime framework. In this talk, I will present the design of TAMM and its approach to performance optimization and productive development of new methods. I will also present recent results in efficient implementations of Coupled Cluster methods using TAMM.


</blurb>
    <EventParentName>MS20 Frameworks/Libraries for High-Performance Tensor Computations - Part I of II</EventParentName>
    <external_id>67794-101909</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Hybrid Analytical/Machine-Learning Model to Optimize Tensor-Contractions on GPUs</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS20</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:25PM</starts_at>
    <ends_at>Feb 12 2020  4:45PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Aravind Sukumaran-Rajam</EventSpeakers>
    <EventSpeakerUniqueID>790060</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101916</EventHandoutURL>
    <blurb>Designing performance models for code generators

The high popularity and demand for domain-specific computing and heterogeneous computing have increased the popularity of domain-specific optimizers and code generators. The performance of the generated code depends on the optimization parameters, and thus, selecting the right optimization configuration is of utmost importance to achieve good performance. Unfortunately, the space of valid optimization configurations is often exponential. As an example, consider Tensor Contractions (TC) -- higher-dimensional analogs of matrix multiplication.  TC can be implemented using (i) Direct approach (no transpose) and (ii) TTGT (transpose + GEMM). Along with these algorithmic choices, a TC code generator has to determine the optimal loop permutation and tile size choices. Building an efficient performance model helps to estimate the cost of each such configuration. Reasonably good performance models can be built using machine learning. However, using a pure machine learning based model potentially ignores any domain-specific information. In this talk, we present the design of an analytical model for tensor contractions and how such a model can be combined with machine learning models to select optimal or close to optimal optimization configuration for tensor contractions. </blurb>
    <EventParentName>MS20 Frameworks/Libraries for High-Performance Tensor Computations - Part I of II</EventParentName>
    <external_id>67794-101916</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Particle Sorting for Projection Based Particle Methods</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS22</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>4000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Xiaohu Guo</EventSpeakers>
    <EventSpeakerUniqueID>768164</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101799</EventHandoutURL>
    <blurb>The projection-based particle method including incompressible SPH and moving
particle semi-implicit both solve pressure Poisson equation using
HelmHoltz-Leray decomposition and application of Chorin’s projection method.
Comparing with weakly compressible SPH, projection-based particle methods
generally have higher accuracy for pressure and volume conservation. Due to the
Lagrangian nature of the particle methods, such as SPH or incompressible SPH, the pattern of data access and computation are unknown until the application’s runtime. This often leads to poor temporal and spatial data accesses and insufficient usage of a memory hierarchy. Sorting algorithms are critical techniques to increase applications data locality.

In this talk, we have investigated the effects of the particles sorting for projection based particle method, particularly studied the effects of natural order sorting, Hilbert and Morton space filling curve sorting. The data layout changes because of sorting and implementation procedure have been described in detail. We have found that the particles sorting does not only improve the general SPH kernels performance, but also has an influence on the sparse matrix structure of the pressure Poisson equation. We have also discussed and compared the performance difference between space filling curve sorting methods and natural order sorting both in serial and parallel with typical violent flow.
</blurb>
    <EventParentName>MS22 Parallel Processing for Particle Codes - Part I of II</EventParentName>
    <external_id>67814-101799</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Communication-Avoiding Sparse Direct Solver for CPU+GPU Platforms</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS24</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Richard Vuduc</EventSpeakers>
    <EventSpeakerUniqueID>731023</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101941</EventHandoutURL>
    <EventParentName>MS24 Parallel Matrix Factorization Algorithms - Part I of III</EventParentName>
    <external_id>67847-101941</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Efficient Sparse Triangular Solve</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS24</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 12:10PM</starts_at>
    <ends_at>Feb 13 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sebastien Cayrols</EventSpeakers>
    <EventSpeakerUniqueID>790090</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101978</EventHandoutURL>
    <blurb>In the context of the solution of sparse system of equations using direct methods, many efforts have been dedicated to improve the performance of the factorization. However the triangular solve still suffers from a lack of optimisation. It is especially true when looking at preconditioned iterative methods such as the Preconditioned Conjugate Gradient. The application of the Block Jacobi preconditioner is performed at each iteration whereas the factorization of the diagonal block is done once.
It often results a time to solve led by the application of the preconditioner. This phasis becomes critical when considering a large number of right-hand sides. In this talk, we present the parallelization of the triangular solve phasis using a task based programming model in a the sparse Chokesly solver called SpLLT [Duff, Hogg, and Lopez. Numerical Algebra, Control and Optimization. Volume 8, 235-237, 2018]. Using OpenMP runtime as well as BLAS3 operations, we show that this approach can outperform state-of-the-art solvers such as MKL Pardiso. We also interface SpLLT with Enlarged Conjugate Gradient [Grigori, and Tissot. Research Report RR-9023, Inria] where the number of directions of research is typically 10.</blurb>
    <EventParentName>MS24 Parallel Matrix Factorization Algorithms - Part I of III</EventParentName>
    <external_id>67847-101978</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>AMR Framework for Large-Scale Simulations on Multiple GPUs</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS25</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 12:10PM</starts_at>
    <ends_at>Feb 13 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Takashi Shimokawabe</EventSpeakers>
    <EventSpeakerUniqueID>763637</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101561</EventHandoutURL>
    <blurb>Recently grid-based physical simulations with GPU require effective methods to adapt grid resolution to certain sensitive regions of simulations. An adaptive mesh refinement (AMR) method is one of the effective methods to compute certain local regions that demand higher accuracy with higher resolution. To develop the applications adopting AMR effectively with maintaining high performance on multiple GPUs, we are developing a block-based AMR framework for stencil applications written in C++ and CUDA. The programmer simply describes a C++11 lambda that updates a grid point, which is applied to the entire grids with various resolution over a tree-based AMR data structure effectively. The framework also provides the halo exchange between GPUs based on the temporal blocking method, which contributes to performance improvement. The framework-based application for compressible flow has demonstrated good weak scalability with 84\% of the parallel efficiency on the TSUBAME3.0 supercomputer at Tokyo Institute of Technology. In this talk, we provide the programming model and implementation of the AMR framework for multiple GPUs, and show the computation results of the compressive fluid calculation based on the proposed AMR framework.</blurb>
    <EventParentName>MS25 Progress and Challenges in Extreme Scale Computing and Big Data - Part I of II</EventParentName>
    <external_id>67709-101561</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Stable Extension of the Uk Met Office's Unified Model into the Mesosphere and Lower Thermosphere</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS28</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:20AM</starts_at>
    <ends_at>Feb 13 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Matthew Griffith</EventSpeakers>
    <EventSpeakerUniqueID>785517</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102040</EventHandoutURL>
    <blurb>Forecasting weather in the mesosphere and lower thermosphere (MLT; 85–120km) is of particular interest due to its impact on spacecraft re-entry and radio communications. We extend the current 85km upper boundary on the Met Office's Unified Model (UM) to a height of around 120km. Thus, we shall raise the roof on current numerical weather prediction and pave the way for the development of a whole atmosphere model.

This region has proven to cause particular difficulties for the UM, with a raised upper boundary leading to unstable and inaccurate solutions. Analysis of model output indicated that the lack of consideration of non-Local Thermodynamic Equilibrium (LTE) effects in the radiation scheme leads to anomalous shortwave radiative heating in the thermosphere.

In order to create a stable model for further development, we replace the radiation scheme by nudging towards a climatological temperature profile above 70km. This nudging implementation gives a stable model for the MLT, and provides a robust foundation for further improvements to the model performance.

In particular, the standard vertical resolution is too coarse (&gt;5km) to represent waves that are important for circulation in the MLT. We build on the success of the nudged model by testing at an improved vertical resolution as well as an increased upper boundary height. Additional changes are needed to maintain a stable model such as an increase to the damping of vertical velocities which we discuss here.</blurb>
    <EventParentName>MS28 High-Performance Numerics and Model Development for Geophysical Systems - Part I of II</EventParentName>
    <external_id>67822-102040</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Scientific Computing Benchmarks for Quantum Computers</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS29</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:20AM</starts_at>
    <ends_at>Feb 13 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Travis Humble</EventSpeakers>
    <EventSpeakerUniqueID>736537</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101653</EventHandoutURL>
    <blurb>The burgeoning interest in using quantum computers for modeling and simulation of physical systems raises the question of how to track progress toward the ultimate goal of scientific discovery with quantum computing. Using noisy, intermediate scale quantum computers as platforms for testing and evaluating, we prototype applications in chemistry and materials science to evaluate the accuracy and computational complexity underlying such approaches. We explore the trade-space of algorithm design and platform constraints to define metrics that offer insights into when quantum computers may be ready for performance-driven applications and what must be overcome to reach this capability. These metrics give rises to scientific computing benchmarks that may be used to evaluate both device-level characteristics and application-level behaviors for the next generation of quantum computers. </blurb>
    <EventParentName>MS29 Recent Advances and Trends in Hybrid Quantum-Classical Algorithms - Part I of II</EventParentName>
    <external_id>67762-101653</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Overview of Hybrid Quantum/Classical Methods for Quantum Annealing</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS29</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 11:45AM</starts_at>
    <ends_at>Feb 13 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Catherine McGeoch</EventSpeakers>
    <EventSpeakerUniqueID>776676</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101654</EventHandoutURL>
    <EventParentName>MS29 Recent Advances and Trends in Hybrid Quantum-Classical Algorithms - Part I of II</EventParentName>
    <external_id>67762-101654</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallelisation of the SMARDDA-PFC Software</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS33</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:35PM</starts_at>
    <ends_at>Feb 13 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>4000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Wayne Arter</EventSpeakers>
    <EventSpeakerUniqueID>790026</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101815</EventHandoutURL>
    <blurb>The talk describes work performed on the SMARDDA-PFC software under the SMARTEST project
run by the Eurofusion High Level Support Team.  SMARDDA-PFC is the computational kernel of the
SMITER package for calculating power deposited by escaping particles onto the first wall of
magnetically confined nuclear fusion devices. The magnetic field is sufficiently strong that
particles may be assumed to follow fieldlines to intersection with what is normally a complex
engineered geometry, thus much of the execution cost lies in operations fundamental to ray-tracing.

SMARDDA-PFC consists of 4 programs run sequentially, of which the final program
powcal is the most computationally expensive and takes up between 50\%
and 80\% of the runtime depending on the processor type used.

The aim of this project was to identify bottlenecks in execution time
and optimise the code to reduce run-time.
It was determined that MPI would be most
effective method for achieving large speedup. First the individual
modules were parallelised, then to reduce delays caused by disk traffic
a monolithic combined kernel was produced.
The target of a 10\,s turnround for processing a detailed 360-degree first wall model of
the JET tokamak model (over 2 million triangles) was successfully met.

</blurb>
    <EventParentName>MS33 Parallel Processing for Particle Codes - Part II of II</EventParentName>
    <external_id>67815-101815</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Current State and Future Goals for Multigrid-Preconditioned Linear Solvers on GPU-Based Supercomputers</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS34</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daniel Ibanez</EventSpeakers>
    <EventSpeakerUniqueID>780577</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101888</EventHandoutURL>
    <blurb>We present our efforts over several years to evaluate the latest state-of-the-art multigrid-preconditioned linear solvers to support our MHD modeling efforts on GPGPU machines. We will give updates as to the capabilities and performance of several linear solvers in the context of a Poisson solve with a large range of material constants. We will advocate for our requirements including changing mesh topology, the ability to maintain a grid hierarchy despite changing matrix values, the need to build a multigrid hierarchy on the GPU, and the need for further established capability in preconditioning more complex equation systems such as curl-curl systems.
</blurb>
    <EventParentName>MS34 Advances and Challenges in Solvers on GPGPU-Based High-Performance Computing Architectures - Part II of II</EventParentName>
    <external_id>67834-101888</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Geometric and Algebraic Multigrid Solvers in {PETSc} on Many-{GPU} Supercomputer Architectures</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS34</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:10PM</starts_at>
    <ends_at>Feb 13 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Richard Mills</EventSpeakers>
    <EventSpeakerUniqueID>732925</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101898</EventHandoutURL>
    <blurb>As the high-performance computing community pushes towards the exascale horizon, many supercomputing designs are relying increasingly heavily on general purpose graphics processing unit (GPGPU) accelerators to provide nearly all of their computational power. Simultaneously, scientific application teams are developing increasingly ambitious simulations, many of which---due to their sheer size---will require efficient multilevel methods that possess (near-)asymptotic computational complexity. In this talk, we will discuss ongoing work to adapt geometric and algebraic multigrid implementations in the open-source library PETSc (the Portable, Extensible Toolkit for Scientific Computation) to compute systems with many powerful GPGPUs on each node. We will present performance measurements from Summit, the IBM AC922 at the Oak Ridge Leadership Computing Facility, that contains more than 27,000 NVIDIA Volta GPGPUs and is currently one of the best available proxies for anticipated exascale system designs.

</blurb>
    <EventParentName>MS34 Advances and Challenges in Solvers on GPGPU-Based High-Performance Computing Architectures - Part II of II</EventParentName>
    <external_id>67834-101898</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Innovative Methods for Scientific Computing in the Exascale Era by Integrations of (Simulation+Data+ Learning)</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS36</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kengo Nakajima</EventSpeakers>
    <EventSpeakerUniqueID>715772</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101560</EventHandoutURL>
    <blurb>Towards the end of Moore's law, we need to develop not only new hardware, but also new algorithms and applications. In this study, we propose an innovative method for computational science for sustainable promotion of scientific discovery by supercomputers in the Exascale Era by combining (Simulation + Data + Learning (S+D+L)), where ideas of data science and machine learning are introduced to computational science. The BDEC system (Big Data &amp; Extreme Computing), which is scheduled to be introduced to the Tokyo University in 2021, is a Hierarchical, Hybrid, Heterogeneous (h3) system, which consists of computing nodes for computational science and those for data science/machine learning. In this study, we consider the BDEC as the platform for integration of (S+D+L), develop an innovative software platform “h3-Open-BDEC” for integration of (S+D+L), and evaluate the effects of integration of (S+D+L) on the BDEC. The h3-Open-BDEC is designed for extracting the maximum performance of the supercomputers with minimum energy consumption focusing on (1) innovative method for numerical analysis with high-performance/high-reliability/power-saving based on the new principle of computing by adaptive precision, accuracy verification and automatic tuning, and (2) Hierarchical Data Driven Approach (hDDA) based on machine learning. Integration of (S+D+L) by h3-Open-BDEC enables significant reduction of computations and power consumption, compared to those by conventional simulations.
</blurb>
    <EventParentName>MS36 Progress and Challenges in Extreme Scale Computing and Big Data - Part II of II</EventParentName>
    <external_id>67710-101560</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Event-Driven, Stream-Based Amr Software</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS37</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tobias Weinzierl</EventSpeakers>
    <EventSpeakerUniqueID>762655</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101680</EventHandoutURL>
    <blurb>Dynamically adaptive mesh refinement (AMR) for PDEs on spacetrees can be realised via 
stacks only, if we fuse the mesh traversal and data storage with space-filling 
curves (SFCs). While this yields quasi-optimal cache access characteristics due to the 
curve's H\"older continuity, we end up with a scheme that yields many memory 
moves and some inherent sequentiality even though SFC cuts empirically give us 
good domain decompositions. In this talk, we review stack-based multiscale
AMR and derive a novel traversal paradigm: It analyses the grid structure
on-the-fly and identifies sections along the SFC which remain invariant under the
refinement criteria. A data movement graph along these sections then is assembled, 
on which we eliminate unnecessary data movements and find parallel data access
patterns before we invoke the actual PDE (stencil) operations. The underlying 
algorithmic mindset resembles automata for LR(k) grammars. We end up with a scheme
which is modest w.r.t. memory movements, identifies stencil evaluation concurrency
on-the-fly, and does not impose any block/grid regularity constraints on the mesh.

</blurb>
    <EventParentName>MS37 Challenges in Parallel Adaptive Mesh Refinement - Part II of III</EventParentName>
    <external_id>67771-101680</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>An Evaluation of Asynchronous Task Execution Strategies in AMT AMR Approaches</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS37</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:45PM</starts_at>
    <ends_at>Feb 13 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Martin Berzins</EventSpeakers>
    <EventSpeakerUniqueID>712891</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101682</EventHandoutURL>
    <blurb>The uintah software has used AMR since  2005 and adopted an asynchronous task based approach since 2011. This approach has made it possible to solve AMR problems for  PDE problems with steep gradients and more complex   fluid-structure problems and to use  AMR for the scalable solution of thermal radiation problems which have global connectivity across billions of mesh cells. The relative maturity of this technology allows the advantages and challenges of an asynchronous approach to be considered. It will be shown s that a combination of asynchronicity with over decomposition helps  with scalability and decreases computation time. Finally, we consider how to extend this approach to many different architectures using an intermediate layer to drive performance portability libraries for present and future leading edge architectures.





</blurb>
    <EventParentName>MS37 Challenges in Parallel Adaptive Mesh Refinement - Part II of III</EventParentName>
    <external_id>67771-101682</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A System for Programming Symbolic Manipulations Finite Element Problems in UFL: FML</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS38</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  4:35PM</starts_at>
    <ends_at>Feb 13 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>David Ham</EventSpeakers>
    <EventSpeakerUniqueID>732280</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102036</EventHandoutURL>
    <blurb>The Firedrake system provides a highly automated mechanism for solving a variational problem once the user has decided on spatial and temporal discretisations. But what happens if the user wants to change those choices, for example to switch timesteppers or to change between continuous and discontinuous formulations? Parametrising a variational problem over function space is straightforward UFL, the symbolic language used by Firedrake, and UFL’s symbolic manipulation features enable manipulations such as replacing the test function to implement SUPG. What is missing are the mechanisms required to programmatically reason about which transformations should be applied and to which terms in the forms.Form Manipulation Labelling (FML) is a new Firedrake feature, currently being piloted in the Gusto atmospheric core toolkit, which addresses this lacuna. FML enables terms in forms to be associated with key-value pairs which it calls labels. Reasoning about symbolic manipulations is performed by applying successive filter passes in which labels are added to, or removed from, terms on the basis of form features or the presence and value of other labels. Ultimately, symbolic transformations are also applied as form filters to, for example, drop all terms with a particular label, or perform a replacement on terms with another label.This presentation will describe the design and features of FML, and present some examples, drawn from Gusto, of its capabilities.</blurb>
    <EventParentName>MS38 High-Performance Numerics and Model Development for Geophysical Systems - Part II of II</EventParentName>
    <external_id>67823-102036</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Semi-Automatic DAG Generation for H-Arithmetic</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS40</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ronald Kriemann</EventSpeakers>
    <EventSpeakerUniqueID>780186</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101874</EventHandoutURL>
    <blurb>A new method for generating task graphs for H-matrix arithmetic is presented which is based on the standard recursive functions of sequential H-arithmetic and the block index set information of the involved matrix blocks. With the new DAG generation algorithm, the process of implementing H-arithmetic on many-core CPUs is much simplified compared to previous approaches while preserving the high parallel efficiency. Various options for memory or runtime optimization are presented together with numerical examples.
</blurb>
    <EventParentName>MS40 Accelerating Data Sparse Applications on Massively Parallel Systems - Part II of III</EventParentName>
    <external_id>67826-101874</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>3D FFTs on HPC Many-Core and Hybrid CPU-GPU Platforms: Applications in Materials and Chemistry Codes</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS43</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:20AM</starts_at>
    <ends_at>Feb 14 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Andrew Canning</EventSpeakers>
    <EventSpeakerUniqueID>712761</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101661</EventHandoutURL>
    <blurb>First principles electronic structure calculations based on a plane wave expansion of the wavefunctions are
the most commonly used approach for electronic structure calculations in materials, chemistry and nanoscience applications. In this approach the electronic wavefunctions are expanded in Fourier components and 3D FFTs are used to construct the charge density in real space.  Due to the large amount of communications required in 3D parallel FFTs the
scaling of these application codes on large parallel machines depends critically on having a 3D FFT that scales efficiently
to large processor counts. In order to avoid latency issues as well as reduce communications as much as possible it is necessary to have a well designed 3D FFT that can exploit the specifics of the FFTs required for plane wave codes.  We present research into the best methods of performing these types of parallel 3D FFTs for materials science and chemistry codes on exascale type architectures such as many core CPUs as well as CPU-GPU nodes. I will also discuss the development of 3D FFTs for plane wave codes in the context of the FFTX project.  	 
</blurb>
    <EventParentName>MS43 Parallel Sparse and Conventional FFTs, Applications and Implementation - Part I of II</EventParentName>
    <external_id>67764-101661</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Software Infrastructure for Scalable Data Analysis on HPC Systems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS45</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:20AM</starts_at>
    <ends_at>Feb 14 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Orcun Yildiz</EventSpeakers>
    <EventSpeakerUniqueID>790002</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101667</EventHandoutURL>
    <blurb>Today's scientific applications can run on hundreds of thousands of processors and produce massive amounts of data. While modern HPC systems promise such large scales, scalable software infrastructure is required to employ these systems efficiently for distributed data analysis at scale. In this talk, we will introduce the software tools that constitute this software infrastructure, and present how we apply these tools to the science problems in high-energy physics. These tools include a library for development of scalable parallel algorithms, an in situ middleware for parallel dataflow coupling and composition of scientific workflows, and a distributed workflow system to allow those workflows to run at several independent systems in a wide area. We hope that the lessons learned will increase understanding and motivate further research into scalable data analysis on HPC systems.</blurb>
    <EventParentName>MS45 High Performance Computing in Scientific Applications</EventParentName>
    <external_id>67767-101667</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Surrogate Optimization for HPC Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS45</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 12:10PM</starts_at>
    <ends_at>Feb 14 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Juliane Mueller</EventSpeakers>
    <EventSpeakerUniqueID>775056</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101668</EventHandoutURL>
    <blurb>High performance computing is crucial in many applications important to the Department of Energy for simulating complex physical phenomena, including climate sciences, high energy physics, and combustion research. These simulations usually contain parameters that determine how well the simulation represents reality. Optimizing these parameters is a computationally expensive task as it may take several minutes to hours to run the simulations with a given parameter set. Efficient optimization algorithms that do not rely on derivative information of the simulation objective function are needed. In this talk, we present an overview of surrogate model algorithms, which are commonly used to tackle these types of black-box expensive optimization problems. We discuss the importance of taking different problem characteristics into account when formulating the optimization problem and  during the algorithm development and the potential impact on parallelizing these methods.</blurb>
    <EventParentName>MS45 High Performance Computing in Scientific Applications</EventParentName>
    <external_id>67767-101668</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Revolutionizing HEP Data Storage using HPC Technologies</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS45</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Saba Sehrish</EventSpeakers>
    <EventSpeakerUniqueID>790084</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101972</EventHandoutURL>
    <blurb>In this talk, we will describe the two approaches we have been investigating to
effectively bring HPC technologies to the existing HEP (non-traditional) data
processing. Our focus is on both performance improvement of the processing and
physics-developer/analyzer time to develop these processing tasks. HEP data
processing is inherently shaped by the tools and resources that have been
available to the HEP developers. More access and availability of the HPC
resources and technologies are providing HEP developers with opportunities to
use modern and high performance infrastructures. We are helping to bridge the
gap between current HEP processing and HPC tools.
 
HEP data processing currently revolves around processing a large number of
small files to satisfy grid computing environment constraints. Each file can be
processed independently allowing for file-granularity parallelism. Our first
approach uses MPI parallel IO with HDF5. Use of HDF5 allows us to do parallel
reads and achieve parallelism finer than the granularity of files. Our second
approach involves the use of object stores, thus removing the concept of files
entirely. With either approach, the end user code does not need to change with
the back-end storage we choose use.</blurb>
    <EventParentName>MS45 High Performance Computing in Scientific Applications</EventParentName>
    <external_id>67767-101972</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Low-Latency Mesh-Refinement Cycle Algorithms for Octrees</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS46</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:20AM</starts_at>
    <ends_at>Feb 14 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hansol David Suh</EventSpeakers>
    <EventSpeakerUniqueID>790138</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102009</EventHandoutURL>
    <EventParentName>MS46 Challenges in Parallel Adaptive Mesh Refinement - Part III of III</EventParentName>
    <external_id>67857-102009</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>AMR During PDE-Constrained Optimization using PETSc</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS46</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:45AM</starts_at>
    <ends_at>Feb 14 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tobin Isaac</EventSpeakers>
    <EventSpeakerUniqueID>752526</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102010</EventHandoutURL>
    <EventParentName>MS46 Challenges in Parallel Adaptive Mesh Refinement - Part III of III</EventParentName>
    <external_id>67857-102010</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Warmstarting PFASST Iterations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS47</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 11:15AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sebastian Götschel</EventSpeakers>
    <EventSpeakerUniqueID>784984</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101769</EventHandoutURL>
    <blurb>Adjoint gradient computation for optimization problems governed by parabolic PDEs requires one solve of the state equation and one back\-ward-in-time solve of the adjoint equation, which  makes the iterative optimization process extremely costly. With today's modern computers, the  time-to-solution can be decreased through massive parallelization, which is traditionally done in the spatial dimensions. In addition, time-parallel methods have received increasing interest in recent years. Iterative multilevel schemes such as PFASST (Parallel Full Approximation Scheme in Space and Time) are currently state of the art and can achieve significant parallel efficiency.

As PFASST is based on spectral deferred correction methods for the time integration, their iterative nature allows re-using previously computed solutions in the optimization loop to reduce SDC iterations required for solving state and adjoint equations at the cost of additional storage. We investigate the the impact of warmstarting PFASST iterations on the parallel performance, and discuss the influence of inexact storage of solutions.

This is joint work with Michael Minion (Lawrence Berkeley National Lab).


</blurb>
    <EventParentName>MS47 Parallel-in-Time Integration Methods - Part I of II</EventParentName>
    <external_id>67796-101769</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Task-Based H-Matrix Solver for Distributed Machines Memory with Manycore Nodes</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS48</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:20AM</starts_at>
    <ends_at>Feb 14 2020 11:40AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>David Goudin</EventSpeakers>
    <EventSpeakerUniqueID>769428</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101879</EventHandoutURL>
    <blurb>Hierarchically compressed matrices (H-matrices) are well-known to significantly boost the performance of codes based on the Boundary Element Method (BEM), not only in terms of execution time, but also regarding memory consumption. There are however only few available implementations of direct solvers based Cholesky or LU decompositions) for hierarchically compressed systems, especially on distributed machines equipped with manycore nodes such as Intel KNL boards. This talk will therefore describe our task-based implementation of a H-matrix direct solver targeting super-computers with many-core nodes. A significant effort was devoted to the careful design of a task-based programming model suitable for such irregular hierarchical workloads in a distributed environment.

The first challenge was to efficiently interleave tasks and asynchronous MPI operations, which we solved by the means of interruptible tasks. The second difficulty was to design a synchronization mechanism to efficiently and easily implement recursive compute kernels that access hierarchical pieces of data.  By introducing hierarchical RW-dependencies into our task-based model, we made it possible to seamlessly design such kernels by letting the runtime system automatically derive task dependencies.

Thanks to these techniques, we reached a parallel efficiency of 70\% on our direct H-matrix solver when solving a problem with 4.4 million unknowns over 380 Intel KNLs (24320 cores) of the CEA's machine.


</blurb>
    <EventParentName>MS48 Accelerating Data Sparse Applications on Massively Parallel Systems - Part III of III</EventParentName>
    <external_id>67827-101879</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Approximate and Exact Selection Algorithms on GPUs</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS48</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 11:45AM</starts_at>
    <ends_at>Feb 14 2020 12:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tobias Ribizel</EventSpeakers>
    <EventSpeakerUniqueID>790046</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101880</EventHandoutURL>
    <EventParentName>MS48 Accelerating Data Sparse Applications on Massively Parallel Systems - Part III of III</EventParentName>
    <external_id>67827-101880</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>GPU-Powered Particle-in-Cell Community Frameworks for Laser-Plasma Interaction</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS49</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 12:10PM</starts_at>
    <ends_at>Feb 14 2020 12:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>701</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Axel Huebl</EventSpeakers>
    <EventSpeakerUniqueID>782315</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101526</EventHandoutURL>
    <blurb>In the context of laser-particle acceleration, the electro-magnetic particle-in-cell codes PIConGPU and WarpX are presented.
Novel developments and workflows that enable high-resolution, fast turn-around computations on manycore-powered, leadership-scale supercomputers are essential to make optimal use of upcoming Exascale machines.
Both codes' software libraries and abstractions are build on top of a generalized, single-source programming model (Alpaka) or parallel-for/-reduce based kernels.
While PIConGPU is designed on top of modular, single-purpose libraries, WarpX's core routines are constructed on top of a more monolithic dependency, AMReX, which is a widely used adaptive-mesh refinement framework.

Both particle-in-cell codes share the same challenges for handling PByte-scale data workflows on pre-Exascale machines.
A common, open data format for particle and mesh data (openPMD) avoids duplicating I/O efforts and allows to reuse scalable data workflows with common libraries.
In production runs, close bindings to scripting languages and the Jupyter platform can provide efficient control of simulations, with the goal of fast turn-arounds and good applicability to experiments.
We present standardization efforts and prototypes of both communities with emphasis on reproducibility and flexibility.</blurb>
    <EventParentName>MS49 GPU Computing for Solving Large Scale Scientific Problems</EventParentName>
    <external_id>67664-101526</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Progress on Trilinos-Muelu Smoothed Aggregation Amg for GPU</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS50</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:45PM</starts_at>
    <ends_at>Feb 14 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jonathan Hu</EventSpeakers>
    <EventSpeakerUniqueID>712687</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101756</EventHandoutURL>
    <EventParentName>MS50 Advances in Parallel Sparse Linear System Solver Stacks for Exascale - Part I of II</EventParentName>
    <external_id>67788-101756</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Enhancing Hypre's Semi-Structured Capabilities</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS50</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:35PM</starts_at>
    <ends_at>Feb 14 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Victor Paludetto Magri</EventSpeakers>
    <EventSpeakerUniqueID>790151</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102016</EventHandoutURL>
    <blurb>Multigrid methods are well suited to large massively parallel computer architectures, because they are mathematically optimal and display excellent parallelization properties. Since current architecture trends are favoring regular compute patterns to achieve high performance, the ability to express structure has become much more important. An alternative to standard sparse matrix classes expressed with row and column indices is a semi-structured matrix class that is primarily described in terms of stencils and logically rectangular grids is. The definition of semi-structured rectangular matrices, which are needed as prolongation operators in algebraic multigrid, is however nontrivial. We will discuss our efforts on the new semi-structured matrix class and a semi-structured algebraic multigrid solver built upon it.

</blurb>
    <EventParentName>MS50 Advances in Parallel Sparse Linear System Solver Stacks for Exascale - Part I of II</EventParentName>
    <external_id>67788-102016</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Software Sustainability Lessons from the Fluid Dynamics Community</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS52</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:45PM</starts_at>
    <ends_at>Feb 14 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kenneth Jansen</EventSpeakers>
    <EventSpeakerUniqueID>747207</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101786</EventHandoutURL>
    <blurb>Fluid dynamics is a broad field spanning a large number of science and engineering problem domains that are critical to a wide variety of important applications. To support the fluid dynamics research and applications community, we are executing a community-driven conceptualization of a future institute —Fluid Dynamics Software Infrastructure (FDSI) — to broadly develop, share, and apply computational tools for the generation and analysis of fluid dynamics data from both experimental and computational sources. Though we originally envisioned the institute’s name to be Computational Fluid Dynamics Software Infrastructure, it became apparent in dialog with our community that having the first word in the institute be computational left experimental data (and more importantly experimental researchers) less than equally important.
\linebreak
 
The primary objective is to facilitate the sharing of computational tools and data resources through a rich and extensible set of software components that can be applied with a wide range of existing fluid dynamics analysis tools. Essential to the success of this objective is to sustainably develop software components to analyze data from a wide range of sources.
\linebreak 
 
Two workshops have already been held and additional workshops are planned.  In this talk we will summarize the results of those workshops and discuss the ongoing plans to design FDSI.

</blurb>
    <EventParentName>MS52 Improving Productivity and Sustainability for Parallel Computing Software - Part II of II</EventParentName>
    <external_id>67773-101786</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Fast Parallel Multidimensional FFT Using Advanced MPI</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS53</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:45PM</starts_at>
    <ends_at>Feb 14 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Lisandro Dalcin</EventSpeakers>
    <EventSpeakerUniqueID>731061</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101922</EventHandoutURL>
    <blurb>We present a new method for performing global multidimensional array redistributions required in the implementation of parallel fast Fourier (or similar) transforms. Our method grounds on subarray datatypes and generalized all-to-all scatter/gather from the MPI-2 standard to describe global/local array layouts and perform communication in a single collective call, thus effectively eliminating local ``transpose" steps. We provide a set of compact, self-contained, high-level routines using pure MPI. These routines can be used on top of any sequential or shared-memory multidimensional FFT library to easily provide a simple and performant distributed memory implementation. A series of strong and weak scaling tests confirm our method can match and often surpass in performance other well-established libraries like MPI-FFTW, P3DFFT, and 2DECOMP\&amp;FFT.

</blurb>
    <EventParentName>MS53 Parallel Sparse and Conventional FFTs, Applications and Implementation - Part II of II</EventParentName>
    <external_id>67765-101922</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Compatible Particle Discretization via Generalized Moving Least Squares</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS54</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:10PM</starts_at>
    <ends_at>Feb 14 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Nathaniel Trask</EventSpeakers>
    <EventSpeakerUniqueID>775768</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101855</EventHandoutURL>
    <blurb>While an important tool well suited for particular applications, particle/meshfree methods for solving PDEs have lagged behind their mesh-based counterparts due to the loss of mathematical structures that occur when one gives up a mesh. In this talk we present results from the Compadre project at Sandia (COMpatible PArticle DiscREtization). We introduce a series of new techniques related to: conservative discretization, variational discretizations, surface PDEs and scientific machine learning. Additionally, we introduce the Compadre toolkit - a Trilinos-based package for particle discretizations utilizing Kokkos kernels for hardware acceleration.</blurb>
    <EventParentName>MS54 Particle Methods: Algorithms and Software Technology for Exascale - Part I of III</EventParentName>
    <external_id>67828-101855</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>An Overview of Particles in Amrex, with Applications to Accelerator Modelling, Cosmology, and Multi-Phase Flow</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS54</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:45PM</starts_at>
    <ends_at>Feb 14 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Andrew Myers</EventSpeakers>
    <EventSpeakerUniqueID>762852</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102053</EventHandoutURL>
    <blurb>AMReX is an ECP-funded software framework for developing massively parallel block-structured adaptive mesh refinement (AMR) applications on current and upcoming supercomputing architectures. It provides the basis for the temporal and spatial discretization strategy for a number of applications - including five of the ECP application development projects - spanning fields such as accelerator design, astrophysics, combustion, cosmology, microfluidics, materials science, and multiphase flows.

In addition to the conventional representation of variables on a cell, edge, face, or node-centered mesh, AMReX also provides support for particle data. Particles introduce additional irregularity and complexity to the way data is stored and operated on, requiring special attention in the presence of the dynamically changing hierarchical mesh structure, particularly when sub-cycling in time.

In this talk, I will give an overview of the particle capabilities provided by AMReX and how those capabilities are used by application codes, with a particular focus on recent work towards supporting hybrid CPU/GPU platforms. Topics will include data layout, the parallel communication of particle data, including both redistribution and ghost exchange, neighbor list construction for particle-particle collisions, particle-mesh operations, and the parallel reduction of particle data. 

	</blurb>
    <EventParentName>MS54 Particle Methods: Algorithms and Software Technology for Exascale - Part I of III</EventParentName>
    <external_id>67828-102053</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Optimising Time to Solution of Finite Volume and Discontinuous Galerkin Tsunami Models in Sam(oa)²</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS56</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Leonhard Rannabauer</EventSpeakers>
    <EventSpeakerUniqueID>774939</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101542</EventHandoutURL>
    <EventParentName>MS56 HPC Aspects of Tsunami Simulation</EventParentName>
    <external_id>67714-101542</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Kokkos Kernels : A Performance Portable Library for Linear Algebra and Graph Algorithms</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Siva Rajamanickam</EventSpeakers>
    <EventSpeakerUniqueID>716029</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102070</EventHandoutURL>
    <blurb>Kokkos Kernels is a library in the Kokkos ecosystem for performance portable sparse/dense linear algebra and graph kernels. Both the performance and the portability aspects are key to the stakeholders of the product.  The two main focuses of the project are performance portability and delivering robust software to computational science and engineering applications. This poster outlines the key functionality in the library along with performance of newly developed kernels on CPUs and GPUs. </blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-102070</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Improving Tsunami-HySEA as FTRT Simulator in the Framework of TEWS</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS56</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:35PM</starts_at>
    <ends_at>Feb 14 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jose Manuel Gonzalez-Vida</EventSpeakers>
    <EventSpeakerUniqueID>728716</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101545</EventHandoutURL>
    <blurb>In the framework of Tsunami Early Warning Systems (TEWS) is being very important the new FTRT (faster than real time) tsunami computations provided by numerical models as Tsunami-HySEA. To have available greatly improved and highly efficient computational methods are the first raw ingredient to achieve extremely fast and effective calculations for these kinds of hazards.

HPC facilities have the role to bring this efficiency to a maximum possible while drastically reducing computational times. In this work, we present the improvements achieved in the Tsunami-HySEA model in the Pilot PD2 in the context of the ChEESE (Center of Excellence (CoE) in the domain of Solid Earth (SE)) European Research Project.

This pilot aim is to increase the size of the problems by increasing spatial resolution and/or producing longer simulations while still computing FTRT for TEWS including inundation for a particular target coastal zone.

Some examples of the achieved progresses will be shown.

Acknowledgements: This research has been partially supported by ChEESE project (EU Horizon 2020, grant agreement Nº 823844). 


</blurb>
    <EventParentName>MS56 HPC Aspects of Tsunami Simulation</EventParentName>
    <external_id>67714-101545</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Parallel Shift-Invert Spectrum Slicing for Symmetric Self-Consistent Eigenvalue Computation</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS57</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:35PM</starts_at>
    <ends_at>Feb 14 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>David Williams-Young</EventSpeakers>
    <EventSpeakerUniqueID>785504</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101724</EventHandoutURL>
    <blurb>The central importance of large scale eigenvalue problems in scientific computation necessitates the development massively parallel algorithms for their solution. Recent advances in dense numerical linear algebra have enabled the routine treatment of eigenvalue problems with dimensions on the order of hundreds of thousands on the world's largest supercomputers. In cases where dense treatments are not feasible, Krylov subspace methods offer an attractive alternative due to the fact that they do not require storage of the problem matrices. However, demonstration of scalability of either of these classes of eigenvalue algorithms on computing architectures capable of expressing excessive parallelism is non-trivial due to communication requirements and serial bottlenecks, respectively. In this work, we introduce the SISLICE method: a parallel shift-invert algorithm for the solution of the symmetric self-consistent field (SCF) eigenvalue problem. The SISLICE method drastically reduces the communication requirement of current parallel shift-invert eigenvalue algorithms through various shift selection and migration techniques based on density of states estimation and k-means clustering, respectively. This work demonstrates the robustness and parallel performance of the SISLICE method on a representative set of SCF eigenvalue problems and outlines research directions which will be explored in future work.

</blurb>
    <EventParentName>MS57 Parallel Eigenvalue Algorithms for Physical Simulation - Part I of III</EventParentName>
    <external_id>67779-101724</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Multigrid-in-Time SQP Methods for PDE-Constrained Optimization</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS58</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:45PM</starts_at>
    <ends_at>Feb 14 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Denis  Ridzal</EventSpeakers>
    <EventSpeakerUniqueID>723670</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101773</EventHandoutURL>
    <blurb>Parallel solution of optimal control problems modeled by time-dependent PDEs is
limited by the serial bottleneck of forward and backward (adjoint) time integration.
To enable scalability in the time dimension, we introduce a multigrid-in-time solver for
a special type of optimality systems, which maintains the coupling of forward and
backward time integration in each time subdomain.
The solver is used as the main computational kernel in composite-step sequential
quadratic programming (SQP) methods for nonlinear optimization with inequality
constraints, where the constraints are handled through an augmented-Lagrangian approach.
We examine the weak and strong scaling of the multigrid-in-time SQP approach on
optimal control problems in electromagnetics and fluid dynamics applications.</blurb>
    <EventParentName>MS58 Parallel-in-Time Integration Methods - Part II of II</EventParentName>
    <external_id>67797-101773</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Multigrid for Shifted Systems Appearing in Parallel-in-Time Integration</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS58</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:10PM</starts_at>
    <ends_at>Feb 14 2020  4:30PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Matthias Bolten</EventSpeakers>
    <EventSpeakerUniqueID>720359</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101774</EventHandoutURL>
    <blurb>Implicit parallel time-intgration methods in many cases require to solve shifted linear systems. The shift depends on the basic time-integration method, e.g., in contour integration, and influences the spectral properties of the system. To solve the arising systems effectively, multigrid methods can be used. In the case of structured discretizations the arising matrices allow for a detailed analysis of the solver and thus its improvement. Further, the presence of structure can be exploited to obtain efficient, scalable implementations of the spatial solver. In this talk the appearing discrete operators and their spectral properties will be presented. Based on this, the analysis and design of suitable multigrid methods to takle the arising linear systems will be discussed.
</blurb>
    <EventParentName>MS58 Parallel-in-Time Integration Methods - Part II of II</EventParentName>
    <external_id>67797-101774</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The Resilience Problem in Extreme Scale Computing</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS59</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  3:40PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christian Engelmann</EventSpeakers>
    <EventSpeakerUniqueID>712668</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101713</EventHandoutURL>
    <blurb>Resilience is one of the critical challenges of extreme-scale high-performance computing (HPC) systems, as component counts increase, individual component reliability decreases, and software complexity increases. Building a reliable supercomputer that achieves the expected performance within a given cost budget and providing efficiency and correctness during operation in the presence of faults, errors, and failures requires a full understanding of the resilience problem. This talk provides an overview of recent achievements in developing a taxonomy, catalog and models that capture the observed and inferred fault, error, and failure conditions in current supercomputers and in extrapolating this knowledge to future-generation systems.
</blurb>
    <EventParentName>MS59 Resilience and Fault Tolerance for Extreme Computing Systems - Part I of III</EventParentName>
    <external_id>67783-101713</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>CP7 Proceedings Papers - Part II of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP7</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  5:00PM</ends_at>
    <EventFilter>PP20|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68614</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68614-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>CP8 Numerical Methods for Flow Simulations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP8</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 12:35PM</ends_at>
    <EventFilter>PP20|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68196</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68196-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>CP9 Autotuning</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP9</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 12:35PM</ends_at>
    <EventFilter>PP20|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68197</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68197-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Intermission</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  4:50PM</starts_at>
    <ends_at>Feb 12 2020  5:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>NA</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68166-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T16:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Intermission</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  5:00PM</starts_at>
    <ends_at>Feb 13 2020  5:15PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>NA</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68177-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T17:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Intermission</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  9:15AM</starts_at>
    <ends_at>Feb 13 2020  9:25AM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>NA</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68172-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T09:15:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The Mathematical Analysis of Faults and the Resilience of Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS59</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  4:35PM</starts_at>
    <ends_at>Feb 14 2020  4:55PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Laura Monroe</EventSpeakers>
    <EventSpeakerUniqueID>788635</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101714</EventHandoutURL>
    <blurb>As the post-Moore’s-Law era advances, faults are expected to increase in number and in complexity on emerging novel devices. This will happen on exascale and post-exascale architectures due to smaller feature sizes, and also on new devices with unusual fault models. Attention to error-correction and resilience will thus be needed in order to use such devices effectively. Known mathematical error-correction methods may not suffice under these conditions, and an ad hoc approach will not cover the cases likely to emerge, so mathematical approaches will be essential. We will discuss the mathematical underpinnings behind such approaches, illustrate with examples, and emphasize the interdisciplinary approaches that combine experimentation, simulation, mathematical theory and applications that will be needed for success.</blurb>
    <EventParentName>MS59 Resilience and Fault Tolerance for Extreme Computing Systems - Part I of III</EventParentName>
    <external_id>67783-101714</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Using Block-Low Rank Techniques for Large Finite Element Industrial Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS60</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:45PM</starts_at>
    <ends_at>Feb 14 2020  4:05PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Francois-Henry Rouet</EventSpeakers>
    <EventSpeakerUniqueID>775732</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101997</EventHandoutURL>
    <blurb>Block-Low Rank matrices (BLR) have attracted a lot of attention in recent years. BLR matrices are dense matrices with low-rank blocks, and they are a subset of the widely-known family of hierarchical matrices (H-matrices); they can be used to design fast solvers and preconditioners. They are easy to work with and have been shown to be applicable to a wide variety of problems. BLR techniques have been implemented in different parallel sparse direct solvers, MUMPS among others; they are used to approximate and accelerate the dense kernels that appear at each step of the sparse factorization. In this presentation, we investigate the use of BLR techniques for problems arising from finite element industrial applications. We use the multiphysics code LS-DYNA and a set of large real-life problems from different applications: implicit structural mechanics (linear and nonlinear, static and dynamic), heat transfer, incompressible fluid flow, and electromagnetics. For each of these problems, LS-DYNA uses MUMPS (and its BLR feature) as a preconditioner for iterative solvers. We investigate different algorithmic parameters of BLR techniques (in particular the compression threshold) and compare BLR against different preconditioners (incomplete factorization, algebraic multigrid...). We demonstrate that BLR techniques are very robust, a key requirement for industrial applications.</blurb>
    <EventParentName>MS60 Low-Rank Compression-Based Fast Sparse Direct Solvers</EventParentName>
    <external_id>67831-101997</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Ceed/vt LibParaNumAl High-Order Matrix-Free AMG on AMD-GPU</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS61</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 11:00AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Noel Chalmers</EventSpeakers>
    <EventSpeakerUniqueID>744667</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101759</EventHandoutURL>
    <EventParentName>MS61 Advances in Parallel Sparse Linear System Solver Stacks for Exascale - Part II of II</EventParentName>
    <external_id>67789-101759</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Asynchronous Jacobi-Richardson,  and Gauss-Seidel Smoothers for Hypre One-Synch FGMRES-AMG</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS61</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:05AM</starts_at>
    <ends_at>Feb 15 2020 11:25AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Stephen Thomas</EventSpeakers>
    <EventSpeakerUniqueID>710829</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101762</EventHandoutURL>
    <blurb>We present recent advances in low-sync Krylov iterative solvers and algebraic multigrid (AMG) preconditioners for the {\it hypre} and Trilinos linear system solver stacks.  The single-reduce orthogonalization algorithms derived by \'{S}wirydowicz et al (2019) maintain the numerical stability of the original Gram-Schmidt and GMRES algorithms . These have been extended to pipelined and s-step GMRES by Yamazaki et al (2020).  
Asynchronous Jacobi-Richardson and Gauss-Seidel smoothers avoid sparse triangular solves and are combined with a low-sync FGMRES-AMG solver.  The set-up time for computing these smoothers on the NVIDIA Volta V100 GPU is minimal and can be combined with matrix assembly on the GPU.  The multi-MPI and multi-GPU implementation of the Nalu-Wind FGMRES-AMG solver results in significant reductions in the compute time per solver iteration and extends strong scaling roll-off by at least 4x on the NREL Eagle and ORNL Summit supercomputers.







</blurb>
    <EventParentName>MS61 Advances in Parallel Sparse Linear System Solver Stacks for Exascale - Part II of II</EventParentName>
    <external_id>67789-101762</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Oversubscription and Your Data, How User Level Scheduling Can Increase Data Flow</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS63</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 11:00AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>801</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Noah Evans</EventSpeakers>
    <EventSpeakerUniqueID>780331</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101763</EventHandoutURL>
    <EventParentName>MS63 Data-Centric Operating Systems and Runtimes</EventParentName>
    <external_id>67775-101763</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Operating System Support for Intelligent Management of Heterogeneous Memory</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS63</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:55AM</starts_at>
    <ends_at>Feb 15 2020 12:15PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>801</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Balazs Gerofi</EventSpeakers>
    <EventSpeakerUniqueID>780336</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101766</EventHandoutURL>
    <EventParentName>MS63 Data-Centric Operating Systems and Runtimes</EventParentName>
    <external_id>67775-101766</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Hpc_td_tbd_battaglino</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS65</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:05AM</starts_at>
    <ends_at>Feb 15 2020 11:25AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Casey Battaglino</EventSpeakers>
    <EventSpeakerUniqueID>769246</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101642</EventHandoutURL>
    <EventParentName>MS65 High-Performance Tensor Computation and Applications - Part II of III</EventParentName>
    <external_id>67757-101642</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>An Efficient Contour Integral Based Eigensolver for Surface Plasmon Simulations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS66</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:30AM</starts_at>
    <ends_at>Feb 15 2020 11:50AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Huang Tsung-Ming</EventSpeakers>
    <EventSpeakerUniqueID>758350</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101601</EventHandoutURL>
    <blurb>Numerical simulations play a significant role for studying the properties of surface plasmon. The surface plasmon problem is first modelled by the Maxwell equations, and the equations is then discretized by the widely-used Yee’s scheme. After applying certain similarity transformations to the discretized system, the original simulation problem becomes a clustered non-Hermitian eigenvalue problem. An efficient contour integral (CI) based eigensolver is developed to overcome the difficulties of applying current existing methods to solve eigenvalues in particular designated regions for this problem. This efficient method combines the contour integral, the fast matrix-vector multiplication and efficient linear system solver. The numerical results can show the efficiency of solving linear systems and eigenvalues with the efficient CI eigensolver.</blurb>
    <EventParentName>MS66 New Approaches for Software Auto-Tuning and Accuracy Assurance - Part I of II</EventParentName>
    <external_id>67742-101601</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Eigensolvers for Ab Initio CI Calculations in Nuclear Physics</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS67</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:05AM</starts_at>
    <ends_at>Feb 15 2020 11:25AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Pieter Maris</EventSpeakers>
    <EventSpeakerUniqueID>752008</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101700</EventHandoutURL>
    <blurb>I describe a recently developed block iterative method to efficiently obtain the lowest eigenpairs of large sparse symmetric matrices that arise in ab initio configuration interaction (CI) calculations for atomic nuclei.  The dimensions of these matrices can be in the (tens of) billions, and one therefore needs efficient algorithms and implementations for large-scale high-performance computing platforms.  Rapid convergence of the block iterative method is achieved by a suitable choice of starting guesses of the eigenvectors and the construction of an effective distributed preconditioner.  The use of a block method leads to greater concurrency and increased arithmetic intensity of the computation, and allows us to take advantage of e.g. the vector units on intel's xeon phi processors.  I also discuss the implementation details that are critical to achieving high performance on massively parallel multi-core supercomputers, and demonstrate that this block iterative solver is two to three times faster than the Lanczos algorithm for problems of moderate sizes on a Cray XC40 system.</blurb>
    <EventParentName>MS67 Parallel Eigenvalue Algorithms for Physical Simulation - Part II of III</EventParentName>
    <external_id>67780-101700</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Adaptive Step Size Strategies for Line Search Methods and their Applications to Electronic Structure Calculations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS67</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:30AM</starts_at>
    <ends_at>Feb 15 2020 11:50AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Xiaoying Dai</EventSpeakers>
    <EventSpeakerUniqueID>751672</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101711</EventHandoutURL>
    <blurb>In this talk, we will introduce an adaptive step size strategy for a class of line search methods for orthogonality constrained minimization problems, which avoids the classic backtracking procedure. We prove the convergence of the line search methods equipped with our adaptive step size
strategy under some mild assumptions.We then apply the adaptive algorithm to electronic structure calculations, which show that our strategy is efficient and recommended. This is a joint work with Liwei Zhang and Aihui Zhou. 
</blurb>
    <EventParentName>MS67 Parallel Eigenvalue Algorithms for Physical Simulation - Part II of III</EventParentName>
    <external_id>67780-101711</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Alleviating the Memory Pressure for Seismic Modeling and Imaging</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS68</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 11:00AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>1900</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Rached Abdelkhalak</EventSpeakers>
    <EventSpeakerUniqueID>785310</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101917</EventHandoutURL>
    <blurb>We revisit the high performance implementations of the first order and second order formulations of the 3D acoustic wave equation using two cache blocking techniques: spatial blocking (SB) and multicore wavefront diamond temporal blocking (MWD-TB) techniques. Solving the acoustic wave equation is cornerstone of seismic modeling, seismic imaging  and seismic inversion. Various hardware platforms are considered to assess the performance impact of each optimization technique. </blurb>
    <EventParentName>MS68 Advanced HPC Trends Oil and Gas Applications - Part I of II</EventParentName>
    <external_id>67846-101917</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>The Many Faces of Simulation for HPC</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS69</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 11:00AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Frédéric Suter</EventSpeakers>
    <EventSpeakerUniqueID>780522</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101738</EventHandoutURL>
    <blurb>In this talk, we will recall how simulation has been widely used in the field of HPC research and development, to evaluate and compare the performance of application implementations and of the algorithms therein. We will provide a rapid survey of the tools developed to this end over the last decade. Then we will explain how recent advances in the simulation methodologies at the core of available simulation frameworks open the way for the study of other compelling use cases beyond performance analysis. 
</blurb>
    <EventParentName>MS69 The Many Faces of Simulation for HPC - Part I of II</EventParentName>
    <external_id>67786-101738</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>New Non-Blocking Extensions to the ULFM Proposal</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS70</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 11:00AM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>George Bosilca</EventSpeakers>
    <EventSpeakerUniqueID>747244</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101719</EventHandoutURL>
    <blurb>This talk presents new asynchronous extensions to the User Level Failure Mitigation (ULFM) MPI Standard draft proposal. The ULFM proposal, under evaluation by the MPI Forum's Fault Tolerance Working Group, support the continued operation of MPI programs after permanent failures have impacted the execution. The key principle is that no MPI call (point-to-point, collective, RMA, IO, ...) can block indefinitely after a failure, but must either succeed or raise an MPI error. The new extensions will allow applications to construct non-blocking faster-changing worlds, and fulfill the resilient needs or more dynamic application.

</blurb>
    <EventParentName>MS70 Resilience and Fault Tolerance for Extreme Computing Systems - Part II of III</EventParentName>
    <external_id>67784-101719</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Resilience in the Context of GPUs: A Technique for Interrupting GPU Kernels</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS70</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 11:55AM</starts_at>
    <ends_at>Feb 15 2020 12:15PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Max Baird</EventSpeakers>
    <EventSpeakerUniqueID>790011</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101725</EventHandoutURL>
    <blurb>
In computing, resilience is a way of maintaining software operability in
the face of unanticipated faults that challenge normal execution. Resilience
strategies are typically employed in systems comprised of many computers
as it is more likely to suffer faults when computing at this scale. In recent
years, these systems have been massively augmented with graphics process-
ing units (GPUs) to accelerate tasks. GPUs present a new challenge to
resilience by being specialized pieces of hardware which do not follow the
traditional execution model relied upon by established strategies. Further,
new or retrofitted strategies quickly become obsolete as they typically rely
on undocumented behaviour which often later changes due to the rapidly
evolving GPU landscape.

This work proposes a new approach to checkpointing MPI applications
that spend the majority of their execution time on the GPU. It is possible
to take snapshots of data residing on GPUs without waiting for kernels to
complete. The proposed technique is implemented in the context of FTI,
a state-of-the-art high performance fault tolerance library. The result is an
elegant solution for developing resilient MPI applications where kernels run
longer than the mean time between hardware failures.</blurb>
    <EventParentName>MS70 Resilience and Fault Tolerance for Extreme Computing Systems - Part II of III</EventParentName>
    <external_id>67784-101725</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>XSDK: Toward Efficient and Interoperable Scientific Library Collection</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS71</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:15PM</starts_at>
    <ends_at>Feb 15 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Asim YarKhan</EventSpeakers>
    <EventSpeakerUniqueID>785198</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101703</EventHandoutURL>
    <blurb>With the development of increasingly complex architectures and software due to multiphysics modeling, and the coupling of simulations and data analytics, applications increasingly require the combined use of software packages developed by diverse, independent teams throughout the HPC community. The Extreme-scale Scientific Software Development Kit (xSDK) is being developed to provide such an infrastructure of independent mathematical libraries to support rapid and efficient development of high-quality applications. 
This presentation will introduce the xSDK, its history and discuss in more detail the development and impact of the xSDK community policies, which were defined to achieve improved code quality and compatibility across xSDK member packages and constitute an integral part of the xSDK.
 


</blurb>
    <EventParentName>MS71 Toward Efficient Software Integration and Deployment</EventParentName>
    <external_id>67782-101703</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Faster Spack Package Manager Installations through Task Parallelism</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS71</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  3:05PM</starts_at>
    <ends_at>Feb 15 2020  3:25PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Samuel Knight</EventSpeakers>
    <EventSpeakerUniqueID>763821</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101709</EventHandoutURL>
    <blurb>Package managers, containers, automated testing, and Continuous Integration (CI), are becoming an essential part of HPC development workflows. These automated tools often require re-compiling entire software stacks from source and spend a significant amount of time and computing resources just for compilation and integration. However, large software stacks such as those deployed on HPC clusters can have complex combinatorial dependencies, and may take a system several days to compile.
Despite the use of simple parallelization (such as 'make -j'), build execution time often do not scale with system resources.
For such cases, it is possible to improve overall installation time by compiling parts of software stack independently, each scheduled on a subset of available cores.
We apply malleable-task scheduling algorithms to better exploit available parallelism in build system workflows and improve stack build time overall.
Using a prototype implementation in the Spack package manager, malleable-task scheduling can improve build times by more than 2x.
</blurb>
    <EventParentName>MS71 Toward Efficient Software Integration and Deployment</EventParentName>
    <external_id>67782-101709</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Taking the Plasma Physics Code XGC to Summit and Beyond with Kokkos/Cabana</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS73</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:40PM</starts_at>
    <ends_at>Feb 15 2020  3:00PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Aaron Scheinberg</EventSpeakers>
    <EventSpeakerUniqueID>790042</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101865</EventHandoutURL>
    <blurb>XGC is a particle-in-cell code that models plasma physics in fusion devices. To achieve legible and sustainable source code that runs with optimized performance across platforms, we adapted XGC to utilize Kokkos, a portability framework that manages data layout and execution strategies, and Cabana, a library built on Kokkos within the ECP-CoPA project with a focus on optimizations specific to particle codes. To further complicate things, XGC is a Fortran code, while Kokkos and Cabana are in C++. Building additional interfaces to make use of these libraries thus posed an additional challenge. Here we summarize our experience adopting them. We report optimized performance on Summit and on Cori supercomputers, demonstrating a viable path for Fortran codes seeking cross-platform performance. However, we also point to the limitations of this strategy and present our progress towards implementing a C++ version of XGC in preparation for future architectures.</blurb>
    <EventParentName>MS73 Particle Methods: Algorithms and Software Technology for Exascale - Part III of III</EventParentName>
    <external_id>67830-101865</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Pumipic: Infrastructure for Unstructured Mesh Pic on GPUs</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS73</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  3:05PM</starts_at>
    <ends_at>Feb 15 2020  3:25PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Gerrett Diamond</EventSpeakers>
    <EventSpeakerUniqueID>763345</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101867</EventHandoutURL>
    <blurb>Designing an efficient infrastructure for unstructured mesh particle-in-cell, PIC, simulations running on GPUs requires precise approaches to promote performance and ease of development for implementing physical simulations. This talk presents the PUMIPic library that provides mesh-centric data structures and operations for unstructured mesh PIC simulations. PUMIPic uses a GPU based unstructured mesh library, Omega, with a generalized particle data structure designed to support a range of PIC simulations. The design of both structures has been strongly influenced by need for performant parallel execution of PIC operations that involve irregular data dependencies when performing the PIC operations that involve mesh-article interactions. To ensure the ability to scale to any desired problem size, both the mesh and particle data can be distributed. Even with the mesh-based PUMIPic structures that tie the particles to the mesh, supporting the distributed mesh complicates the algorithms needed to scale on today’s heterogeneous systems. The methods being developed to minimize data movement/communication and support load effective balancing will be discussed.  Performance results will be presented for execution on the Summit system at Oak Ridge National Laboratory. </blurb>
    <EventParentName>MS73 Particle Methods: Algorithms and Software Technology for Exascale - Part III of III</EventParentName>
    <external_id>67830-101867</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Particle-in-Cell Simulations at Exascale</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS73</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:15PM</starts_at>
    <ends_at>Feb 15 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Robert Bird</EventSpeakers>
    <EventSpeakerUniqueID>790177</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102039</EventHandoutURL>
    <blurb>Current and future exascale systems are introducing unparalleled levels of heterogeneity, with planned Department of Energy (DoE) alone offering at least three radically different hardware configurations. This presents a challenge for all codes, especially particle simulations which rely on high levels of performance and large particle counts to achieve the desired level of scientific fidelity. Increasingly, we are seeing a shift from code developers deploying platform specific optimizations to a focus on achieving portable-performance as code developers need to be able to run on all available platforms.

In this work we present a performance-portable version of the VPIC which aims to deliver high-performance on current and up-coming DoE systems through the deployment of the Kokkos programming model. This gives us a single, easy to read, code base that is not obfuscated by platform specific optimizations.  In so doing, we aim to allow code users and physicists to focus on scientific research, and removing the focus on code development. 
As part of this work we offer a study into performance-portability across different hardware, and demonstrate the codes ability to scale successfully across both Summit and Sierra. Finally, we offer some broader picture insights for Particle-in-Cell codes as a whole as we head into the exascale era.</blurb>
    <EventParentName>MS73 Particle Methods: Algorithms and Software Technology for Exascale - Part III of III</EventParentName>
    <external_id>67830-102039</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Computing Generalized CP Decompositions on Emerging Parallel Architectures</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS74</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Eric Phipps</EventSpeakers>
    <EventSpeakerUniqueID>708009</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101644</EventHandoutURL>
    <blurb>In this talk we describe the computation of generalized canonical polyadic (GCP) tensor decompositions of large, sparse tensors on emerging manycore architectures such as multicore CPUs, Intel Xeon Phi, and Nvidia GPUs.  In particular we describe the implementation of numerical sampling and gradient kernels arising in the computation of GCP decompositions using stochastic gradient descent algorithms using a software tool called Kokkos that enables a single software implementation of these kernels that is portable to and performant on a variety of contemporary manycore architectures.  Performance of GCP decompositions on several data sets and hardware platforms will be presented.  All of the numerical methods described have been encapsulated into a publicly available software tool called GenTen.</blurb>
    <EventParentName>MS74 High-Performance Tensor Computation and Applications - Part III of III</EventParentName>
    <external_id>67758-101644</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Precision Tuning of the Arithmetic Units in Matrix Multiplication on FPGA</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS75</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yiyu Tan</EventSpeakers>
    <EventSpeakerUniqueID>780219</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101603</EventHandoutURL>
    <blurb>Data precision affects memory demand and computation performance. However, current computer systems only support limited data formats, such as single precision, double precision, and users can not customize their own data format. This results in low efficiency of computation resources. In this research, the precision tuning of data in matrix multiplication on FPGA is investigated, and its impacts on system performance are evaluated, including computation performance, energy efficiency, and hardware resource utilization.</blurb>
    <EventParentName>MS75 New Approaches for Software Auto-Tuning and Accuracy Assurance - Part II of II</EventParentName>
    <external_id>67743-101603</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Reproducible Linear Algebra from Application to Architecture</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS75</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:15PM</starts_at>
    <ends_at>Feb 15 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jason Riedy</EventSpeakers>
    <EventSpeakerUniqueID>706689</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101604</EventHandoutURL>
    <blurb>All computing must be parallel to take advantage of modern systems like multicore processors, GPUs, and distributed systems. Results that are not bit-wise reproducible introduce doubt on many levels. Sometimes that is appropriate. Reproducibility limitations occur because underlying libraries do not specify their reproducibility requirements. New advances in interfaces, algorithms, and architectures allow selecting among those requirements in the future. This talk covers many of the upcoming options and their trade-offs.</blurb>
    <EventParentName>MS75 New Approaches for Software Auto-Tuning and Accuracy Assurance - Part II of II</EventParentName>
    <external_id>67743-101604</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>IP2 Accelerated-Node-Enabled Computational and Data Science: It’s not just for Exascale</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  8:30AM</starts_at>
    <ends_at>Feb 13 2020  9:15PM</ends_at>
    <EventFilter>PP20|Invited Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67803</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>67803-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T08:30:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>IP3 Development of an Eigen-Analysis Engine for Large-Scale Simulation and Big Data Analysis</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP3</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  2:05PM</starts_at>
    <ends_at>Feb 13 2020  2:50PM</ends_at>
    <EventFilter>PP20|Invited Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67766</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>67766-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T14:05:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>IP4 Modeling of Heterogeneous Computing Systems and Their Usages </name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP4</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  2:05PM</starts_at>
    <ends_at>Feb 14 2020  2:50PM</ends_at>
    <EventFilter>PP20|Invited Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67802</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>67802-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T14:05:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>IP5 Methods and Models for Reducing Communication</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP5</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  8:30AM</starts_at>
    <ends_at>Feb 15 2020  9:15AM</ends_at>
    <EventFilter>PP20|Invited Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67804</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>67804-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T08:30:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>IP6 Cognitive Discovery: Pushing the Frontier of Technical R&amp;D with AI</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP6</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  9:25AM</starts_at>
    <ends_at>Feb 15 2020 10:10AM</ends_at>
    <EventFilter>PP20|Invited Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67800</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>67800-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T09:25:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Lunch Break</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 12:35PM</starts_at>
    <ends_at>Feb 13 2020  2:05PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Attendees on their own</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68175-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T12:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Large Scale Inversion of CSEM Data in the Time Domain</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS77</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  2:10PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>1900</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Stefano Zampini</EventSpeakers>
    <EventSpeakerUniqueID>768190</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102071</EventHandoutURL>
    <blurb>Controlled-source electromagnetic (CSEM) time-domain data acquired by seabed receivers provide an imaging modality that can be used, either alone or jointly with seismic data, for resolving resistive bodies in increasingly complex geological settings and improving identification of hydrocarbon reserves. There are however a number of computational challenges that have to be overcome in order to develop parallel scalable inversion algorithms for CSEM data, particularly in the time domain. These include the need for efficient and robust models for the propagation of electromagnetic disturbances in a reservoir, governed by Maxwell equations in the low-frequency regime. Implicit time integration schemes are necessary for insuring stable simulations, and the required linear solves at every time step represent a key challenge for scalability. On the optimization side, there is a need for efficient gradient and Hessian-vector products needed for a Krylov solver, which require adjoint solves as well as incremental forward and incremental adjoint solves for their evaluation. Regularization of the optimization problem must allow non-smooth solutions and promote consistency between vertical and horizontal resistivities. This may be done through Total Variation (TV) vector versions (VTV). In this talk, we describe parallel algorithms for addressing these challenges and show illustrative results on three-dimensional problems.
</blurb>
    <EventParentName>MS77 Advanced HPC Trends for Oil and Gas Applications - Part II of II</EventParentName>
    <external_id>67853-102071</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Hiding I/O Latency in Large-Scale Scientific Computation Through Buffering and Prefetching on Advanced Storage Technologies</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS77</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:40PM</starts_at>
    <ends_at>Feb 15 2020  3:00PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>1900</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tariq   Alturkestani </EventSpeakers>
    <EventSpeakerUniqueID>790294</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102072</EventHandoutURL>
    <blurb>Simulation systems often produce a massive amount of data that cannot fit a single compute node, and they also require to read back these data for computation. As a result, I/O data movement can be a bottleneck in large-scale simulations. Advances in memory architecture have made it feasible and affordable to include multiple heterogeneous storage media in a single compute node. A typical workstation nowadays contains several Hard Disk Drives (HDDs) and Solid State Drives (SSDs) while an advanced one might contain a high-throughput Non-Volatile Memory (NVMe). However, while adding additional and faster storage media increases I/O bandwidth, it pressures the Central Processing Unit (CPU) as it becomes responsible for managing and moving data between these layers of storage. Simulation systems are thus vulnerable to being blocked by I/O operations. The system, Multilayer Layer Buffer System (MLBS), proposed in this talk, demonstrates a general and versatile method for overlapping I/O with computation that helps to ameliorate the strain on the processors through asynchronous access, while minimizing the impact on the computational kernel. 


</blurb>
    <EventParentName>MS77 Advanced HPC Trends for Oil and Gas Applications - Part II of II</EventParentName>
    <external_id>67853-102072</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>New Horizons for Debugging Long-Running Parallel Programs: DMTCP and SimGrid</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS78</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  2:15PM</starts_at>
    <ends_at>Feb 15 2020  2:35PM</ends_at>
    <EventFilter>PP20|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Gene Cooperman</EventSpeakers>
    <EventSpeakerUniqueID>790016</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101736</EventHandoutURL>
    <blurb>A confluence of recent advances in two independent domains has
serendipitously opened up new horizons for debugging long-running
programs.  The two independent domains are:  (a) checkpoint-restart; and
(b) process simulation through stateless model checking.  In each case,
the recent advances concern extensions to better support parallel and
distributed computation (e.g., MPI).  The speaker's own team supports a
long-running project, DMTCP (Distributed MultiThreaded CheckPointing),
for checkpoint-restart.  A new approach, called split processes,
allows DMTCP to now checkpoint under one implementation of MPI, and
then restart under another implementation of MPI.  In particular, the
second implementation of MPI can in fact be the smpi/SimGrid simulator,
which targets the production code of an MPI application.  Thus, a new
collaboration was born, which now allows simulation and debugging of a
long-running parallel program.  SimGrid, in combination with DMTCP, can
now simulate a restarted MPI application.  That restarted application
is based on a checkpoint that occurred before a crash or failed assert.
SimGrid can then produce an execution trace showing a deterministic
schedule from the time of the checkpoint to the crash or failed assert.
This new collaboration originated soon after the recent extension of
SimGrid to support distributed simulations of MPI applications.  But then
a simpler approach was found, and therein lies an interesting story.</blurb>
    <EventParentName>MS78 The Many Faces of Simulation for HPC - Part II of II</EventParentName>
    <external_id>67787-101736</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Combinatorial Optimization on Quantum Computers  </name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MT1</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  2:40PM</ends_at>
    <EventFilter>PP20|Minitutorial</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605 </location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ruslan Shaydulin</EventSpeakers>
    <EventSpeakerUniqueID>788362</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101808</EventHandoutURL>
    <blurb>Quantum computing has the potential to provide speedups over classical state-of-the-art for some combinatorial optimization problems. Recent advances in both hardware and algorithm development have made it possible to solve small problems on modern quantum computers. Combinatorial optimization problems (especially NP-hard problems) are of particular interest, since for many of these problems best classical algorithms can not provide solutions of sufficient quality in reasonable time. One such problem is MaxCut on graphs. In this minitutorial, we will introduce the MaxCut problem and explain how it can be solved on IBM quantum computers available on the cloud today using the Qiskit framework. Our presentation will assume little to no prior knowledge of quantum computation. Moreover, we will provide examples of how more complicated problems can be solved using the QAOA (Quantum Approximate Optimization Algorithm). The tutorial will be interactive and will use Jupyter notebooks to explain step by step how to formulate and solve optimization problems on quantum computers.	
</blurb>
    <EventParentName>MT1 Combinatorial Optimization on Quantum Computers</EventParentName>
    <external_id>67813-101808</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Accelerating Alternating Least Squares for Tensor Decomposition by Pairwise Perturbation</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Linjian Ma</EventSpeakers>
    <EventSpeakerUniqueID>790109</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=101989</EventHandoutURL>
    <blurb>The alternating least squares algorithm for CP and Tucker decomposition is dominated in cost by the tensor contractions necessary to set up the quadratic optimization subproblems. We introduce a novel family of algorithms that uses perturbative corrections to the subproblems rather than recomputing the tensor contractions. This approximation is accurate when the factor matrices are changing little across iterations, which occurs when alternating least squares approaches convergence. We provide a theoretical analysis to bound the approximation error. Our numerical experiments demonstrate that the proposed pairwise perturbation algorithms are easy to control and converge to minima that are as good as alternating least squares. We tested our algorithm on 1, 16 and 256 Intel KNL nodes of the Stampede2 supercomputer. The performance of one pairwise perturbation approximation step is 7.8-10.5X faster than one state of the art ALS step, and the overall performance of the new algorithms shows improvements of 1.3-2.8X with respect to alternating least squares approaches for various model tensor problems and real datasets.
</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-101989</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>GPU-Accelerated Barycentric Cluster-Particle Treecodes</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Leighton Wilson</EventSpeakers>
    <EventSpeakerUniqueID>782875</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102001</EventHandoutURL>
    <blurb>We present GPU-accelerated implementations of cluster-particle versions of the kernel independent Barycentric Lagrange Treecode (BLTC) and the weakly kernel dependent Barycentric Hermite Treecode (BHTC). In the cluster-particle form, hierarchical octtrees are built on the target sites instead of the source particles. Particle-cluster treecodes are typically superior to cluster-particle treecodes, except when the number of targets is greater than the number of sources. The simple structure of the cluster-particle approximations in these treecodes allow for efficient GPU parallelization. We implement the treecodes with OpenACC and OpenMP and test on NVIDIA GPUs. We demonstrate the treecode on several test cases consisting of $10^5-10^7$ randomly distributed particles with randomly distributed charges, interacting via the Coulomb and Yukawa kernels. We show strong GPU speedup, both for a single GPU versus a single CPU and for a complete GPU node versus a complete CPU node. 


</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-102001</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Adapting a Multibody System Simulator to Auto-Tuning Linear Algebra Routines</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jesús Cámara</EventSpeakers>
    <EventSpeakerUniqueID>790149</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102025</EventHandoutURL>
    <blurb>Multibody systems (MBS) consist of a set of bodies interconnected through mechanical joints which allow combined movements among them. Computational kinematics studies the movement of MBS from different approaches. One recently developed modular approach divides a MBS into a set of modules whose kinematics can be solved in a hierarchical order to analyse the complete MBS. Each module is solved efficiently using a block of linear algebra routines which organize their computation in tasks which can be carried out sequentially or simultaneously in different ways to reduce the simulation time in contemporary standard computational nodes (multicore CPU+multiGPU). This particular modular approach, implemented in a multibody system simulator (MBSS) can be applied to other problems which admit a hierarchical solution of modules defined by specific blocks of linear algebra routines, although it needs to be adapted to determine the appropriate block size in algorithms by blocks and the assignation of tasks to the computing units. This poster presents the structure of the MBSS and includes possible modifications to adapt it to help in the auto-tuning of linear algebra routines. Examples and results with Strassen's matrix multiplication and an LU factorization by blocks are shown.</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-102025</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Parallel Framework for Nonlinear Optimization</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mohan Krishnamoorthy</EventSpeakers>
    <EventSpeakerUniqueID>790001</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102029</EventHandoutURL>
    <blurb>Optimizations performed in today’s scientific applications have to contend with finding the global optimum for highly nonlinear and nonconvex functions. Even when the functions are differentiable, the optimization techniques developed so far either find a local optimum, or do not scale well in terms of dimension for the problem of finding the global optimum. This problem is further compounded when closed form expressions for these functions are unavailable, thereby needing expensive simulations to find their value at the current iterate. In this poster, we will present a preliminary framework to bridge this gap. We show that by extending the existing gradient-based and gradient-free approaches to make use of HPC infrastructure, it is possible to efficiently find the global optimum for these highly nonlinear and nonconvex functions. Additionally, we will show how we use this framework to solve parameter inversion problems in high-energy physics where optimization needs to be performed over high-dimensional nonlinear and nonconvex functions. We hope that this effort will broaden the understanding of how to make use of state-of-the-art HPC infrastructure to find the global optimum in ever increasing complexity of optimization problems and will encourage further research into developing such techniques.</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-102029</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Parallel Implementation of Non-Linear Least Squares Method for CP Decomposition for Tensors</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Navjot Singh</EventSpeakers>
    <EventSpeakerUniqueID>790176</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102034</EventHandoutURL>
    <blurb>The Canonical Polyadic decomposition (CPD) is used to approximate high dimensional data in various fields including Quantum Chemistry and Machine Learning. Alternating Least Squares (ALS) algorithm is usually regarded as the workhorse algorithm for CPD, however, ALS may exhibit slow or no convergence when the CP rank is greater than the dimensions of the Tensor or the factor matrices of CPD have high collinearity. For the above-mentioned scenarios, the Non-linear least squares (NLS) methods are considered superior to ALS but are not scalable for large tensors.

Damped Gauss Newton algorithm is a well-known method to solve the NLS problem iteratively, where in each iteration, a system of equations of the approximate Hessian and the gradient of the residual function is solved.

We explore parallelism in this method for CPD by realizing the approximate Hessian as tensor operators. This formulation is useful when the system of equations is solved by using Conjugate Gradient algorithm which would require tensor contractions in each iteration, allowing a parallel tensor contraction library to exploit parallelism and enabling NLS to outperform ALS in terms of time and accuracy for computing CPD for large high CP rank tensors.

	</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-102034</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Semi-Structured Hybrid Multigrid on Octree Meshes</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yu-Hsuan Shih</EventSpeakers>
    <EventSpeakerUniqueID>790036</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102044</EventHandoutURL>
    <blurb>We present a multigrid method that combines the flexibility of unstructured octree meshes with the efficiency of structured grid computations. While octrees allow for flexible adaptive mesh refinement, matrix-free finite element computations on non-conforming adaptive meshes require a large number of conditional statements and indirect memory access. This is due to the need to gather contributions from neighboring element at coarse-fine interfaces to shared unknowns. We thus combine non-conforming adaptivity with large regular mesh patches, which reuse data structures generated for high-order elements. We perform several levels of geometric multigrid on these meshes, and use algebraic multigrid as coarse grid solver. We present convergence and scalability results for Poisson's equation and linear elasticity with strongly varying coefficients.</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-102044</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Multicolor Block Gauss-Seidel Using Kokkos</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Brian Kelley</EventSpeakers>
    <EventSpeakerUniqueID>783200</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=102067</EventHandoutURL>
    <blurb>The classical Gauss-Seidel method is a very common preconditioner, but it is inherently sequential. Gauss-Seidel may be adapted into a distributed algorithm by coloring each processor's subdomain so that all processors sharing a color can update the global residual in parallel. This can be further improved by partitioning each local subdomain before coloring [Saad 99]. The Gauss-Seidel method may also be parallelized on a shared memory system by coloring individual rows. This approach achieves high performance on GPUs and multicore CPUs but converges more slowly than the classical method [Deveci 16]. We present a shared-memory version of multicolor block Gauss-Seidel using Kokkos that allows the user to manage the tradeoff between parallelism and convergence by tuning the block size.</blurb>
    <EventParentName>PP1 Poster Session</EventParentName>
    <external_id>68179-102067</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>A Scalable Block Preconditioner for High-Order Hybridized Discontinuous Galerkin Methods Applied to Incompressible Resistive MHD</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sriramkrishnan Muralikrishnan</EventSpeakers>
    <EventSpeakerUniqueID>772169</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104257</EventHandoutURL>
    <blurb>We propose a scalable block preconditioning strategy for linear systems arising from high-order hybridized discontinuous Galerkin discretization of incompressible visco-resistive magnetohydrodynamics (MHD) equations. Incompressible resistive MHD presents several challenges in terms of nonlinearity, coupled fluid and magnetic physics, incompressibility constraints in both velocity and magnetic fields to name a few. The preconditioner uses a least squares commutator approximation for the inverse of the Schur complement and algebraic multigrid with GMRES smoother or the multilevel preconditioner for the approximate inverse of the nodal block. For several 2D and 3D transient examples from MHD, including, but not limited to the island coalescence problem at high Lundquist numbers the preconditioner is robust. We show strong and weak scalability of the block preconditioner up to 8192 cores in the Stampede2 supercomputer.  

	</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104257</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Adaptive Domain Decomposition Method for Saddle Point Problem in Matrix Form </name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Frederic Nataf</EventSpeakers>
    <EventSpeakerUniqueID>718654</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104262</EventHandoutURL>
    <blurb>Convergence of domain decomposition methods rely heavily on the efficiency of the coarse space used in the second level. The GenEO coarse [Spillane N., Dolean V., Hauret P., Nataf F., Pechstein C. and Scheichl R., Abstract robust coarse spaces for systems of PDEs via generalized eigenproblems in the overlaps, Numer. Math., 2014], [Dolean, V. and Jolivet, P. and Nataf, F., An Introduction to Domain Decomposition Methods: algorithms, theory and parallel implementation, SIAM, 2015] space has been shown to lead to a robust two-level Schwarz preconditioner which scales well over thousands of cores. The robustness is due to its good approximation properties for problems with highly heterogeneous parameters. It is available in the finite element package FreeFem++ [Hecht F., New development in FreeFem++, J. Numer. Math., 2012] and as a standalone library in HPDDM [Jolivet, P. and Nataf, F., HPDDM: High-Performance Unified framework for Domain Decomposition methods, MPI-C++ library, https://github.com/hpddm/hpddm, 2014] as well as a PETSc preconditioner. 

We introduce here its extension for solving saddle point problems defined as a block two by two matrix. The algorithm does not require any knowledge of the constrained space. We assume that all sub matrices are sparse and that the diagonal blocks are the sum of positive semi definite matrices. This enables the design of adaptive coarse space for DD methods in the spirit of the GenEO method. 
</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104262</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Solving Hyperbolic PDE Systems with ExaHyPE</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Michael Bader</EventSpeakers>
    <EventSpeakerUniqueID>730729</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104651</EventHandoutURL>
    <blurb>We present latest features and results for ExaHyPE, an engine to solve hyperbolic PDE systems. ExaHyPE employs high-order discontinuous Galerkin with ADER time stepping and a-posteriori Finite-Volume limiting. It builds on the Peano framework, which features tree-structured Cartesian meshes and shared- and distributed-memory parallelism using MPI and Intel TBB. 

ExaHyPE provides role-oriented code generation utilities that offer tailored views for application, algorithm and optimisation experts. Application experts can quickly realise solver via providing only PDE-specific implementation, such as for conservative fluxes, non-conservative products, source terms, etc.

We present several use cases for the engine, partiocularly focusing on computational seismology. Here, we show results for a curvilinear model for seismic wave propagation and for a diffused interface model - both able to treat complex topographies with almost no effort on meshing.  
	</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104651</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Using Quantized Integer in $LU$ Factorization with Partial Pivoting</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yaohung Tsai</EventSpeakers>
    <EventSpeakerUniqueID>762930</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104703</EventHandoutURL>
    <blurb>Quantization is a common technique to speed the deep learning inference. It is using integers with a shared scalar to represent a set of equally spaced numbers. The quantized integer method has shown great success in compressing the deep learning models, reducing the computation cost without losing too much accuracy. New application specific hardware and specialized CPU extension instructions like \texttt{Intel AVX-512 VNNI} are providing capabilities for us to do integer \texttt{MADD} (multiply and add) efficiently. In this poster, we would like to show our preliminary results of using quantization integers for $LU$ factorization with partial pivoting. Using \texttt{Int32}, the backward error can outperform single precision. However, quantized integer has the similar issue of limited range as \texttt{FP16} that it would not work directly for large matrices because of big numbers would occur in factored $U$. We will show some possible solutions to it and how we would like to apply this quantized integer technique to other numerical linear algebra applications.

</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104703</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Algorithm-Specific Checkpointing vs. Exact State Reconstruction for the Preconditioned Conjugate Gradient Method</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christina Pacher</EventSpeakers>
    <EventSpeakerUniqueID>790169</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=104739</EventHandoutURL>
    <blurb>As current and future computing clusters grow in scale, the likelihood of hardware failures increases. This has motivated the development of a number of strategies to enable HPC applications to recover from node failures.

We experimentally evaluate different strategies for protecting the Preconditioned Conjugate Gradient (PCG) solver against node failures. The exact state reconstruction (ESR) approach exploits inherent redundancies of the sparse matrix-vector product to store copies of the local search direction on neighbouring nodes (Chen 2011, Pachajoa et al. 2019). After a node failure, this information can be used to reconstruct the lost parts of the solver’s state. We compare this approach to two algorithm-specific checkpointing strategies for PCG, a disk-storage-based approach and an in-memory buddy-checkpointing approach.

Our experiments investigate the performance of these schemes both for single and multiple overlapping node failures. ESR and in-memory checkpointing are both shown to be very efficient, with the superiority of one or the other mostly depending on the given scenario, and they both outperform the disk-based checkpointing approach by a wide margin.</blurb>
    <EventParentName>PP2 Poster Session</EventParentName>
    <external_id>68722-104739</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>SIAG/Supercomputing Career Prize: Ghosts of Parallel Computing: Past, Present, and Future</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>SP3</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  9:55AM</starts_at>
    <ends_at>Feb 14 2020 10:20AM</ends_at>
    <EventFilter>PP20|Prize Speaker</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Steve Plimpton</EventSpeakers>
    <EventSpeakerUniqueID>727177</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=103315</EventHandoutURL>
    <blurb>Receiving a career prize means you're old.  So in the first part of my talk I'll highlight a couple themes from the past 30 years of supercomputing which have taken us from gigaflops to the threshold of exaflops.  In the latter part I'll speculate on what comes next.

Spoiler alert: it seems unlikely the youngest members of the audience will see another 9 orders-of-magnitude speed-up within their career horizons.  What does that mean for SIAM members interested in supercomputing and the topics we research?  My perspective here will be application-centric (really, what else matters for science), so there will hopefully be ideas to both agree and disagree with.
</blurb>
    <EventParentName>SP3 SIAG/Supercomputing Career Prize: Ghosts of Parallel Computing: Past, Present, and Future</EventParentName>
    <external_id>68267-103315</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T09:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Coffee Break</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  2:40PM</starts_at>
    <ends_at>Feb 12 2020  3:10PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Gallery - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68165-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T14:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Coffee Break</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  2:50PM</starts_at>
    <ends_at>Feb 13 2020  3:20PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Gallery - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68176-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T14:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Coffee Break</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:25AM</starts_at>
    <ends_at>Feb 13 2020 10:55AM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Gallery - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68174-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:25:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Coffee Break</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  2:50PM</starts_at>
    <ends_at>Feb 14 2020  3:20PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Gallery - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68186-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T14:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Coffee Break</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:25AM</starts_at>
    <ends_at>Feb 14 2020 10:55AM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Gallery - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68184-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:25:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Coffee Break</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:10AM</starts_at>
    <ends_at>Feb 15 2020 10:40AM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Gallery - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68188-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>CP1 UQ and Stochastic Processes</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP1</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  2:40PM</ends_at>
    <EventFilter>PP20|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68191</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68191-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>CP10 Proceedings Papers - Part III of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP10</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 12:35PM</ends_at>
    <EventFilter>PP20|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68615</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68615-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>CP11 Applications - Part III of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP11</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  5:00PM</ends_at>
    <EventFilter>PP20|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68198</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68198-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>CP12 Communication Performance</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP12</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 12:20PM</ends_at>
    <EventFilter>PP20|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68199</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68199-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>CP13 Accelerating Software with GPUs</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP13</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  3:30PM</ends_at>
    <EventFilter>PP20|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68200</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68200-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>CP14 HPC for Data Science and Large Graphs</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP14</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  3:30PM</ends_at>
    <EventFilter>PP20|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68201</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68201-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>CP15 Highly Parallel Algorithms</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP15</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  3:30PM</ends_at>
    <EventFilter>PP20|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68202</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68202-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>CP2 Efficient Methods for PDEs and IDEs</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP2</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  2:40PM</ends_at>
    <EventFilter>PP20|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68192</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68192-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>CP3 Application - Part I of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP3</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  4:50PM</ends_at>
    <EventFilter>PP20|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68193</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68193-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>CP4 Proceedings Papers - Part I of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP4</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 12:35PM</ends_at>
    <EventFilter>PP20|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68613</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68613-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>CP5 Application - Part II of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP5</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  5:00PM</ends_at>
    <EventFilter>PP20|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68194</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68194-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>CP6 Multigrid and Preconditioning</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP6</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  5:00PM</ends_at>
    <EventFilter>PP20|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68195</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68195-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS11 Formal (Mathematical) Methods Enabling Applications of Quantum Computers</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS11</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  4:50PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67858</EventHandoutURL>
    <blurb>
What are quantum computers (QCs) today and what will they be in 5, 10, or 15 years? In what mathematical models should subject-matter experts formulate their problems so applications will benefit from QCs in a sustainable way?  How will subject-matter experts develop applications for QCs? What are to be considered best practices in developing and programming quantum computing architectures? What opportunities for quasi-automatic transformation by new compiler-like tools can exploit the power of well-matched mathematical models?  As the first generation of commercial QCs matures, the answers to such questions have become fundamentally important for the successful evolution of QCs to widespread industrial use. Whereas the development of previous-generation QCs relied heavily on hardware-focused heuristics, with this workshop, we aim to bring together leading experts working in areas of formal mathematical methods to discuss how these methods can address the challenges of developing next-generation and future QCs and developing QC applications that enable broad use.

</blurb>
    <EventParentName></EventParentName>
    <external_id>67858-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS12 Experiences in Developing GPU Support for Department of Energy Math Libraries - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS12</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  4:50PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67791</EventHandoutURL>
    <blurb>The FASTMath (Frameworks, Algorithms and Scalable Technologies for Mathematics) Institute is a R\&amp;D project funded by the SciDAC Program at the U.S. Department of Energy (DOE). The goal of FASTMath is to develop and deploy scalable mathematical algorithms and software tools for reliable simulation of complex physical phenomena and collaborating with DOE domain scientists to ensure the usefulness and applicability of the work in the project. The focus of FASTMath is strongly driven by the requirements of DOE application scientists who require fast, accurate, and robust forward simulation along with the ability to efficiently perform ensembles of simulations in optimization or uncertainty quantification studies. This minisymposium will present work by FASTMath participants focused on developing GPU support for their numerical mathematics libraries.  Presentations will include discussion of different strategies as well as performance results.</blurb>
    <EventParentName></EventParentName>
    <external_id>67791-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS21 Advances in Algorithms Exploiting Low Precision Floating-Point Arithmetic - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS21</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67713</EventHandoutURL>
    <blurb>In the June 2019 Top 500 list, 133 systems are
accelerator-based, more than half of which support half precision
floating-point arithmetic.  Since low precision floating-point formats are
increasingly being supported by hardware vendors, developing algorithms and
software that can exploit these formats is of increasing importance. This
minisymposium will give a broad overview of recent contributions in
exploiting low precision arithmetic in scientific computing, and details of
applications in which it can successfully be used.</blurb>
    <EventParentName></EventParentName>
    <external_id>67713-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS13 Physical-Based Modeling and Machine Learning for Biological and Environmental Sciences - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS13</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  4:50PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67752</EventHandoutURL>
    <blurb>Scientific computing is critical in understanding the biological, biogeochemical, and physical processes that span from molecular and genomics-controlled scales in biological systems, to the regional and global scales in climate and the earth system. The integration of physical models and machine learning play a significant role in both bio-science discovery and neural network based algorithms, where high-performance computing (HPC) is heavily used. Similarly, recent data science advances hold significant opportunity to transform our ability to rapidly use diverse datasets and physics-based models for predicting how earth systems respond to perturbations. On the other hand, the improved visualization and predictions in various domains of the biological and environmental systems translate to innovations of technologies and methodologies that may contribute to artificial intelligence. Those computationally intensive models usually rely on parallel processing and are impossible to implement without HPC clusters. In this mini-symposium, we gather researchers from various fields in biological and environmental sciences to talk about the frontiers of scientific computing and machine learning in their domain work, which covers bioinformatics, hemodynamics and neuroscience in biological sciences, and atmosphere and hydrology research in environmental science. Relevant work that leverages PDE-constrained optimization, generative models, and deep learning using HPC will be discussed.</blurb>
    <EventParentName></EventParentName>
    <external_id>67752-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS14 Advanced Visualisation, Analysis, and Parallelisation Concepts for Multi-Scale CFD Simulations in Science and Engineering</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS14</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 12:20PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>701</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67759</EventHandoutURL>
    <blurb>Due to recent advances in supercomputing, more and more scientific questions - especially from the so-called emerging sciences such as medicine, sociology, biology, virology, chemistry, climate or geo-sciences - can be answered today using high-performance computing (HPC). Such questions could cover the structural analysis of buildings and constructions in a global context (e.g. earth quakes), the prediction of floods and flooding damages due to heavy rainfall, the thread of tsunamis on coastal regions, the risk analysis of pollutant diffusion in populated regions, the simulation of evacuation scenarios on the facility, urban quarter, and city scale, or the optimisation of traffic flow within entire cities during rush hour—just to name a few. On the other side, "high-performance computing must now assume a broader meaning, encompassing not only flops, but also the ability, for example, to efficiently manipulate vast and rapidly increasing quantities of both numerical and non-numerical data" (Kalil, Miller: Advancing U.S. Leadership in High-Performance Computing. The White House, 2015.). In this minisymposium, different aspects of multi-scale, multi-level, multi-physics applications from science and engineering should be addressed, dealing with topics – but not limited to – such as parallelisation strategies, advanced numerical algorithms, coupling interfaces, data orchestration, interaction concepts, (big) data exploration, or visual data analytics.</blurb>
    <EventParentName></EventParentName>
    <external_id>67759-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS15 Nonlinear Preconditioning</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS15</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  4:50PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67821</EventHandoutURL>
    <blurb>Traditionally, the nonlinear systems arising from the discretization of nonlinear partial differential equations are solved by variants of Newton’s method. Often, a Newton-Krylov approach in combination with suitable preconditioners (multigrid, domain decomposition) is used. If necessary, globalization techniques, e.g., trust region, line search, load stepping, etc. are applied additionally.

Nonlinear preconditioning (or nonlinear elimination) is an alternative approach to improve the robustness and convergence properties of nonlinear solvers. But these methods also have a great potential to increase parallel scalability and to decrease time to solution. Here, one important property is localization of work and to decrease reduction of communication, which can lead to scalability up to more than a million parallel processes. Different approaches, including nonlinear Schwarz methods (ASPIN) and nonlinear FETI-DP/BDDC methods, are considered.</blurb>
    <EventParentName></EventParentName>
    <external_id>67821-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS16 High Performance Krylov Subspace Methods: Theory, Implementation, and Application - Part II of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS16</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  4:50PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67749</EventHandoutURL>
    <blurb>Krylov subspace methods are widely used for solving very large (sparse) linear systems and eigenvalue problems. However, when these algorithms are implemented on parallel architectures in their original forms, communication (rather than floating point operations) becomes the dominant cost. To address this, many mathematically equivalent implementations, designed explicitly to reduce the time spent on communication, have been introduced. However on some problems these new formulations can behave (very) differently in finite precision than the original implementations. This makes the design of numerically stable variants for parallel architectures challenging.

In this minisymposium we hear from researchers about the design and analysis of high performance Krylov subspace methods, practical concerns relating to implementing these methods on modern parallel architectures, and applications relevant today.</blurb>
    <EventParentName></EventParentName>
    <external_id>67749-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS17 HPC Simulation of the Hydrological Cycle</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS17</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  4:50PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67856</EventHandoutURL>
    <blurb>Large-scale computer simulations are increasingly important in
addressing research and operational water resource issues related to
the hydrological cycle. These issues include improving our
understanding of complex interactions affecting streamflow,
quantifying the effects of climate and landcover change, analyzing the
frequency of extreme events, predicting flood impacts, and designing
future infrastructure. Such simulations face significant challenges
such as observational data scarcity and uncertainty, surface and
sub-surface heterogeneity, scale disparity, and complex topography. In
order to address these challenges, increasingly sophisticated
numerical models are being developed and, along with them,
increasingly sophisticated and computationally intense numerical
algorithms and software. The advantages of such models include
increased predictive capabilities, but these advantages can only be
transformed into practical policies or disaster mitigation strategies
through the use of scalable high-performance computing. This
minisymposium offers four presentations on four different software
packages for hydrological simulation, illustrating the successes and
challenges for HPC in this increasingly important area.
up.</blurb>
    <EventParentName></EventParentName>
    <external_id>67856-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS18 Exploiting Task Parallelism in Exascale Computing Era</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS18</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  4:50PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67850</EventHandoutURL>
    <blurb>As we approach the age of exascale computing, developers of large scale scientific applications will be facing new challenges in terms of performance and portability. In this regard, task parallel computation paradigm presents a promising way forward as it allows programmers to express a high degree of parallelism, utilize advanced scheduling techniques based on the data dependency information available, and obtain architectural portability relatively easily through runtime support. This minisymposium will feature talks on recent developments in task-parallel runtimes, middlewares and applications to explore the advantages of the task-parallel computation paradigm, as well as to identify the challenges ahead for a successful transition to the exascale era.</blurb>
    <EventParentName></EventParentName>
    <external_id>67850-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS19 Parallel Adaptive Multigrid - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS19</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  4:50PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67819</EventHandoutURL>
    <blurb>The multigrid method is well-known as an efficient and highly-scalable solver for many applications. Parallel adaptive mesh refinement nicely integrates with the geometric multigrid method and parallel algebraic multigrid methods are established as state-of-the-art parallel sparse matrix solvers. This session is dedicated to advances for the multigrid method with a focus on adaptive and parallel simulations. It offers a platform to present applications that allow for profitable employment of adaptive mesh refinement and implementations for high-performance computing.</blurb>
    <EventParentName></EventParentName>
    <external_id>67819-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS2 Meaningful Performance Indicators for Scientific Computing</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS2</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  2:40PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67722</EventHandoutURL>
    <blurb>It is essential be able to evaluate computational and data analysis systems during many stages of their life cycle, from design, to development, to deployment, and then life-cycle long system improvement. Therefore, the metrics and methods for consistent and meaningful performance evaluation is important for the scientific computing and data analysis. This symposium focuses on the performance evaluation for systems developed as the fundamental infrastructure of scientific research. A wide area related to the performance evaluation, such as benchmark design, initial and on-going performance metrics, comprehensive evaluation methodologies, productivity and reproducibility will be discussed.</blurb>
    <EventParentName></EventParentName>
    <external_id>67722-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS20 Frameworks/Libraries for High-Performance Tensor Computations - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS20</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  3:10PM</starts_at>
    <ends_at>Feb 12 2020  4:50PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67794</EventHandoutURL>
    <blurb>Tensors are higher dimensional analogs of matrices, and represent a key data abstraction for many applications in computational science and data science. While robust high-performance libraries for matrix computations are widely available for a broad range of hardware platforms, the same has not been true for tensor computations. However, there has been considerable recent activity in developing libraries/frameworks for efficient tensor computations on different target platforms. This minisymposium will bring together researchers from academia, industry, and research laboratories to present recent advances, for dense as well as sparse tensors.</blurb>
    <EventParentName></EventParentName>
    <external_id>67794-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS22 Parallel Processing for Particle Codes - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS22</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>4000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67814</EventHandoutURL>
    <blurb>A large number of industrial problems can be modelled using particle-based methods. The particle method based application software is one of the main development areas in exascale computing including DOE Exascale Computing Projects(ECP). The reason is that particle based methods provide extremely fine-grained parallelism and allow the exploitation of asynchronous parallelism. Efficient parallel particle method applications require the efficient implementation when computing with billions and trillions of particles. The main objective of this proposal is to getting together experts to specifically address issues on the following common topics for large scale application simulations, scalable distributed computing (domain decomposition, dynamic load balancing), optimised data mapping (structured and unstructured communication), efficient parallel I/O, nearest neighbour lists searching using tree algorithms(particular for particle refinement) and cell linked lists, particle-to-mesh, and mesh-to-particle interpolation, sparse linear solver for incompressible problems, particle based applications involving complex geometries, parallel-in-time for particle based methods, best practice using machine learning and future-proof programming models for various modern and future computing architectures.</blurb>
    <EventParentName></EventParentName>
    <external_id>67814-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS23 Advances and Challenges in Solvers on GPGPU-based High-Performance Computing Architectures - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS23</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67833</EventHandoutURL>
    <blurb>As the high-performance computing community pushes towards the exascale horizon, power and heat considerations have driven the increasing importance and prevalence of fine-grained parallelism in new computer architectures. This is particularly evident in the widespread adoption of general purpose graphics processing unit (GPGPU) accelerators at high-performance computing centers; reflecting this trend, the announced plans for the first generation of exascale-class systems to be fielded in the United States all rely on GPGPUs to provide the bulk of their computational power. This trend presents several challenges for scientific algorithms and software that were originally designed with scalar processor architectures in mind: large kernel launch and host-device transfer latencies, need for explicit application management of memory hierarchies, many different vectorization strategies and programming models to choose from, etc. This minisymposium will bring together researchers working on adapting and optimizing existing solvers for GPGPUs as well as developing novel approaches that map naturally onto emerging GPGPU computing resources.</blurb>
    <EventParentName></EventParentName>
    <external_id>67833-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS24 Parallel Matrix Factorization Algorithms - Part I of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS24</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67847</EventHandoutURL>
    <blurb>This minisymposium brings together research on parallel algorithms for matrix factorizations for sparse, dense, and structured methods. Communication-avoidance has motivated new algorithms for dense matrix factorizations that are faster and more parallelizable. These techniques are being adapted also by new algorithms for parallel sparse direct solvers and preconditioning via approximate sparse matrix factorization. Low rank and hierarchically low rank matrix representations provide a competing route for finding solutions to numerical PDEs, but face their own scalability challenges. For example, low-rank factorizations motivate the development of efficient parallel pivoted QR factorization. The minisymposium will encompass state-of-the-art in parallel algorithms for factorization of dense, sparse, and structured matrices, and other related topics. In particular, speakers will discuss communication-avoiding parallel algorithms for dense QR, sparse QR, and QR with randomized column pivoting. Further, talks will describe innovations in state-of-the-art parallel libraries for sparse direct solvers and their application as solvers for numerical PDEs and dynamic optimization.</blurb>
    <EventParentName></EventParentName>
    <external_id>67847-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS25 Progress and Challenges in Extreme Scale Computing and Big Data - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS25</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67709</EventHandoutURL>
    <blurb>Extreme scale computing efforts have resulted in numerous advances for multicore, manycore and accelerator based scalable systems. In addition, large-scale applications must increasingly deal with data management and analysis as a first-class concern. Therefore, new applications often have to manage distributed and parallel computing, and have to manage workflows of different tasks (computing, data analytics, machine learning, visualization,…). In this MS, we present some of the latest works in scalable algorithms, programming paradigms, and libraries for next generation computing platforms. Furthermore, we discuss efforts to better incorporate data science concerns as a principle component of our scientific workflows.</blurb>
    <EventParentName></EventParentName>
    <external_id>67709-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS26 High Performance Krylov Subspace Methods: Theory, Implementation, and Application - Part III of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS26</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67750</EventHandoutURL>
    <blurb>Krylov subspace methods are widely used for solving very large (sparse) linear systems and eigenvalue problems. However, when these algorithms are implemented on parallel architectures in their original forms, communication (rather than floating point operations) becomes the dominant cost. To address this, many mathematically equivalent implementations, designed explicitly to reduce the time spent on communication, have been introduced. However on some problems these new formulations can behave (very) differently in finite precision than the original implementations. This makes the design of numerically stable variants for parallel architectures challenging.

In this minisymposium we hear from researchers about the design and analysis of high performance Krylov subspace methods, practical concerns relating to implementing these methods on modern parallel architectures, and applications relevant today.</blurb>
    <EventParentName></EventParentName>
    <external_id>67750-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS28 High-Performance Numerics and Model Development for Geophysical Systems - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS28</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67822</EventHandoutURL>
    <blurb>There is a current explosion of interest in new numerical methods for the simulation of complex geophysical systems (atmosphere, ocean, land, ice) on next-generation supercomputers. To generate accurate predictions, models must be able to efficiently resolve multi-scale processes, surpass computational barriers for higher-resolution simulations, utilize scalable methods for emerging architectures, and provide actionable predictions driven by localized dynamics. To accomplish this, models require state-of-the-art numerical algorithms which can exploit current trends in supercomputing design. This minisymposium highlights some of the development challenges, with application areas relevant for large-scale atmospheric forecasting, ocean circulation, storm-surge modeling, and glaciology. Emphasis is placed on efficient solver algorithms, software frameworks, and application-driven model development.</blurb>
    <EventParentName></EventParentName>
    <external_id>67822-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS29 Recent Advances and Trends in Hybrid Quantum-Classical Algorithms - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS29</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67762</EventHandoutURL>
    <blurb>Noisy Intermediate-Scale Quantum (NISQ) devices that are becoming available on the clouds are limited to between tens and hundreds of qubits. Recent experimental demonstrations suggest that in the next few years we will witness qubits with noise levels sufficiently low to solve computationally hard problems. Combinatorial optimization problems are considered one of the main candidates for demonstrating quantum advantage, i.e. solving a given problem faster or finding a better solution that classical state-of-the-art solvers. However, a limited number of qubits (and, thus, a limited number of corresponding optimization variables) presents a major obstacle driving the design of the novel algorithms efficiently using NISQ devices.  Combining classical and quantum resources is one of the expedient answers that researchers suggest today to tackle real-life problems with existing quantum hardware. These hybrid algorithms attempt to take advantage of “the best of both worlds”, leveraging the power of quantum computation while using a classical machine to address the limitations of NISQ devices. In this mini-symposium, we will present state of the art methods and ideas related to hybrid quantum-classical algorithms, demonstrate computational results, focus on combining NISQ devices and HPC, and discuss problems and obstacles related to these approaches.</blurb>
    <EventParentName></EventParentName>
    <external_id>67762-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS3 Experiences in Developing GPU Support for Department of Energy Math Libraries - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS3</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  2:40PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67790</EventHandoutURL>
    <blurb>The FASTMath (Frameworks, Algorithms and Scalable Technologies for Mathematics) Institute is a R\&amp;D project funded by the SciDAC Program at the U.S. Department of Energy (DOE). The goal of FASTMath is to develop and deploy scalable mathematical algorithms and software tools for reliable simulation of complex physical phenomena and collaborating with DOE domain scientists to ensure the usefulness and applicability of the work in the project. The focus of FASTMath is strongly driven by the requirements of DOE application scientists who require fast, accurate, and robust forward simulation along with the ability to efficiently perform ensembles of simulations in optimization or uncertainty quantification studies. This minisymposium will present work by FASTMath participants focused on developing GPU support for their numerical mathematics libraries.  Presentations will include discussion of different strategies as well as performance results.</blurb>
    <EventParentName></EventParentName>
    <external_id>67790-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS30 Accelerating Data Sparse Applications on Massively Parallel Systems - Part I of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS30</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67825</EventHandoutURL>
    <blurb>With multiple monikers such as block low-rank, data sparse, or hierarchical
matrices, this minisymposium features presentations on exploiting structure of
matrix data for more efficient solvers in science and engineering.
This low-rank approximation technique exploits the data sparsity of the
application system matrix by compressing matrix entries. Often, the
off-diagonal submatrices are amenable to compression and may be regulated with
a user-defined accuracy ranges. During the actual solver phase, the underlying
linear algebra operations are carried out on the compressed data format rather
than the original full data. As a result, the arithmetic complexity of the new
solver ends up being reduced to much lower levels. This allows to tackle much
larger simulation problems that would otherwise be constrained by either their
storage size of computational cost. The wide variety of solvers and compression
techniques will be presented to showcase to broad applicability of the
data-sparse techniques in modern scientific applications.</blurb>
    <EventParentName></EventParentName>
    <external_id>67825-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS31 Frameworks/Libraries for High-Performance Tensor Computations - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS31</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020 10:55AM</starts_at>
    <ends_at>Feb 13 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67795</EventHandoutURL>
    <blurb>Tensors are higher dimensional analogs of matrices, and represent a key data abstraction for many applications in computational science and data science. While robust high-performance libraries for matrix computations are widely available for a broad range of hardware platforms, the same has not been true for tensor computations. However, there has been considerable recent activity in developing libraries/frameworks for efficient tensor computations on different target platforms. This minisymposium will bring together researchers from academia, industry, and research laboratories to present recent advances, for dense as well as sparse tensors.</blurb>
    <EventParentName></EventParentName>
    <external_id>67795-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS32 Transparency, Reproducibility, Sustainability, and Security: The Four Pillars of the Next Generation Scientific Software Stack</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS32</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>4000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67792</EventHandoutURL>
    <blurb>The software stacks used in computationally- and data-enabled research and discovery is increasingly dependent on transparency, reproducibility, sustainability, and security. Transparency exposes the salient computational aspects of the discovery workflow and permits reviewers and readers of published research findings to understand and assess the computational result. Reproducibility refers to the regeneration of computationally- and data-enabled findings on the same system or a different system. Sustainability goes to the ability to persistently reuse software. Open science needs assurance of computational systems that behave as intended, including data integrity, defending against assertions of data manipulation in an increasing polarized world, and support for work that is often extremely collaborative. To trust the scientific claims that result from these systems, we not only need to trust the scientific methodology but also trust the behavior of the underlying computational system itself, in other words the implementation in software and the software environment itself. Today, few guarantees are given regarding the computational infrastructure that supports scientific discovery and even fewer guarantees are given regarding security for openly shared data and codes that support computational reproducibility in the scientific realm. Researchers are increasingly sharing and reusing data and code, with very little guidance on appropriate security guarantees or standards</blurb>
    <EventParentName></EventParentName>
    <external_id>67792-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS33 Parallel Processing for Particle Codes - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS33</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>4000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67815</EventHandoutURL>
    <blurb>A large number of industrial problems can be modelled using particle-based methods. The particle method based application software is one of the main development areas in exascale computing including DOE Exascale Computing Projects(ECP). The reason is that particle based methods provide extremely fine-grained parallelism and allow the exploitation of asynchronous parallelism. Efficient parallel particle method applications require the efficient implementation when computing with billions and trillions of particles. The main objective of this proposal is to getting together experts to specifically address issues on the following common topics for large scale application simulations, scalable distributed computing (domain decomposition, dynamic load balancing), optimised data mapping (structured and unstructured communication), efficient parallel I/O, nearest neighbour lists searching using tree algorithms(particular for particle refinement) and cell linked lists, particle-to-mesh, and mesh-to-particle interpolation, sparse linear solver for incompressible problems, particle based applications involving complex geometries, parallel-in-time for particle based methods, best practice using machine learning and future-proof programming models for various modern and future computing architectures.</blurb>
    <EventParentName></EventParentName>
    <external_id>67815-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS34 Advances and Challenges in Solvers on GPGPU-Based High-Performance Computing Architectures - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS34</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67834</EventHandoutURL>
    <blurb>As the high-performance computing community pushes towards the exascale horizon, power and heat considerations have driven the increasing importance and prevalence of fine-grained parallelism in new computer architectures. This is particularly evident in the widespread adoption of general purpose graphics processing unit (GPGPU) accelerators at high-performance computing centers; reflecting this trend, the announced plans for the first generation of exascale-class systems to be fielded in the United States all rely on GPGPUs to provide the bulk of their computational power. This trend presents several challenges for scientific algorithms and software that were originally designed with scalar processor architectures in mind: large kernel launch and host-device transfer latencies, need for explicit application management of memory hierarchies, many different vectorization strategies and programming models to choose from, etc. This minisymposium will bring together researchers working on adapting and optimizing existing solvers for GPGPUs as well as developing novel approaches that map naturally onto emerging GPGPU computing resources.</blurb>
    <EventParentName></EventParentName>
    <external_id>67834-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS58 Parallel-in-Time Integration Methods - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS58</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67797</EventHandoutURL>
    <blurb>Driven by the rapid increase in core numbers in high-performance computing systems, parallel-in-time methods are a quickly growing field of study that promise parallelization beyond widely developed space-parallel algorithms. The mini symposium will feature presentations on recent developments with respect to algorithms, implementation and analysis and application of various types of algorithms offering concurrency along the time direction.</blurb>
    <EventParentName></EventParentName>
    <external_id>67797-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS35 Parallel Matrix Factorization Algorithms - Part II of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS35</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67848</EventHandoutURL>
    <blurb>This minisymposium brings together research on parallel algorithms for matrix factorizations for sparse, dense, and structured methods. Communication-avoidance has motivated new algorithms for dense matrix factorizations that are faster and more parallelizable. These techniques are being adapted also by new algorithms for parallel sparse direct solvers and preconditioning via approximate sparse matrix factorization. Low rank and hierarchically low rank matrix representations provide a competing route for finding solutions to numerical PDEs, but face their own scalability challenges. For example, low-rank factorizations motivate the development of efficient parallel pivoted QR factorization. The minisymposium will encompass state-of-the-art in parallel algorithms for factorization of dense, sparse, and structured matrices, and other related topics. In particular, speakers will discuss communication-avoiding parallel algorithms for dense QR, sparse QR, and QR with randomized column pivoting. Further, talks will describe innovations in state-of-the-art parallel libraries for sparse direct solvers and their application as solvers for numerical PDEs and dynamic optimization.</blurb>
    <EventParentName></EventParentName>
    <external_id>67848-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS36 Progress and Challenges in Extreme Scale Computing and Big Data - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS36</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67710</EventHandoutURL>
    <blurb>Extreme scale computing efforts have resulted in numerous advances for multicore, manycore and accelerator based scalable systems. In addition, large-scale applications must increasingly deal with data management and analysis as a first-class concern. Therefore, new applications often have to manage distributed and parallel computing, and have to manage workflows of different tasks (computing, data analytics, machine learning, visualization,…). In this MS, we present some of the latest works in scalable algorithms, programming paradigms, and libraries for next generation computing platforms. Furthermore, we discuss efforts to better incorporate data science concerns as a principle component of our scientific workflows.</blurb>
    <EventParentName></EventParentName>
    <external_id>67710-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS37 Challenges in Parallel Adaptive Mesh Refinement - Part II of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS37</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67771</EventHandoutURL>
    <blurb>Parallel adaptive mesh refinement (AMR) is a key technique when simulations are required to capture time-dependent and/or multiscale features. Frequent re-adaptation and repartitioning of the mesh during the simulation can impose significant overhead, particularly in largescale parallel environments. Further challenges arise due to the availability of accelerated or special-purpose hardware, and the trend toward hierarchical and hybrid compute architectures. Our minisymposium addresses algorithms, scalability, and software issues of parallel AMR.</blurb>
    <EventParentName></EventParentName>
    <external_id>67771-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS38 High-Performance Numerics and Model Development for Geophysical Systems - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS38</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67823</EventHandoutURL>
    <blurb>There is a current explosion of interest in new numerical methods for the simulation of complex geophysical systems (atmosphere, ocean, land, ice) on next-generation supercomputers. To generate accurate predictions, models must be able to efficiently resolve multi-scale processes, surpass computational barriers for higher-resolution simulations, utilize scalable methods for emerging architectures, and provide actionable predictions driven by localized dynamics. To accomplish this, models require state-of-the-art numerical algorithms which can exploit current trends in supercomputing design. This minisymposium highlights some of the development challenges, with application areas relevant for large-scale atmospheric forecasting, ocean circulation, storm-surge modeling, and glaciology. Emphasis is placed on efficient solver algorithms, software frameworks, and application-driven model development.</blurb>
    <EventParentName></EventParentName>
    <external_id>67823-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS39 Recent Advances and Trends in Hybrid Quantum-Classical Algorithms - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS39</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67763</EventHandoutURL>
    <blurb>Noisy Intermediate-Scale Quantum (NISQ) devices that are becoming available on the clouds are limited to between tens and hundreds of qubits. Recent experimental demonstrations suggest that in the next few years we will witness qubits with noise levels sufficiently low to solve computationally hard problems. Combinatorial optimization problems are considered one of the main candidates for demonstrating quantum advantage, i.e. solving a given problem faster or finding a better solution that classical state-of-the-art solvers. However, a limited number of qubits (and, thus, a limited number of corresponding optimization variables) presents a major obstacle driving the design of the novel algorithms efficiently using NISQ devices.  Combining classical and quantum resources is one of the expedient answers that researchers suggest today to tackle real-life problems with existing quantum hardware. These hybrid algorithms attempt to take advantage of “the best of both worlds”, leveraging the power of quantum computation while using a classical machine to address the limitations of NISQ devices. In this mini-symposium, we will present state of the art methods and ideas related to hybrid quantum-classical algorithms, demonstrate computational results, focus on combining NISQ devices and HPC, and discuss problems and obstacles related to these approaches.</blurb>
    <EventParentName></EventParentName>
    <external_id>67763-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS4 Physical-Based Modeling and Machine Learning for Biological and Environmental Sciences - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS4</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  2:40PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67751</EventHandoutURL>
    <blurb>Scientific computing is critical in understanding the biological, biogeochemical, and physical processes that span from molecular and genomics-controlled scales in biological systems, to the regional and global scales in climate and the earth system. The integration of physical models and machine learning play a significant role in both bio-science discovery and neural network based algorithms, where high-performance computing (HPC) is heavily used. Similarly, recent data science advances hold significant opportunity to transform our ability to rapidly use diverse datasets and physics-based models for predicting how earth systems respond to perturbations. On the other hand, the improved visualization and predictions in various domains of the biological and environmental systems translate to innovations of technologies and methodologies that may contribute to artificial intelligence. Those computationally intensive models usually rely on parallel processing and are impossible to implement without HPC clusters. In this minisymposium, we gather researchers from various fields in biological and environmental sciences to talk about the frontiers of scientific computing and machine learning in their domain work, which covers bioinformatics, hemodynamics and neuroscience in biological sciences, and atmosphere and hydrology research in environmental science. Relevant work that leverages PDE-constrained optimization, generative models, and deep learning using HPC will be discussed.</blurb>
    <EventParentName></EventParentName>
    <external_id>67751-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS40 Accelerating Data Sparse Applications on Massively Parallel Systems - Part II of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS40</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  3:20PM</starts_at>
    <ends_at>Feb 13 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67826</EventHandoutURL>
    <blurb>With multiple monikers such as block low-rank, data sparse, or hierarchical
matrices, this minisymposium features presentations on exploiting structure of
matrix data for more efficient solvers in science and engineering.
This low-rank approximation technique exploits the data sparsity of the
application system matrix by compressing matrix entries. Often, the
off-diagonal submatrices are amenable to compression and may be regulated with
a user-defined accuracy ranges. During the actual solver phase, the underlying
linear algebra operations are carried out on the compressed data format rather
than the original full data. As a result, the arithmetic complexity of the new
solver ends up being reduced to much lower levels. This allows to tackle much
larger simulation problems that would otherwise be constrained by either their
storage size of computational cost. The wide variety of solvers and compression
techniques will be presented to showcase to broad applicability of the
data-sparse techniques in modern scientific applications.</blurb>
    <EventParentName></EventParentName>
    <external_id>67826-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS41 Trilinos and Hardware Independent Computing (Kokkos)</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS41</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67852</EventHandoutURL>
    <blurb>TBA</blurb>
    <EventParentName></EventParentName>
    <external_id>67852-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS42 Improving Productivity and Sustainability for Parallel Computing Software - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS42</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67772</EventHandoutURL>
    <blurb>As we move toward exascale machines and beyond, we face a daunting task of providing software for emerging extreme-scale disruptive architectures that will allow applications to run with extreme performance, high scalability, and reliability. In addition, demand for crosscutting software that spans multiple domain-specific sciences, a clamor for greater reproducibility, and new opportunities for greatly improved simulation capabilities, especially through coupling of data and physics across scales, will result in software being extremely complex and challenging. While hardware remains a primary focus for next-generation exascale computing, the aforementioned challenges also demand large investments in scientific software development.

In this minisymposium, we aim to cover lost ground and address growing technical, practical, and social challenges in software productivity, quality, and sustainability, with a goal of helping develop better and long-living, cutting-edge parallel computing software that can fulfill its designated roles in emerging exascale ecosystems. Along with sharing of diverse experiences by experts in the parallel computing field, the hope is that this minisymposium will motivate the community to realign and refocus on developer productivity and software sustainability---which are both urgent and essential---to build better scientific software.</blurb>
    <EventParentName></EventParentName>
    <external_id>67772-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS43 Parallel Sparse and Conventional FFTs, Applications and Implementation - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS43</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67764</EventHandoutURL>
    <blurb>The fast Fourier Transform (FFT) is an algorithm used in a wide variety of applications, yet does not make optimal use of many current and emerging platforms such as many-core processors, GPUs, and distributed-memory systems. Hardware utilization performance on its own does not, however, imply optimal problem-solving. The purpose of this minisymposium is to enable an exchange of information between people working on FFT algorithms such as sparse and conventional FFTs, to those working on FFT implementations, in particular for parallel hardware.</blurb>
    <EventParentName></EventParentName>
    <external_id>67764-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS44 Parallel Matrix Factorization Algorithms - Part III of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS44</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67849</EventHandoutURL>
    <blurb>This minisymposium brings together research on parallel algorithms for matrix factorizations for sparse, dense, and structured methods. Communication-avoidance has motivated new algorithms for dense matrix factorizations that are faster and more parallelizable. These techniques are being adapted also by new algorithms for parallel sparse direct solvers and preconditioning via approximate sparse matrix factorization. Low rank and hierarchically low rank matrix representations provide a competing route for finding solutions to numerical PDEs, but face their own scalability challenges. For example, low-rank factorizations motivate the development of efficient parallel pivoted QR factorization. The minisymposium will encompass state-of-the-art in parallel algorithms for factorization of dense, sparse, and structured matrices, and other related topics. In particular, speakers will discuss communication-avoiding parallel algorithms for dense QR, sparse QR, and QR with randomized column pivoting. Further, talks will describe innovations in state-of-the-art parallel libraries for sparse direct solvers and their application as solvers for numerical PDEs and dynamic optimization.</blurb>
    <EventParentName></EventParentName>
    <external_id>67849-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS45 High Performance Computing in Scientific Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS45</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67767</EventHandoutURL>
    <blurb>Modern high-performance computing (HPC) is being widely used to massively parallelize scientific applications such as those in high-energy physics, climate, weather, materials science, and computational chemistry, to name a few. Some of the applied mathematics problems being tackled include optimizing objective functions having multiple local minima, optimizing simulation-based objective functions, and optimizing functions that are composed of information from multiple sources, including functions over surrogate models. Devising highly scalable, parallel, and efficient solutions to these problems requires an interdisciplinary approach drawing on expertise from domain sciences, data science, applied mathematics, and HPC. This minisymposium brings together pioneers solving problems in these areas. They will discuss the current state of the art, domain science impact and challenges, modeling and optimization approaches, scalable parallel software frameworks, and potential future research directions. They will describe software tools, algorithms, and techniques applied to problems in high-energy physics, but we believe that the lessons learned can be applied to other scientific domains as well.</blurb>
    <EventParentName></EventParentName>
    <external_id>67767-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS46 Challenges in Parallel Adaptive Mesh Refinement - Part III of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS46</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67857</EventHandoutURL>
    <blurb>Parallel adaptive mesh refinement (AMR) is a key technique when simulations are required to capture time-dependent and/or multiscale features. Frequent re-adaptation and repartitioning of the mesh during the simulation can impose significant overhead, particularly in largescale parallel environments. Further challenges arise due to the availability of accelerated or special-purpose hardware, and the trend toward hierarchical and hybrid compute architectures. Our minisymposium addresses algorithms, scalability, and software issues of parallel AMR.</blurb>
    <EventParentName></EventParentName>
    <external_id>67857-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS47 Parallel-in-Time Integration Methods - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS47</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>800</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67796</EventHandoutURL>
    <blurb>Driven by the rapid increase in core numbers in high-performance computing systems, parallel-in-time methods are a quickly growing field of study that promise parallelization beyond widely developed space-parallel algorithms. The mini symposium will feature presentations on recent developments with respect to algorithms, implementation and analysis and application of various types of algorithms offering concurrency along the time direction.</blurb>
    <EventParentName></EventParentName>
    <external_id>67796-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS48 Accelerating Data Sparse Applications on Massively Parallel Systems - Part III of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS48</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67827</EventHandoutURL>
    <blurb>With multiple monikers such as block low-rank, data sparse, or hierarchical
matrices, this minisymposium features presentations on exploiting structure of
matrix data for more efficient solvers in science and engineering.
This low-rank approximation technique exploits the data sparsity of the
application system matrix by compressing matrix entries. Often, the
off-diagonal submatrices are amenable to compression and may be regulated with
a user-defined accuracy ranges. During the actual solver phase, the underlying
linear algebra operations are carried out on the compressed data format rather
than the original full data. As a result, the arithmetic complexity of the new
solver ends up being reduced to much lower levels. This allows to tackle much
larger simulation problems that would otherwise be constrained by either their
storage size of computational cost. The wide variety of solvers and compression
techniques will be presented to showcase to broad applicability of the
data-sparse techniques in modern scientific applications.</blurb>
    <EventParentName></EventParentName>
    <external_id>67827-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS49 GPU Computing for Solving Large Scale Scientific Problems</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS49</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020 10:55AM</starts_at>
    <ends_at>Feb 14 2020 12:35PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>701</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67664</EventHandoutURL>
    <blurb>Graphic processing units (GPUs) are becoming an ever more important aspect of the high performance computing landscape. This is particularly true as the majority of compute performance for a number of pre-exascale systems is due to GPUs. For these, and many other systems, optimal performance can only be achieved if GPUs are used efficiently. 

Nevertheless, in many application domains significant challenges to effectively use GPUs remain. To mention just a few: GPUs require very fine grained parallelism (the result is that algorithms which are most effective on CPU based systems are not necessarily the best choice for GPUs), application developers have to deal with less available memory and smaller caches, communication between host and device and between multiple devices can be a bottleneck, and obtaining good performance on multiple architectures without code duplication (i.e. performance portability).

This minisymposium is dedicated to the discussion of how to overcome some of these difficulties. We aim to highlight successful strategies for incorporating GPU support into scientific codes and present novel research that will help to improve, both in terms of development time as well as in terms of performance, the development of the next generation of GPU enabled codes.</blurb>
    <EventParentName></EventParentName>
    <external_id>67664-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS5 Parallel Simulation of Circuits, Devices, and Electromagnetics Environments Effects</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS5</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  2:40PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67840</EventHandoutURL>
    <blurb>This minisymposium will present the current research, development, and performance of parallel simulation tools for electrical systems and electromagnetic environments effects.  These tools enable simulation of integrated circuits (Xyce), devices (Charon), and electromagnetics (EMPIRE/GEMMA) at unprecedented scales on advanced architectures.  This collection of talks will present the unique needs and challenges in achieving efficient and scalable parallel performance for each of these simulation tools.  This rich diversity permeates every layer of the simulation infrastructure from system assembly through the implementation of numerical methods for solving linear systems.  Current results will be presented for each of the simulation tools and future research directions will be identified for addressing the needs of scaling, robustness, and efficiency on next-generation computing platforms.</blurb>
    <EventParentName></EventParentName>
    <external_id>67840-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS50 Advances in Parallel Sparse Linear System Solver Stacks for Exascale - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS50</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67788</EventHandoutURL>
    <blurb>Our minisymposium presents advances in high-performance algorithms and GPU implementations in sparse linear system solver stacks including Trilinos-Belos and Muelu, Hypre-BoomerAMG, VT libParaNumAl high-order matrix-free libraries and fast direct methods based on low-rank approximation.   Although the focus is on low communication algorithms and GPU implementations, we also consider Exascale applications and mini-apps that can demonstrate significant performance gains by employing these frameworks.  For Krylov methods our focus is on recent advances in
communication-avoiding and low-synch algorithms.  Preconditioners, such as ILU will require fast and possibly iterative triangular solvers. Polynomial type preconditioners and AMG smoothers have also been advocated for GPU, along with sparse approximate inverse algorithms such as AINV. GPU acceleration is prompting new directions and a re-examination of the basic set-up and solution algorithms.  For example, semi-structured AMG facilitates the local tuning of smoothers to improve convergence and execution speed.  We bring together the architects of these important solver frameworks to explore algorithm choices, describe the latest performance results using these
approaches and discuss future plans for the recently announced Exascale
computers Aurora and Frontier.</blurb>
    <EventParentName></EventParentName>
    <external_id>67788-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS51 Novel Computational Algorithms for Future Computing Platforms - Part I of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS51</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67837</EventHandoutURL>
    <blurb>In the early 2000s, due to constraints on economical heat dissipation, clock speeds of single-core CPUs could no longer be increased, which marked the adoption of multi-core CPUs, together with a paradigm shift to algorithms specifically designed for multi-core architectures.
About 15 years into this current architectural cycle and on its way to exascale performance, the computing industry finds itself at the confluence of technical difficulties that cast doubt on its
ability to sustain this architectural model beyond the exascale capability. These difficulties are driving the hardware industry to develop application-specific chips and to look beyond silicon-
based chips (e.g., quantum computing, physical annealing, neuromorphics, etc.), with a continued emphasis on raw processing power and emerging concerns about energy efficiency.

Hardware specialization will likely redefine the way computational algorithms are developed over the next two decades for a wide range of important applications: large-scale PDE-based problems (CFD, wave propagation, subsurface modeling, etc.), artificial intelligence,
computational chemistry, and optimization problems, to name a few. The pressure to decrease time to solution or improve simulation fidelity, or both, for these applications will continue unabated.

This minisymposium provides a forum for sharing innovative ideas on algorithm development for leveraging future computing platforms.</blurb>
    <EventParentName></EventParentName>
    <external_id>67837-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS52 Improving Productivity and Sustainability for Parallel Computing Software - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS52</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>502</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67773</EventHandoutURL>
    <blurb>As we move toward exascale machines and beyond, we face a daunting task of providing software for emerging extreme-scale disruptive architectures that will allow applications to run with extreme performance, high scalability, and reliability. In addition, demand for crosscutting software that spans multiple domain-specific sciences, a clamor for greater reproducibility, and new opportunities for greatly improved simulation capabilities, especially through coupling of data and physics across scales, will result in software being extremely complex and challenging. While hardware remains a primary focus for next-generation exascale computing, the aforementioned challenges also demand large investments in scientific software development.

In this minisymposium, we aim to cover lost ground and address growing technical, practical, and social challenges in software productivity, quality, and sustainability, with a goal of helping develop better and long-living, cutting-edge parallel computing software that can fulfill its designated roles in emerging exascale ecosystems. Along with sharing of diverse experiences by experts in the parallel computing field, the hope is that this minisymposium will motivate the community to realign and refocus on developer productivity and software sustainability---which are both urgent and essential---to build better scientific software.</blurb>
    <EventParentName></EventParentName>
    <external_id>67773-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS53 Parallel Sparse and Conventional FFTs, Applications and Implementation - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS53</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67765</EventHandoutURL>
    <blurb>The fast Fourier Transform (FFT) is an algorithm used in a wide variety of applications, yet does not make optimal use of many current and emerging platforms such as many-core processors, GPUs, and distributed-memory systems. Hardware utilization performance on its own does not, however, imply optimal problem-solving. The purpose of this minisymposium is to enable an exchange of information between people working on FFT algorithms such as sparse and conventional FFTs, to those working on FFT implementations, in particular for parallel hardware.</blurb>
    <EventParentName></EventParentName>
    <external_id>67765-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS54 Particle Methods: Algorithms and Software Technology for Exascale - Part I of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS54</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67828</EventHandoutURL>
    <blurb>Computational algorithms that use particles are ubiquitous in high performance computing. In a given algorithm, particles may represent physical particles as in molecular dynamics algorithms, they may represent ensembles of physical particles such as in some variations of particle-in-cell methods, or they may represent a discrete computational element of the continuum in Lagrangian or hybrid Lagrangian-Eulerian computational schemes for fluids and mechanics. Within the Exascale Computing Project (ECP), a large project sponsored by the U.S. Department of Energy for the development of exascale hardware, software, and science applications, numerous groups are developing particle methods targeting forthcoming exascale machines with a focus on performance and portability. In this minisymposium we will present software libraries and frameworks currently under development in ECP that are designed for deploying particle methods at scale, we will discuss algorithmic developments and performance engineering of particle methods along with their unique challenges, and then finally we will cover several of the ECP applications themselves, many of which use the aforementioned software libraries and frameworks. ECP particle applications covered will include particle-in-cell algorithms, molecular dynamics, as well as other Lagrangian and hybrid Lagrangian-Eulerian schemes with specific applications to linear accelerator modeling, plasma physics, cosmology, and multi-phase flow.</blurb>
    <EventParentName></EventParentName>
    <external_id>67828-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS55 High-Performance Tensor Computation and Applications - Part I of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS55</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67756</EventHandoutURL>
    <blurb>Tensors are higher order generalization of matrices that provide a natural way to represent a multi-relational dataset. Given a dataset encoded as a tensor, tensor decomposition serves as a promising analytics tool for mining this data to uncover hidden structure within the data's relations.

This minisymposium explores efficient and scalable solutions for calculating tensor decomposition, as well as its application in data analytics across areas spanning signal processing, cybersecurity, machine learning, and beyond.</blurb>
    <EventParentName></EventParentName>
    <external_id>67756-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS56 HPC Aspects of Tsunami Simulation</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS56</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67714</EventHandoutURL>
    <blurb>To support hazard assessment and mitigation, large ensembles of tsuinami simulations are required, to cover parameter spaces, expose sensitivities and possibly exploit formal uncertatinty quantification techniques. Hence, we need to bring down the time to solution for individual tsunami simulations as far as possible, exploiting latest advances in numerics and (adaptive) algorithms and especially latest HPC architectures.</blurb>
    <EventParentName></EventParentName>
    <external_id>67714-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS57 Parallel Eigenvalue Algorithms for Physical Simulation - Part I of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS57</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67779</EventHandoutURL>
    <blurb>Large scale eigenvalue computations are ubiquitous throughout scientific computation.
However, the steep $O(N^3)$ scaling of traditional eigenvalue algorithms often lead to the solution of a particular eigenvalue problem becoming the computational bottleneck in many simulations of physical systems.
Over the years, many important algorithmic developments have been made to allow leverage of the latest advances in massively parallel computing architectures to enable the simulation of large physical systems on the world's largest supercomputers.
In this minisymposium, we examine several recent advances in parallel eigenvalue algorithms for eigenvalue problems which arise in scientific computation.</blurb>
    <EventParentName></EventParentName>
    <external_id>67779-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS59 Resilience and Fault Tolerance for Extreme Computing Systems - Part I of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS59</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67783</EventHandoutURL>
    <blurb>The reliability of large scale computing systems has been a major concern of high performance computing due to the ever increasing complexity of hardware and software components combined with the tight power budget for the operations.  Under such unreliable computing systems, it is essential to introduce failure mitigations at the runtime and application layers to complement the resilience at the hardware/system levels. Today, the major resilience scheme is coordinated checkpoint and restart (CR) that involves global coordination of processes and threads.  This global recovery model entails inherent scalability issues to handle a large class of errors and failures, and thus alternative approaches would improve the scalability and reliability of computing systems and applications together. In this minisymposium, we will discuss the reliability issues of the large scale computing systems, application/algorithm based resilience and programming model support to enhance the reliability of application program executions.  We will also cover the recent global CR techniques that mitigates the performance interference to applications from checkpointing operations. </blurb>
    <EventParentName></EventParentName>
    <external_id>67783-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS6 High Performance Krylov Subspace Methods: Theory, Implementation, and Application - Part I of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS6</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  2:40PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67748</EventHandoutURL>
    <blurb>Krylov subspace methods are widely used for solving very large (sparse) linear systems and eigenvalue problems. However, when these algorithms are implemented on parallel architectures in their original forms, communication (rather than floating point operations) becomes the dominant cost. To address this, many mathematically equivalent implementations, designed explicitly to reduce the time spent on communication, have been introduced. However on some problems these new formulations can behave (very) differently in finite precision than the original implementations. This makes the design of numerically stable variants for parallel architectures challenging.

In this minisymposium we hear from researchers about the design and analysis of high performance Krylov subspace methods, practical concerns relating to implementing these methods on modern parallel architectures, and applications relevant today.</blurb>
    <EventParentName></EventParentName>
    <external_id>67748-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS60 Low-Rank Compression-Based Fast Sparse Direct Solvers</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS60</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  3:20PM</starts_at>
    <ends_at>Feb 14 2020  5:00PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>607</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67831</EventHandoutURL>
    <blurb>Recent advances have drastically reduced the asymptotic complexity of computation and memory usage of sparse factorization based solvers by incorporating hierarchical rank-structured or other data-sparse matrix compression techniques. This has resulted in complex solvers that are highly scalable, purely algebraic and robust even for highly indefinite and ill-conditioned systems.  In this minisymposium, we discuss algorithmic innovations, recent software releases and potential applications of these solvers. We hope to open a discussion on the relative merits of different approaches in terms of both their asymptotic and practical benefits and improve collaboration among mathematics researchers and application scientists in need of fast sparse direct solvers.</blurb>
    <EventParentName></EventParentName>
    <external_id>67831-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS61 Advances in Parallel Sparse Linear System Solver Stacks for Exascale - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS61</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 12:20PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>100</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67789</EventHandoutURL>
    <blurb>Our minisymposium presents advances in high-performance algorithms and GPU implementations in sparse linear system solver stacks including Trilinos-Belos and Muelu, Hypre-BoomerAMG, VT libParaNumAl high-order matrix-free libraries and fast direct methods based on low-rank approximation.   Although the focus is on low communication algorithms and GPU implementations, we also consider Exascale applications and mini-apps that can demonstrate significant performance gains by employing these frameworks.  For Krylov methods our focus is on recent advances in
communication-avoiding and low-synch algorithms.  Preconditioners, such as ILU will require fast and possibly iterative triangular solvers. Polynomial type preconditioners and AMG smoothers have also been advocated for GPU, along with sparse approximate inverse algorithms such as AINV. GPU acceleration is prompting new directions and a re-examination of the basic set-up and solution algorithms.  For example, semi-structured AMG facilitates the local tuning of smoothers to improve convergence and execution speed.  We bring together the architects of these important solver frameworks to explore algorithm choices, describe the latest performance results using these
approaches and discuss future plans for the recently announced Exascale
computers Aurora and Frontier.</blurb>
    <EventParentName></EventParentName>
    <external_id>67789-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS62 Novel Computational Algorithms for Future Computing Platforms - Part II of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS62</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 12:20PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67838</EventHandoutURL>
    <blurb>In the early 2000s, due to constraints on economical heat dissipation, clock speeds of single-core CPUs could no longer be increased, which marked the adoption of multi-core CPUs, together with a paradigm shift to algorithms specifically designed for multi-core architectures.
About 15 years into this current architectural cycle and on its way to exascale performance, the computing industry finds itself at the confluence of technical difficulties that cast doubt on its
ability to sustain this architectural model beyond the exascale capability. These difficulties are driving the hardware industry to develop application-specific chips and to look beyond silicon-
based chips (e.g., quantum computing, physical annealing, neuromorphics, etc.), with a continued emphasis on raw processing power and emerging concerns about energy efficiency.

Hardware specialization will likely redefine the way computational algorithms are developed over the next two decades for a wide range of important applications: large-scale PDE-based problems (CFD, wave propagation, subsurface modeling, etc.), artificial intelligence,
computational chemistry, and optimization problems, to name a few. The pressure to decrease time to solution or improve simulation fidelity, or both, for these applications will continue unabated.

This minisymposium provides a forum for sharing innovative ideas on algorithm development for leveraging future computing platforms.</blurb>
    <EventParentName></EventParentName>
    <external_id>67838-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS63 Data-Centric Operating Systems and Runtimes</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS63</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 12:20PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>505</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>801</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67775</EventHandoutURL>
    <blurb>Data-Centric Operating Systems and
Runtimes are becoming increasingly important to maximize the efficiency of
scalable parallel scientific computing
applications.

These new systems develop custom runtime and operating systems
software to maximize the integration of heterogeneous coprocessors
and I/O devices, using global system knowledge to efficiently
schedule tasks between system components, making it possible to
develop applications which fully utilize system resources without
imposing an undue hardship on the application developer.

The proposed minisymposium gathers
4 talks, covering systems software
support for future heterogeneous
architectures, specifically new
Operating Systems, proving runtimes
correct, the challenges of heterogeneity
and future systems architectures and
their systems software. 

The objective is
to summarize the latest developments
in data-centric operating systems and runtime
research and how the capabilities they
provide relate to the construction of scientific
applications. 

The goal is to provide
applications researchers a view of
future systems software and how
these new capabilities relate to the
construction of scientific applications
on future data-centric systems.</blurb>
    <EventParentName></EventParentName>
    <external_id>67775-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS64 Particle Methods: Algorithms and Software Technology for Exascale - Part II of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS64</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 12:20PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67829</EventHandoutURL>
    <blurb>Computational algorithms that use particles are ubiquitous in high performance computing. In a given algorithm, particles may represent physical particles as in molecular dynamics algorithms, they may represent ensembles of physical particles such as in some variations of particle-in-cell methods, or they may represent a discrete computational element of the continuum in Lagrangian or hybrid Lagrangian-Eulerian computational schemes for fluids and mechanics. Within the Exascale Computing Project (ECP), a large project sponsored by the U.S. Department of Energy for the development of exascale hardware, software, and science applications, numerous groups are developing particle methods targeting forthcoming exascale machines with a focus on performance and portability. In this minisymposium we will present software libraries and frameworks currently under development in ECP that are designed for deploying particle methods at scale, we will discuss algorithmic developments and performance engineering of particle methods along with their unique challenges, and then finally we will cover several of the ECP applications themselves, many of which use the aforementioned software libraries and frameworks. ECP particle applications covered will include particle-in-cell algorithms, molecular dynamics, as well as other Lagrangian and hybrid Lagrangian-Eulerian schemes with specific applications to linear accelerator modeling, plasma physics, cosmology, and multi-phase flow.</blurb>
    <EventParentName></EventParentName>
    <external_id>67829-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS65 High-Performance Tensor Computation and Applications - Part II of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS65</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 12:20PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67757</EventHandoutURL>
    <blurb>Tensors are higher order generalization of matrices that provide a natural way to represent a multi-relational dataset. Given a dataset encoded as a tensor, tensor decomposition serves as a promising analytics tool for mining this data to uncover hidden structure within the data's relations.

This minisymposium explores efficient and scalable solutions for calculating tensor decomposition, as well as its application in data analytics across areas spanning signal processing, cybersecurity, machine learning, and beyond.</blurb>
    <EventParentName></EventParentName>
    <external_id>67757-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS66 New Approaches for Software Auto-Tuning and Accuracy Assurance - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS66</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 12:20PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67742</EventHandoutURL>
    <blurb>Exascale computers are expected to be deployed in the next several years. Their architectures will be complex, building uponmulti-core units, and offering unprecedented levels of parallelism. It is expected that auto-tuning (AT) research and technology will continue building upon its proven success for delivering high performance on a variety of computer architectures, thus enabling optimized, high performance implementations of specific computations for those challenging architectures. 

While performance is a major goal, in many conventional numerical computations accuracy is not well-assured. Here, we can think of the Basic Linear Algebra Subprograms (BLAS), which are often used as kernels in more complex linear algebra computations. The BLAS are carefully optimized for speed (e.g. by computer vendors), but the accuracy of the results is somewhat neglected. In fact, ensuring the accuracy of the computational results of BLAS operations is recurrent and crucial problem. Accuracy assurance becomes more difficult for more complex algorithms, for example for eigensolvers, which are essential parts in a number of applications of interest.

This minisymposium will discuss state-of-the-art technologies for AT, accuracy assurance, and innovative algorithms that target extreme levels of computing.</blurb>
    <EventParentName></EventParentName>
    <external_id>67742-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS67 Parallel Eigenvalue Algorithms for Physical Simulation - Part II of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS67</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 12:20PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67780</EventHandoutURL>
    <blurb>Large scale eigenvalue computations are ubiquitous throughout scientific computation.
However, the steep $O(N^3)$ scaling of traditional eigenvalue algorithms often lead to the solution of a particular eigenvalue problem becoming the computational bottleneck in many simulations of physical systems.
Over the years, many important algorithmic developments have been made to allow leverage of the latest advances in massively parallel computing architectures to enable the simulation of large physical systems on the world's largest supercomputers.
In this minisymposium, we examine several recent advances in parallel eigenvalue algorithms for eigenvalue problems which arise in scientific computation.</blurb>
    <EventParentName></EventParentName>
    <external_id>67780-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS68 Advanced HPC Trends Oil and Gas Applications - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS68</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 12:20PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>1900</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67846</EventHandoutURL>
    <blurb>For decades, advances in high performance computing have led to more accurate, safer, and faster oil and gas exploration and production processes.  Applications range from seismic imaging and reservoir simulations to seismic interpretation and digital rock physics.  This evolution is paramount today to further develop production by exploring increasingly complex reservoirs and by enhancing the recovery ratios from existing fields. In fact, these requirements translate into larger volumes of data to process and models of higher fidelity in terms of physical formulation and space and time resolutions.
 
To cope with these challenges, the oil and gas industry has to adapt constantly to a changing technology landscape in terms of algorithms, platforms, software, and tools. The move towards less synchrony at all system levels, the widening gap between cost of IO versus floating-point operations, and the recent advent of Deep Learning are a few of these important changes the oil and gas software industry has to consider in moving forward with exascale. 

This minisymposium is an opportunity to discuss today's and future trends defining oil and gas HPC applications design.  

</blurb>
    <EventParentName></EventParentName>
    <external_id>67846-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS69 The Many Faces of Simulation for HPC - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS69</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 12:20PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67786</EventHandoutURL>
    <blurb>In the field of HPC research and development, simulation has mainly been used for the purpose of evaluating and comparing the performance of application implementations and of the algorithms therein. While this use remains critical, for good reasons, many other compelling use cases have emerged. These have often been made possible by recent advances in the simulation methodologies at the core of available simulation frameworks.  Examples of new areas in which simulation has become a compelling proposition include debugging and verification, application/simulation co-design, and HPC education. In this multi-part minisymposium, we bring together researchers who have contributed to traditional and explored emerging uses of simulation of HPC systems and applications. The objective is for them to share their experiences, present recent results, identify areas of convergence, and discuss future directions.</blurb>
    <EventParentName></EventParentName>
    <external_id>67786-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS7 Advances in Parallel-in-Time Integration Methods</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS7</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  2:40PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>600</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67845</EventHandoutURL>
    <blurb>The rapid increase and availability of massively parallel computational resources has inspired new ways to parallelize numerical algorithms associated with evolutionary processes. The need for parallel-in-time integration methods is mainly being driven by changes in computer architectures where future speedups will be available through greater concurrency rather than reduced clock-speeds, which are stagnant.
Parallel-in-time approaches introduce a new dimension of parallelism by distributing workload to multiple processors along the time domain of dynamical systems. This minisymposium presents recent advances in both theoretical and algorithmic aspects of parallel-in-time approaches for simulation and optimization with evolutionary processes, with applications to realistic scenarios in science and engineering.</blurb>
    <EventParentName></EventParentName>
    <external_id>67845-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS70 Resilience and Fault Tolerance for Extreme Computing Systems - Part II of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS70</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020 10:40AM</starts_at>
    <ends_at>Feb 15 2020 12:20PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67784</EventHandoutURL>
    <blurb>The reliability of large scale computing systems has been a major concern of high performance computing due to the ever increasing complexity of hardware and software components combined with the tight power budget for the operations.  Under such unreliable computing systems, it is essential to introduce failure mitigations at the runtime and application layers to complement the resilience at the hardware/system levels. Today, the major resilience scheme is coordinated checkpoint and restart (CR) that involves global coordination of processes and threads.  This global recovery model entails inherent scalability issues to handle a large class of errors and failures, and thus alternative approaches would improve the scalability and reliability of computing systems and applications together. In this minisymposium, we will discuss the reliability issues of the large scale computing systems, application/algorithm based resilience and programming model support to enhance the reliability of application program executions.  We will also cover the recent global CR techniques that mitigates the performance interference to applications from checkpointing operations. </blurb>
    <EventParentName></EventParentName>
    <external_id>67784-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS71 Toward Efficient Software Integration and Deployment</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS71</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67782</EventHandoutURL>
    <blurb>With the increasing complexity and diversity of the software stack and system architecture of high performance computing (HPC) systems, the traditional HPC community is facing a huge productivity challenge in software building, integration and deployment for multiple exascale computing systems that will be deployed in year 2020 and after.  Recently, this challenge has been addressed by new software build management tools such as Spack and Guix that enable seamless software building and integration.  Despite these efforts, we are wanting in software testing, verification and deployment methodologies and tools to complement the capability of these new software integration tools. Effective automation of these processes will reduce the code development time and the system resource usage of exascale systems for preparing applications for future systems.

In this minisymposium, we will discuss the recent efforts and techniques to improve software integration process, testing and deployment for HPC platforms.  We will cover the issues on software development practice, use of containers and automation of performance testing and software building.
</blurb>
    <EventParentName></EventParentName>
    <external_id>67782-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS72 Novel Computational Algorithms for Future Computing Platforms - Part III of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS72</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>501</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67839</EventHandoutURL>
    <blurb>In the early 2000s, due to constraints on economical heat dissipation, clock speeds of single-core CPUs could no longer be increased, which marked the adoption of multi-core CPUs, together with a paradigm shift to algorithms specifically designed for multi-core architectures.
About 15 years into this current architectural cycle and on its way to exascale performance, the computing industry finds itself at the confluence of technical difficulties that cast doubt on its
ability to sustain this architectural model beyond the exascale capability. These difficulties are driving the hardware industry to develop application-specific chips and to look beyond silicon-
based chips (e.g., quantum computing, physical annealing, neuromorphics, etc.), with a continued emphasis on raw processing power and emerging concerns about energy efficiency.

Hardware specialization will likely redefine the way computational algorithms are developed over the next two decades for a wide range of important applications: large-scale PDE-based problems (CFD, wave propagation, subsurface modeling, etc.), artificial intelligence,
computational chemistry, and optimization problems, to name a few. The pressure to decrease time to solution or improve simulation fidelity, or both, for these applications will continue unabated.

This minisymposium provides a forum for sharing innovative ideas on algorithm development for leveraging future computing platforms.</blurb>
    <EventParentName></EventParentName>
    <external_id>67839-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS73 Particle Methods: Algorithms and Software Technology for Exascale - Part III of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS73</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>507</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67830</EventHandoutURL>
    <blurb>Computational algorithms that use particles are ubiquitous in high performance computing. In a given algorithm, particles may represent physical particles as in molecular dynamics algorithms, they may represent ensembles of physical particles such as in some variations of particle-in-cell methods, or they may represent a discrete computational element of the continuum in Lagrangian or hybrid Lagrangian-Eulerian computational schemes for fluids and mechanics. Within the Exascale Computing Project (ECP), a large project sponsored by the U.S. Department of Energy for the development of exascale hardware, software, and science applications, numerous groups are developing particle methods targeting forthcoming exascale machines with a focus on performance and portability. In this minisymposium we will present software libraries and frameworks currently under development in ECP that are designed for deploying particle methods at scale, we will discuss algorithmic developments and performance engineering of particle methods along with their unique challenges, and then finally we will cover several of the ECP applications themselves, many of which use the aforementioned software libraries and frameworks. ECP particle applications covered will include particle-in-cell algorithms, molecular dynamics, as well as other Lagrangian and hybrid Lagrangian-Eulerian schemes with specific applications to linear accelerator modeling, plasma physics, cosmology, and multi-phase flow.</blurb>
    <EventParentName></EventParentName>
    <external_id>67830-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS74 High-Performance Tensor Computation and Applications - Part III of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS74</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>508</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67758</EventHandoutURL>
    <blurb>Tensors are higher order generalization of matrices that provide a natural way to represent a multi-relational dataset. Given a dataset encoded as a tensor, tensor decomposition serves as a promising analytics tool for mining this data to uncover hidden structure within the data's relations.

This minisymposium explores efficient and scalable solutions for calculating tensor decomposition, as well as its application in data analytics across areas spanning signal processing, cybersecurity, machine learning, and beyond.</blurb>
    <EventParentName></EventParentName>
    <external_id>67758-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS75 New Approaches for Software Auto-Tuning and Accuracy Assurance - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS75</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>512</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>807</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67743</EventHandoutURL>
    <blurb>Exascale computers are expected to be deployed in the next several years. Their architectures will be complex, building uponmulti-core units, and offering unprecedented levels of parallelism. It is expected that auto-tuning (AT) research and technology will continue building upon its proven success for delivering high performance on a variety of computer architectures, thus enabling optimized, high performance implementations of specific computations for those challenging architectures. 

While performance is a major goal, in many conventional numerical computations accuracy is not well-assured. Here, we can think of the Basic Linear Algebra Subprograms (BLAS), which are often used as kernels in more complex linear algebra computations. The BLAS are carefully optimized for speed (e.g. by computer vendors), but the accuracy of the results is somewhat neglected. In fact, ensuring the accuracy of the computational results of BLAS operations is recurrent and crucial problem. Accuracy assurance becomes more difficult for more complex algorithms, for example for eigensolvers, which are essential parts in a number of applications of interest.

This minisymposium will discuss state-of-the-art technologies for AT, accuracy assurance, and innovative algorithms that target extreme levels of computing.</blurb>
    <EventParentName></EventParentName>
    <external_id>67743-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS76 Parallel Eigenvalue Algorithms for Physical Simulation - Part III of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS76</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>603</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67781</EventHandoutURL>
    <blurb>Large scale eigenvalue computations are ubiquitous throughout scientific computation.
However, the steep $O(N^3)$ scaling of traditional eigenvalue algorithms often lead to the solution of a particular eigenvalue problem becoming the computational bottleneck in many simulations of physical systems.
Over the years, many important algorithmic developments have been made to allow leverage of the latest advances in massively parallel computing architectures to enable the simulation of large physical systems on the world's largest supercomputers.
In this minisymposium, we examine several recent advances in parallel eigenvalue algorithms for eigenvalue problems which arise in scientific computation.</blurb>
    <EventParentName></EventParentName>
    <external_id>67781-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS77 Advanced HPC Trends for Oil and Gas Applications - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS77</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>1900</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67853</EventHandoutURL>
    <blurb>For decades, advances in high performance computing have led to more accurate, safer, and faster oil and gas exploration and production processes.  Applications range from seismic imaging and reservoir simulations to seismic interpretation and digital rock physics.  This evolution is paramount today to further develop production by exploring increasingly complex reservoirs and by enhancing the recovery ratios from existing fields. In fact, these requirements translate into larger volumes of data to process and models of higher fidelity in terms of physical formulation and space and time resolutions.
 
To cope with these challenges, the oil and gas industry has to adapt constantly to a changing technology landscape in terms of algorithms, platforms, software, and tools. The move towards less synchrony at all system levels, the widening gap between cost of IO versus floating-point operations, and the recent advent of Deep Learning are a few of these important changes the oil and gas software industry has to consider in moving forward with exascale. 

This minisymposium is an opportunity to discuss today's and future trends defining oil and gas HPC applications design.  

</blurb>
    <EventParentName></EventParentName>
    <external_id>67853-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS78 The Many Faces of Simulation for HPC - Part II of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS78</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67787</EventHandoutURL>
    <blurb>In the field of HPC research and development, simulation has mainly been used for the purpose of evaluating and comparing the performance of application implementations and of the algorithms therein. While this use remains critical, for good reasons, many other compelling use cases have emerged. These have often been made possible by recent advances in the simulation methodologies at the core of available simulation frameworks.  Examples of new areas in which simulation has become a compelling proposition include debugging and verification, application/simulation co-design, and HPC education. In this multi-part minisymposium, we bring together researchers who have contributed to traditional and explored emerging uses of simulation of HPC systems and applications. The objective is for them to share their experiences, present recent results, identify areas of convergence, and discuss future directions.</blurb>
    <EventParentName></EventParentName>
    <external_id>67787-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS79 Resilience and Fault Tolerance for Extreme Computing Systems - Part III of III</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS79</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  1:50PM</starts_at>
    <ends_at>Feb 15 2020  3:30PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>802</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67785</EventHandoutURL>
    <blurb>The reliability of large scale computing systems has been a major concern of high performance computing due to the ever increasing complexity of hardware and software components combined with the tight power budget for the operations.  Under such unreliable computing systems, it is essential to introduce failure mitigations at the runtime and application layers to complement the resilience at the hardware/system levels. Today, the major resilience scheme is coordinated checkpoint and restart (CR) that involves global coordination of processes and threads.  This global recovery model entails inherent scalability issues to handle a large class of errors and failures, and thus alternative approaches would improve the scalability and reliability of computing systems and applications together. In this minisymposium, we will discuss the reliability issues of the large scale computing systems, application/algorithm based resilience and programming model support to enhance the reliability of application program executions.  We will also cover the recent global CR techniques that mitigates the performance interference to applications from checkpointing operations. </blurb>
    <EventParentName></EventParentName>
    <external_id>67785-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS8 Co-Design of Networking for Scientific HPC Applications</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS8</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  2:40PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>604</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>700</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67721</EventHandoutURL>
    <blurb>Modernizing scientific codes to run performantly on multiple emerging architectures is a task of great complexity and challenge for computational scientists today. We examine the intersection between multiple topic areas related to networking and scientific HPC applications; specifically, the evaluation of both network tapering, and new topologies, and the evaluation of on-node bottlenecks, that impact internode communication. This minisymposium will address strategies to act on issues that arise from networking, and evaluations of new topics, for some large codes today.</blurb>
    <EventParentName></EventParentName>
    <external_id>67721-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MS9 Parallel Adaptive Multigrid - Part I of II</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS9</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  2:40PM</ends_at>
    <EventFilter>PP20|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>606</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords>3000</keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67818</EventHandoutURL>
    <blurb>The multigrid method is well-known as an efficient and highly-scalable solver for many applications. Parallel adaptive mesh refinement nicely integrates with the geometric multigrid method and parallel algebraic multigrid methods are established as state-of-the-art parallel sparse matrix solvers. This session is dedicated to advances for the multigrid method with a focus on adaptive and parallel simulations. It offers a platform to present applications that allow for profitable employment of adaptive mesh refinement and implementations for high-performance computing.</blurb>
    <EventParentName></EventParentName>
    <external_id>67818-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>MT1 Combinatorial Optimization on Quantum Computers</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MT1</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  1:00PM</starts_at>
    <ends_at>Feb 12 2020  2:40PM</ends_at>
    <EventFilter>PP20|Minitutorial|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>605 </location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67813</EventHandoutURL>
    <blurb>Quantum computing has the potential to provide speedups over classical state-of-the-art for some combinatorial optimization problems. Recent advances in both hardware and algorithm development have made it possible to solve small problems on modern quantum computers. Combinatorial optimization problems (especially NP-hard problems) are of particular interest, since for many of these problems best classical algorithms can not provide solutions of sufficient quality in reasonable time. One such problem is MaxCut on graphs. In this minitutorial, we will introduce the MaxCut problem and explain how it can be solved on IBM quantum computers available on the cloud today using the Qiskit framework. Our presentation will assume little to no prior knowledge of quantum computation. Moreover, we will provide examples of how more complicated problems can be solved using the QAOA (Quantum Approximate Optimization Algorithm). The tutorial will be interactive and will use Jupyter notebooks to explain step by step how to formulate and solve optimization problems on quantum computers.

Participants are encouraged to bring their own laptops.</blurb>
    <EventParentName></EventParentName>
    <external_id>67813-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>PD1 Is AI transforming HPC or HPC transforming AI?</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PD1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  9:25AM</starts_at>
    <ends_at>Feb 13 2020 10:25AM</ends_at>
    <EventFilter>PP20|Panel Discussion|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68721</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68721-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T09:25:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>PP1 Poster Session</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP1</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68179</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68179-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>PP2 Poster Session</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP2</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  6:00PM</starts_at>
    <ends_at>Feb 13 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name></location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68722</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68722-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Registration</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  8:00AM</starts_at>
    <ends_at>Feb 12 2020  5:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - </location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68164-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T08:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Registration</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  8:00AM</starts_at>
    <ends_at>Feb 13 2020  5:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68169-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T08:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Registration</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  8:00AM</starts_at>
    <ends_at>Feb 14 2020  5:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68170-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T08:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Registration</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 15, 2020</EventDate>
    <starts_at>Feb 15 2020  8:00AM</starts_at>
    <ends_at>Feb 15 2020  2:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Foyer - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68171-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-15T08:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>SIAG/SC Business Meeting</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 13, 2020</EventDate>
    <starts_at>Feb 13 2020  5:15PM</starts_at>
    <ends_at>Feb 13 2020  6:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68178-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-13T17:15:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>SP1 SIAG Best Paper Prize:  The BLIS Framework: Experiments in Portability</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>SP1</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  8:30AM</starts_at>
    <ends_at>Feb 14 2020  9:15AM</ends_at>
    <EventFilter>PP20|Prize Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68266</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68266-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T08:30:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>SP2 SIAG/Supercomputing Early Career Prize: Scalable Algorithms for Tensor Computations</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>SP2</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  9:25AM</starts_at>
    <ends_at>Feb 14 2020  9:55AM</ends_at>
    <EventFilter>PP20|Prize Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68268</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68268-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T09:25:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>SP3 SIAG/Supercomputing Career Prize: Ghosts of Parallel Computing: Past, Present, and Future</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>SP3</EventNumber>
    <EventDate>Feb 14, 2020</EventDate>
    <starts_at>Feb 14 2020  9:55AM</starts_at>
    <ends_at>Feb 14 2020 10:25AM</ends_at>
    <EventFilter>PP20|Prize Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=68267</EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68267-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-14T09:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Welcome Reception</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  6:00PM</starts_at>
    <ends_at>Feb 12 2020  8:00PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Gallery - 5th Floor</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68168-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP20</ConfCode>
    <name>Welcome Remarks</name>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 12, 2020</EventDate>
    <starts_at>Feb 12 2020  5:00PM</starts_at>
    <ends_at>Feb 12 2020  5:15PM</ends_at>
    <EventFilter>PP20|</EventFilter>
    <EventLocation></EventLocation>
    <location_name>Elwha Ballroom</location_name>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <keywords></keywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerUniqueID>0</EventSpeakerUniqueID>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventParentName></EventParentName>
    <external_id>68167-SESS</external_id>
    <STATUS>active</STATUS>
    <start>2020-02-12T17:00:00</start>
  </meeting>
</meetings>
