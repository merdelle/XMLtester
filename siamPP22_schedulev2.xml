<meetings xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Quantum-Based Molecular Dynamics with Tensor Cores</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP1</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:20:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Joshua Finkelstein</EventSpeakers>
    <EventSpeakerIDs>786005</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116789</EventHandoutURL>
    <EventDescription>Tensor cores, along with tensor processing units, represent a new form of hardware acceleration specifically designed for deep neural network calculations in artificial intelligence applications.  Tensor cores provide extraordinary computational speed and energy efficiency, but with the caveat that they were designed for tensor contractions (matrix-matrix multiplications) using only low precision floating point operations.  Despite this perceived limitation, we demonstrate how tensor cores can be applied with high efficiency to the challenging and numerically sensitive problem of quantum-based Born-Oppenheimer molecular dynamics, which requires highly accurate electronic structure optimizations and conservative force evaluations.   </EventDescription>
    <EventParentName>73758-SESS</EventParentName>
    <EventUniqueID>73758-116789</EventUniqueID>
    <STATUS>inactive</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Openmp Target Task: Tasking and Target Offloading for Heterogeneous Linear Algebra Implementations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP1</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:15:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Pedro Valero-Lara</EventSpeakers>
    <EventSpeakerIDs>771959</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118699</EventHandoutURL>
    <EventDescription>In this talk, it is evaluated the use of OpenMP tasking with target GPU offloading as a potential solution for the implementation of heterogeneous linear algebra algorithms. Also, it is proposed a new OpenMP specification to make the implementation of heterogeneous codes simpler by using OpenMP target task, which integrates both OpenMP tasking and target GPU offloading in a single OpenMP pragma. As a test case, we use one of the most popular and widely used Basic Linear Algebra Subprogram Level-3 routines: triangular solver (TRSM). To benefit from the heterogeneity of the current high-performance computing systems, we propose a different parallelization of the algorithm by using a nonuniform decomposition of the matrices to be computed. We evaluate the use of target GPU offloading inside OpenMP tasks to address the heterogeneity found in the hardware. This new approach
can outperform the state-of-the-art algorithms, which use a uniform decomposition of the data, on both the CPU-only and hybrid CPU-GPU systems, reaching speedups of up to one order of magnitude. The performance that this approach achieves is faster than the IBM ESSL math library on CPU and competitive relative to a highly optimized heterogeneous CUDA version. One node of Oak Ridge National Laboratory’s
supercomputer, Summit, was used for performance analysis.

	</EventDescription>
    <EventParentName>73758-SESS</EventParentName>
    <EventUniqueID>73758-118699</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Load Balancing Parallel Fock Matrix Computation in Nwchemex using Submodular B-Matching</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS6</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>S M Ferdous</EventSpeakers>
    <EventSpeakerIDs>782736</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117832</EventHandoutURL>
    <EventDescription>Computational load balancing in a Parallel environment is challenging. In this talk, we will show how submodular optimization can implicitly provide load balancing. We will formulate a load balancing problem, that arises in parallel Fock matrix computation in NWChemEx, as a submodular b-matching, and will provide an efficient approximation algorithm for it. Using this assignment, we then demonstrate the empirical scalability of Fock matrix computation in 14k processors of Summit supercomputer at ORNL.</EventDescription>
    <EventParentName>73137-SESS</EventParentName>
    <EventUniqueID>73137-117832</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Exascale-Enabled Physical Modeling for Post-Moore Devices</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP1</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:45:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Zhi Jackie Yao</EventSpeakers>
    <EventSpeakerIDs>795435</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119088</EventHandoutURL>
    <EventDescription>We have developed scalable simulation tools to enable leadership computing systems to model emerging post-CMOS microelectronic devices (electronic, spintronic, nanomagnetic and nanomechanical). Our open-source code, ARTEMIS (Adaptive mesh Refinement Time-domain ElectrodynaMIcs Solver), contains support for dispersive material properties, user-defined excitations and boundary conditions, and heterogeneous physical coupling that can be present in next-generation devices. The basic module of ARTEMIS solves Maxwell’s equations for the time-evolving electric and magnetic fields using a finite-difference time-domain approach on structured Cartesian grids. ARTEMIS leverages the developments of two Exascale Computing Projects: (1) the AMReX software libraries for scalable block-structured adaptive mesh refinement applications and (2) the WarpX electromagnetic Particle-In-Cell code. Thus, it is portable and scales well on many-core/GPU-based supercomputers that are far beyond the reach of commercial tools, allowing us to capture larger-scale spatial disparities inherent to realistic circuits. We have demonstrated algorithmic flexibility by developing a micromagnetics module, making it suitable for nonlinear spintronic applications. Our current efforts include upgrading the functionality of ARTEMIS to accurately describe the working mechanism of new devices such as a magnetoelectric spin-orbit logic, enabling ARTEMIS to serve as the device-level design and optimization tool.

	</EventDescription>
    <EventParentName>73758-SESS</EventParentName>
    <EventUniqueID>73758-119088</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Prediction of Optimal Solvers for Sparse Linear Systems using Deep Learning</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP3</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:20:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:40:00 AM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yannick Funk</EventSpeakers>
    <EventSpeakerIDs>805314</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119923</EventHandoutURL>
    <EventDescription>To follow

	</EventDescription>
    <EventParentName>73828-SESS</EventParentName>
    <EventUniqueID>73828-119923</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Comparison of a Randomized and a Deterministic Fixed-Precision Low-Rank Approximation Method for Sparse Matrices</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP4</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:45:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:05:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Robert Ernstbrunner</EventSpeakers>
    <EventSpeakerIDs>792440</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118466</EventHandoutURL>
    <EventDescription>Low-rank approximations of large sparse matrices are important in many scientific applications. Fixed-precision methods compute such approximations when a priori, the approximate rank \(k\) is unknown. A relevant question is the cost required for achieving a certain approximation quality for a large sparse matrix.
We design and evaluate parallel implementations of a variant of the Randomized QB Factorization (RandQB\_EI) and the Truncated LU Factorization with Column and Row Tournament Pivoting (LU\_CRTP).
We experimentally evaluate for different problems the computational cost required for achieving a certain approximation quality. Our results show that LU\_CRTP tends to be particularly competitive for low approximation quality. However, the runtime of LU\_CRTP highly depends on the amount of fill-in produced when updating the input matrix after each iteration, and is therefore outperformed by RandQB\_EI especially for higher approximation quality. Motivated by this observation, we developed a fill-in reducing parallel incomplete LU\_CRTP algorithm where small enough elements of the updated matrix are dropped after each iteration. We illustrate that this approach leads to speedups higher than 20 for some matrices.

</EventDescription>
    <EventParentName>73760-SESS</EventParentName>
    <EventUniqueID>73760-118466</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Scalable GPU H2-Matrix Solvers</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP4</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:20:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:40:00 AM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Wajih Boukaram</EventSpeakers>
    <EventSpeakerIDs>804281</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119089</EventHandoutURL>
    <EventDescription>Hierarchical matrices allow for memory efficient representation of the data sparse matrices that often appear in scientific applications.
The open source H2Opus library provides distributed CPU and GPU implementations of several key operations using the H2-variant of hierarchical matrices,
where nested row and column bases allow for asymptotically optimal memory storage requirements.
In this talk, we discuss the details of the Hierarchical Adaptive Randomized Approximation (HARA) algorithm in H2Opus, where an H2-matrix is constructed
using only fast matrix-vector products in a top-down algorithm. We show how one of the most time consuming phases of the algorithm, the application of low rank updates
followed by multiple compressions, can be improved using randomized sketching.
We also discuss some initial details of a fully algebraic variant of recursive skeletonization factorization based on sketching for matrices constructed using strong admissibility.
</EventDescription>
    <EventParentName>73760-SESS</EventParentName>
    <EventUniqueID>73760-119089</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Cache-Efficient Hybrid-Parallel Algorithm for Sparse Deep Neural Network Inference</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP7</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:35:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:55:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Filip Pawlowski</EventSpeakers>
    <EventSpeakerIDs>788597</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119085</EventHandoutURL>
    <EventDescription>We investigate parallel algorithms for sparse neural network inference, that is, the task of classifying a number of input data items using a sparse neural network. While sparsity is typically exploited to reduce the memory footprint by compressing the matrix storage, we also use it to reduce the communication volume at parallel inference. We propose the staircase model of a neural network, based on rearranging the layers into a diagonal-like form, that allows finding optimal partitionings of the nonzeros of a neural network using hypergraph partitioning software. We propose the hybrid-parallel algorithm which (1) implements model-parallel inference based on partitioning the network into $p0$ parts via the staircase model and (2) replicates such model-parallel inference $p1$ times, with each group of $p0$ processors computing inference on a different batch of input feature matrix, akin to data-parallelism. The proposed hybrid-parallel algorithm on generated deep neural networks consisting of up to 1920 layers on MNIST dataset is up to 23\% faster on a 44-core 2-socket Intel machine and up to 73\% faster on a 96-core 2-socket ARM versus a baseline data-parallel algorithm, confirming that hybrid-parallelism is necessary to obtain scalable performance. To improve performance, we use tiling techniques that cache the intermediate results and/or the neural network layers and estimate $p0$ and $p1$ using a computational model.</EventDescription>
    <EventParentName>73822-SESS</EventParentName>
    <EventUniqueID>73822-119085</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Parallel Minimum Spanning Forest Computation using Sparse Matrix Kernels</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP9</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tim Baer</EventSpeakers>
    <EventSpeakerIDs>796052</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119935</EventHandoutURL>
    <EventDescription>Formulations of graph algorithms using sparse linear algebra have yielded highly scalable distributed algorithms for problems such as connectivity and shortest path computation. We develop the first formulation of the Awerbuch-Shiloach parallel minimum spanning forest (MSF) algorithm using linear algebra primitives. We introduce a multilinear kernel that operates on an adjacency matrix and two vectors. This kernel updates graph vertices by simultaneously using information from both adjacent edges and vertices. In addition, we explore optimizations to accelerate the shortcutting step in the Awerbuch-Shiloach algorithm. We implement this MSF algorithm with Cyclops, a distributed-memory library for generalized sparse tensor algebra. We analyze the parallel scalability of our implementation on the Stampede2 supercomputer.</EventDescription>
    <EventParentName>73830-SESS</EventParentName>
    <EventUniqueID>73830-119935</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Scalability Studies of Albany Land Ice: a Performance Portable, Ice Sheet Solver</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP10</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:50:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:10:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jerry Watkins</EventSpeakers>
    <EventSpeakerIDs>780578</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118588</EventHandoutURL>
    <EventDescription>High resolution simulations of polar ice sheets play a crucial role in the ongoing effort to develop more accurate and reliable Earth-system models for probabilistic sea-level projections. These simulations often require a massive amount of memory and computation from large, heterogeneous supercomputing clusters to provide sufficient accuracy and resolution. In an effort to avoid architecture specific programming, the open-source, C++, finite element code base called Albany uses high level abstractions to integrate Trilinos libraries and the Kokkos programming model for performance portable code across a variety of different architectures. Albany Land Ice utilizes this framework to provide a scalable solver for advanced analysis of the first-order Stokes equations on Greenland and Antarctica.

The primary challenge in simulating ice sheets at scale is solving the linear system associated with a thin, high-aspect ratio mesh. To address this problem, a preconditioner is constructed with matrix-dependent multigrid to semicoarsen to a single layer and algebraic multigrid to coarsen the layer further. This presentation focuses on the extensions to Albany Land Ice and matrix-dependent multigrid to provide a performance portable solver. A scalability study on NERSC Cori and OLCF Summit is used to provide a benchmark for future performance enhancements.</EventDescription>
    <EventParentName>73763-SESS</EventParentName>
    <EventUniqueID>73763-118588</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Matar: A Performance Portability and Productivity Implementation of Data-Oriented Design with Kokkos</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP11</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:30:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:50:00 AM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tanya Tafolla</EventSpeakers>
    <EventSpeakerIDs>796853</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118531</EventHandoutURL>
    <EventDescription>There is a need for simple, fast, and memory-efficient multi-dimensional data structures for dense and sparse storage that arise with numerical methods and in software applications. The data structures must perform equally well across multiple computer architectures, including CPUs and GPUs. For this purpose, we developed MATAR, a C++ software library that allows for simple creation and use of intricate data structures that is also portable across disparate architectures using Kokkos. The performance aspect is achieved by forcing contiguous memory layout (or as close to contiguous as possible) for multi-dimensional and multi-size dense or sparse MATrix and ARray (hence, MATAR) types. Our results show that MATAR has the capability to improve memory utilization,  performance, and programmer productivity in scientific computing. This is achieved by fitting more work into the available memory, minimizing memory loads required, and by loading memory in the most efficient order. We describe the purpose of the work, the implementation of each of the data types, and the resulting performance both in some simple baseline test cases and in an application code. 

	</EventDescription>
    <EventParentName>73764-SESS</EventParentName>
    <EventUniqueID>73764-118531</EventUniqueID>
    <STATUS>inactive</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Welcome Remarks and Presentation: Julia: The Power of Language as an Alternative to MPI</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP1</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  5:15:00 PM</EventStartTime>
    <EventEndTime>02/23/22  5:55:00 PM</EventEndTime>
    <EventFilter>PP22|Invited Speaker</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Alan Edelman</EventSpeakers>
    <EventSpeakerIDs>711886</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119077</EventHandoutURL>
    <EventDescription>When I was a graduate student, Jack Dongarra presented MPI at a big meeting, I think at Oak Ridge. I remember raising my hand and asked whether MPI would just be for some people, and there would be an easier approach for everyone else. As I recall, everybody nodded sure, yes, that's right.  Somehow that did not happen. We very much believe this hard problem has stood in the way of progress in high performance computing. We believe that Julia is part of the solution to this problem.


	

</EventDescription>
    <EventParentName>73437-SESS</EventParentName>
    <EventUniqueID>73437-119077</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T17:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Quantum Future of Computation</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP4</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  9:55:00 AM</EventStartTime>
    <EventEndTime>02/25/22 10:35:00 AM</EventEndTime>
    <EventFilter>PP22|Invited Speaker</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Matthias  Troyer</EventSpeakers>
    <EventSpeakerIDs>802637</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119081</EventHandoutURL>
    <EventDescription>Still in early development, quantum computing is already overturning our contemporary notions of computational methods and devices. Using new concepts of computing based in quantum physics,  quantum computers will be able to solve certain problems that are completely intractable on any imaginable classical computer, such as accurate simulations of molecules and materials, or breaking public key encryption. While this potential is real, quantum computers are best viewed as special purpose accelerators for specific problem classes.
In an effort to bring clarity to the fast-growing field of quantum computing, I will describe the hardware and software architecture of quantum computers and discuss how they differ from conventional classical high performance computers. Based on this, I will also attempt to dispel myths and hype surrounding the field and present a realistic assessment of the potential of these devices and the specific application areas on which they are expected to have a large impact. I will end by showing that quantum computing already generates value today, through quantum inspired approaches. These are quantum approaches implemented on classical hardware that outperform the state of the art of classical methods known before, with applications  in health care, logistics, chemistry and other areas.


	
</EventDescription>
    <EventParentName>73442-SESS</EventParentName>
    <EventUniqueID>73442-119081</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T09:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Nonnegativity Constrained Low Rank Approximations for Scalable Data Analytics on Distributed Memory Parallel Environment</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP5</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  2:20:00 PM</EventStartTime>
    <EventEndTime>02/25/22  3:00:00 PM</EventEndTime>
    <EventFilter>PP22|Invited Speaker</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Haesun Park</EventSpeakers>
    <EventSpeakerIDs>726876</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119080</EventHandoutURL>
    <EventDescription>Constrained Low Rank Approximation (CLRA) is a powerful foundation for important data analytic tasks such as topic modeling and community detection. Some advantages of CLRA include scalable algorithms and software based on advances in numerical linear algebra and parallel computing. Nonnegativity constraints allow more judicious formulation, interpretable results, and effective methods whether the input data is in feature-data relationship as in the Nonnegative Matrix Factorization (NMF) or data-data relationship as in spectral clustering or symmetric NMF (SymNMF). A common foundation of CLRA also allows a hybrid method for information fusion called JointNMF from merging the objective functions of the NMF and SymNMF for multi-view data sets with both content and connection information.

In this talk, a distributed memory parallel algorithm and software framework, PLANC (Parallel Low-rank Approximation with Nonnegativity Constraints), is described for nonnegativity constrained matrix and tensor low rank approximations. PLANC uses parallel distributions and algorithms designed to optimize communication and computation, allows extension regarding data characteristics, algorithm, constraints, and architecture beyond the above mentioned NMF variants. Efficiency and scalability of PLANC and some new knowledge discoveries from large real-world data sets are presented.


	
</EventDescription>
    <EventParentName>73439-SESS</EventParentName>
    <EventUniqueID>73439-119080</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T14:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>CP9 Parallel Numerical Methods (This session includes proceedings papers)</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP9</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 12:50:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>773756</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>773756,796052,805319,710229</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73830</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73830-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>IP1 Welcome Remarks and Presentation: Julia: The Power of Language as an Alternative to MPI </EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP1</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  5:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  6:00:00 PM</EventEndTime>
    <EventFilter>PP22|Invited Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>712754,89317,798097</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>711886</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73437</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73437-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>1</DirectLinkEnabled>
    <DirectLinkRecordingEnabled>1</DirectLinkRecordingEnabled>
    <start>2022-02-23T17:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>IP2 </EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP2</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  8:30:00 AM</EventStartTime>
    <EventEndTime>02/24/22  9:15:00 AM</EventEndTime>
    <EventFilter>PP22|Invited Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>741298</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>735590</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73441</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73441-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>1</DirectLinkEnabled>
    <DirectLinkRecordingEnabled>1</DirectLinkRecordingEnabled>
    <start>2022-02-24T08:30:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Load-Balancing in Cabana and MPM/PIC Specifically</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS1</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Stephan Schulz</EventSpeakers>
    <EventSpeakerIDs>804163</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118948</EventHandoutURL>
    <EventDescription>For a more efficient usage of computation ressources the load balancing library ALL is integrated into Cabana/Cajita.
The load balancer returns a new domain decomposition with a better balanced workload.
Only full cells can be moved across processes.
Although common in PIC based codes, MD codes do not have an inherent cell structure.
They should use a very fine internal grid so the boundaries can be moved with high resolution allowing a finer load balancing.
The performance gain of two setups is shown as well as the necessity of being cautious with memory allocations on the GPU.
The proxy apps ExaMPM and CabanaMD are used to demonstrate the performance gains on summit.
Cabana is backed by kokkos and hence these performance improvements are also portable to other architectures.

</EventDescription>
    <EventParentName>73409-SESS</EventParentName>
    <EventUniqueID>73409-118948</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Alpine: A Portable Plasma and Particle-in-Cell Mini-App for Exascale</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS1</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sriramkrishnan Muralikrishnan</EventSpeakers>
    <EventSpeakerIDs>772169</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118950</EventHandoutURL>
    <EventDescription>The goal of the ALPINE project is to create an electrostatic particle-in-cell mini-app that simulates a set of benchmark problems in plasma physics targeting exascale architectures. The mini-app serves as a proxy for real plasma physics applications. Hence, it can be used as a sandbox for implementing new algorithms and measuring performance on modern computing architectures. It is written using the library independent parallel particle layer (IPPL) which has been recently enhanced with performance portable capabilities. We show the performance for an idealized uniform plasma test case, Penning trap and Landau damping benchmark plasma physics problems in 3D on modern CPU and GPU-based machines.

</EventDescription>
    <EventParentName>73409-SESS</EventParentName>
    <EventUniqueID>73409-118950</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Solving Sparse Linear Systems on FPGAs using OneAPI</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS3</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christopher Siefert</EventSpeakers>
    <EventSpeakerIDs>723261</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117858</EventHandoutURL>
    <EventDescription>Intel's OneAPI  and Data-Parallel C++ (DPCPP) offer the potential to program Intel FPGA devices with the ease of high-level C++ programming, rather than having to write Verilog or other low-level languages.  We consider the solution of linear systems derived from partial differential equations using modified versions of HPCG and MiniFE on Intel Stratix10 FPGA devices.  We discuss implementation considerations of various kernels (e.g. matrix vector multiplication, symmetric Gauss-Seidel smoothing and sparse direct solves) that would be necessary in a multigrid-preconditioned conjugate gradient solve and how implementation choices relate to FPGA performance.


</EventDescription>
    <EventParentName>72941-SESS</EventParentName>
    <EventUniqueID>72941-117858</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Graph Algorithms for the Exascale: A Case Study with Influence Maximization</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS6</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Marco Minutoli</EventSpeakers>
    <EventSpeakerIDs>795247</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117831</EventHandoutURL>
    <EventDescription>Availability of unprecedented amounts of data has ushered in a new era of data-driven sciences, and graph analytics has emerged as one of the leading tools for analysis of unstructured data. Availability of exascale computing platforms will provide an unprecedented access to computing power that will lead to novel discoveries in science. Generalization of graph algorithms in the form of combinatorial optimization has numerous applications in scientific computing and data-driven discovery. Despite widespread use, efficient parallel tools for graph analytics are hard to come by, especially when targeting the hybrid CPU-Graphics Processing Unit (GPU) architectures at extreme scales. In this talk, we will present our ongoing work on distributed multi-GPU systems for influence maximization, a prototypical graph problem. We will demonstrate substantial gains in performance on the current #2 supercomputer, Summit.</EventDescription>
    <EventParentName>73137-SESS</EventParentName>
    <EventUniqueID>73137-117831</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Scalable Algorithms for Controlling Epidemics on Networks using Multiplicative Weights Update (MWU) Method</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS6</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Prathysh Sambaturu</EventSpeakers>
    <EventSpeakerIDs>796163</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117834</EventHandoutURL>
    <EventDescription>As the COVID-19 outbreak has shown, effective interventions strategies such as vaccinations and social distancing are crucial to controlling epidemic outbreaks. We study the Epidemic Control problem, which involves designing interventions, to minimize the expected number of infections in an epidemic outbreak. This is a challenging stochastic optimization problem in the context of the SIR epidemic model on a network. We developed efficient approximation algorithms for this problem using the sample average approximation (SAA) technique from stochastic optimization and linear programming based rounding. We show that this algorithm scales up to networks with about a million nodes corresponding to county-level populations. This algorithm needs to solve the Linear Program (LP) relaxation of the problem using an LP solver, which restricts the scalability of this approach. Therefore, instead of using an LP solver, we design an algorithm that adapts the multiplicative weights update (MWU) framework to approximately solve the LP relaxation for the problem instance. We also present a memory-efficient version of this algorithm which allows it to scale well to networks corresponding to state- and even country-level populations. Also, we show that this approach provides good performance guarantees in practice. </EventDescription>
    <EventParentName>73137-SESS</EventParentName>
    <EventUniqueID>73137-117834</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Power-Aware Resource Management</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS7</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tapasya Patki</EventSpeakers>
    <EventSpeakerIDs>768277</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117227</EventHandoutURL>
    <EventDescription>Power is a critical problem as the supercomputing community ventures toward exascale. Several system, hardware and application-level power requirements make it challenging to schedule jobs effectively with high throughput and utilization on large-scale supercomputers. In this talk, I will present two power-aware scheduling strategies. The first is a dynamic power-aware backfilling algorithm implemented as a simulation study within the popular resource manager, SLURM, and the second is a power-aware manager being developed as part of LLNL’s novel resource management framework for El Capitan, called Flux. Additionally, I will also briefly discuss a variation-aware scheduling algorithm implemented within Flux that addresses manufacturing variability under power constraints. Using real-world datasets from large supercomputers at LLNL and University of Tokyo, I will demonstrate the effectiveness of these power-management scheduling algorithms.</EventDescription>
    <EventParentName>72962-SESS</EventParentName>
    <EventUniqueID>72962-117227</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS87 Kinetic and Continuum Theories for Computational Plasma Physics - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS87</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs>796612,790096,719838</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803632,803650,803653,796612</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73284</EventHandoutURL>
    <EventDescription>Computational plasma physics is a venerable area built on decades of work and many high-quality implementations. This minisymposium will bring together researchers focused on both kinetic and continuum theories. We hope to examine the boundary between these descriptions, look at scenarios which require closer interaction between them such as the solar corona and ultra-cold neutral plasmas, and implementations which can combine these theories.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73284-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Efficient Preconditioners for Interior Point Methods via a New Schur Complementation Strategy</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS10</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Samah Karim</EventSpeakers>
    <EventSpeakerIDs>790068</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118702</EventHandoutURL>
    <EventDescription>For the past few decades, interior point methods (IPM) have gained wide appreciation due to their remarkable success in solving linear and non-linear optimization problems. These iterative methods are dominated in cost by the solution of the Karush-Kuhn-Tucker (KKT) equations at every iteration. We propose a new interior point algorithm for convex quadratic programming problems which aims to reduce the overall cost by re-using the factorization of a fixed KKT subsystem across interior point iterations. Our method uses preconditioned iterative methods to solve the linear systems that arise at every Newton step. These preconditioned conjugate gradient methods operate on an implicit Schur complement of the KKT system at each iteration. In contrast to standard approaches, the Schur complement we consider enables the reuse of the factorization of the Hessian of the equality-constraint Lagrangian across all interior point iterations. Further, the resulting reduced system admits preconditioners that directly alleviate the ill-conditioning associated with the strict complementarity condition in interior point methods. The two preconditioners we propose also provably reduce the number of unique eigenvalues for the coefficient matrix (CG iteration count). One is efficient when the number of equality constraints is small, while the other is efficient when the number of remaining degrees of freedom is small.

</EventDescription>
    <EventParentName>73351-SESS</EventParentName>
    <EventUniqueID>73351-118702</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Predictions of Compressible Steady and Unsteady Fluid Flow using PointNet Deep Learning Backbone</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP2</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Shyma Alhuwaider</EventSpeakers>
    <EventSpeakerIDs>803814</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118715</EventHandoutURL>
    <EventDescription>Computational fluid dynamics (CFD) is known as the analytic process of predicting fluid flow, mass transfer, and other related phenomena during the design or manufacturing process. 
Aggressive use of CFD is credited with drastic reductions in wind tunnel time and lower numbers of experimental rig tests.
CFD has saved millions of dollars for industries, governments, and national laboratories. CFD offers the potential of delivering an insight into the critical physical phenomena limiting component performance, thus opening new frontiers in multiple fields, especially vehicle design. 
One of the key strengths of CFD is its ability to produce simulations (forward steps) that can be used in an inverse design. However, each forward step can be very time-consuming to converge. To enable more efficient and scalable CFD forward steps, we leverage the universal approximation property of deep neural networks (DNNs) to estimate a surrogate to the CFD simulation.
We pass the point cloud of a mesh through several layers of shared multi-layer perceptrons. The network learns an end-to-end mapping between spatial positions and CFD quantities. After training, predictions are instantaneous on new objects with arbitrary geometries and a selected number of time steps. We train the neural network on compressible laminar steady and unsteady flows past various shapes. The network predicts the flow fields hundreds of times faster than our conventional CFD solver while maintaining good accuracy.</EventDescription>
    <EventParentName>73759-SESS</EventParentName>
    <EventUniqueID>73759-118715</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Performance Optimization Methods for a Memory-Bound, Unstructured CFD Application on Massively Parallel GPU Platforms</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP2</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christopher Stone</EventSpeakers>
    <EventSpeakerIDs>803863</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118716</EventHandoutURL>
    <EventDescription>Computational performance of the FUN3D unstructured-grid computational fluid dynamics (CFD) application on massively parallel GPU environments is memory-bound and highly dependent upon efficient reads from and atomic updates to the irregular cell-, edge-, and node-based data structures. In this talk, we present recent efforts into optimizing select performance-critical kernels on NVIDIA Tesla V100 and A100 GPUs and AMD CDNA MI100 GPUs. A novel use of L2 cache residency controls and asynchronous loads into on-chip shared memory are explored on the A100 GPU for the sparse iterative solver, which is dominated by mixed-precision, sparse matrix vector multiplication. Demonstrations show that these methods improve global memory bandwidth utilization by 13.5% on the A100 GPU. Several techniques are also presented that use registers and/or shared memory to facilitate array transposition and aggregation which combine to reduce the frequency and increase the cache efficiency of floating-point atomic updates to the irregular data structures. These methods are demonstrated to improve the kernel throughput by nearly 500% on select kernels on the AMD MI100 over atomic updates directly to global memory. Overall, both V100 and A100 GPUs outperformed the MI100 GPU on kernels dominated by double-precision atomic updates; however, the techniques demonstrated here reduced the performance gap and improved the MI100 performance.
</EventDescription>
    <EventParentName>73759-SESS</EventParentName>
    <EventUniqueID>73759-118716</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Computational Investigation of the Effect of Chemistry on Mars Retropropulsion Environments Using a Massively Parallel GPU Approach</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP2</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Gabriel Nastac</EventSpeakers>
    <EventSpeakerIDs>796761</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118762</EventHandoutURL>
    <EventDescription>In this work, we investigate the effects of chemistry on a human-scale Mars lander concept using scale-resolving computational fluid dynamics (CFD) with finite-rate chemistry and a graphics processing unit (GPU)-enabled implementation of the NASA FUN3D flow solver, enabling run-times of a few days for the simulations presented. Simulations are carried out on Summit at Oak Ridge Leadership Computing Facility using thousands of GPUs. Retropropulsion ground tests require significant compromises on physical scale, instrumentation, configuration, and environments. Ground tests of retropropulsion configurations thus far have neglected effects of chemistry due to physical constraints of wind tunnel models and facilities; most experiments use inert simulant gases at low temperatures. As such, a strong reliance on high-fidelity computational analyses such as those presented in this work is required to expand the knowledge of retropropulsion aerodynamics. An overview of the GPU approach will be presented. Results are compared to a previous scaled perfect gas (air) campaign.</EventDescription>
    <EventParentName>73759-SESS</EventParentName>
    <EventUniqueID>73759-118762</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Gptuneband: Multi-Task and Multi-Fidelity Autotuning for Large-Scale High Performance Computing Applications</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP3</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:15:00 AM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Xinran Zhu</EventSpeakers>
    <EventSpeakerIDs>795697</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119919</EventHandoutURL>
    <EventDescription>Autotuning aims at automatically ?nding code parameters that optimize the runtime, memory, communication cost, or accuracy for complex, black-box functions. We propose a novel multi-task and multi-?delity autotuning framework,
 GPTuneBand, for tuning large-scale expensive high performance computing (HPC) applications. GPTuneBand combines a multi-task Bayesian optimization algorithm with a multi-armed bandit strategy, well-suited for tuning expensive HPC applications such as numerical
 libraries, scienti?c simulation codes and machine learning (ML) models, particularly with a very limited tuning budget. Our numerical results show that compared to other state-of-the-art autotuners, which only allows single-task or single-?delity tuning, GPTuneBand
 obtains signi?cantly better performance for numerical libraries and simulation codes, and competitive validation accuracy for training ML models. When tuning the Hypre library with 12 parameters, GPTuneBand wins over its single-?delity predecessor GPTune on
 62.5% tasks, with a maximum speedup of 1.2x, and wins over a single-task, multi-?delity tuner BOHB on 72.5% tasks. When tuning the MFEM library on large numbers of CPU cores, GPTuneBand obtains a 1.7x speedup when compared with the default code parameters.

	
</EventDescription>
    <EventParentName>73828-SESS</EventParentName>
    <EventUniqueID>73828-119919</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Tuned Preconditioners for the Spectral Element Method on GPUs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP3</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 12:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22 12:30:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Malachi Phillips</EventSpeakers>
    <EventSpeakerIDs>795361</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119940</EventHandoutURL>
    <EventDescription>The Poisson pressure solve resulting from the spectral element discretization
of the incompressible Navier-Stokes equation requires fast, robust, and
scalable preconditioning.  In the current work, a parallel scaling study of
Chebyshev-accelerated Schwarz and Jacobi preconditioning schemes is presented,
with special focus on GPU architectures, such as OLCF's Summit.  Convergence
properties of the Chebyshev-accelerated schemes are compared with alternative
methods, such as low-order preconditioners combined with algebraic multigrid.
Performance and scalability results are presented for a variety of
preconditioner and solver settings. The authors demonstrate that
Chebyshev-accelerated-Schwarz methods provide a robust and effective smoothing
strategy when using $p$-multigrid as a preconditioner in a Krylov-subspace
projector.
The variety of cases to be addressed, on a
wide range of processor counts, suggests that
performance can be enhanced by automated run-time selection of the
preconditioner and associated parameters.
</EventDescription>
    <EventParentName>73828-SESS</EventParentName>
    <EventUniqueID>73828-119940</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Accelerating Coupled Climate Simulations using Efficient Remapping Techniques</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP5</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:15:00 AM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Iulian Grindeanu</EventSpeakers>
    <EventSpeakerIDs>744636</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119105</EventHandoutURL>
    <EventDescription>Predictable simulation of coupled climate models require accurate and conservative transfer of scalar and flux fields defined on unstructured meshes that are often tuned to resolve individual component physics. In the Energy Exascale Earth System Model (E3SM), the Model Coupling Toolkit (MCT) coupler is used for transferring data between components, with the remapping weights computed through an offline pre-processing step. Recent efforts using the Mesh Oriented datABase (MOAB) to compute the remap operator during the simulation has removed several of the bottlenecks in such offline-online workflows. The use of the MOAB coupler has been proven to be efficient at scale (https://doi.org/10.5194/gmd-13-2355-2020) without sacrificing discretization accuracy during transfers. In this talk, we describe latest improvements in MOAB-based framework that has scalable intersection methods, use Zoltan mesh partitioners, fast searching methods from ArborX, accurate projections with TempestRemap library and that can be used on both multi-node manycore and multi-GPU hybrid architectures. We also present several performance and accuracy results to emphasize the feasibility of using this infrastructure for E3SM production climate simulations. 

	
</EventDescription>
    <EventParentName>73826-SESS</EventParentName>
    <EventUniqueID>73826-119105</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>GPU Accelerated Sparse Floyd-Warshall Algorithm for Graph All-Pair Shortest Path (APSP) Computation</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP5</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:20:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:40:00 AM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ramakrishnan Kannan</EventSpeakers>
    <EventSpeakerIDs>771753</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119107</EventHandoutURL>
    <EventDescription>The Floyd-Warshall (FW) algorithm is an attractive choice for computing Graph All-pair Shortest Path (APSP) on high-performing systems as it can be expressed as dense matrix-matrix multiplication on Min-plus semiring.
However, the higher asymptotic complexity of FW for sparse graphs makes it less attractive to work optimal Johnson’s algorithm.
Recently, we proposed a sparse variant of FW (SpFW) algorithm that improves the asymptotic complexity for several classes of sparse graphs.

In this work, we present a GPU accelerated variant of SpFW algorithm based on an efficient min-plus semi-ring matrix multiplication (SrGEMM).
Our GPU accelerated SpFW achieves 80\% of theoretical performance on Nvidia V100 GPUs. On multi-GPU configuration, our implementation achieves 32TFLOP/sec single precision performance on a single node of Summit supercomputer.

Such performance improvement considerably increases the number of cases where SpFW outperforms an efficient multithreaded Johnson's algorithm.</EventDescription>
    <EventParentName>73826-SESS</EventParentName>
    <EventUniqueID>73826-119107</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Swept Framework for Performing Memory Efficient Stencil Calculations in Parallel on Unstructured Meshes</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP6</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:30:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mohammad Islam</EventSpeakers>
    <EventSpeakerIDs>790204</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118739</EventHandoutURL>
    <EventDescription>Stencil calculations lie at the heart of many scientific computing applications, such as the solution of hyperbolic partial differential equations (PDEs) via explicit time-stepping methods, and the solution of linear systems of equations via stationary iterative methods (e.g. Jacobi iteration, Gauss-Seidel). Although straightforward to implement, stencil updates are difficult to perform efficiently on high performance computing architectures as they require much more data access relative to computation (making them memory bound). Many studies for improving the memory efficiency of stencil calculations have focused primarily on structured domains.

In this talk, we present an algorithm for performing stencil calculations on a general unstructured mesh efficiently in parallel (i.e. on parallel CPUs or a GPU). On a GPU, this efficiency is achieved through the use of cached on-chip shared memory for all stencil updates. At each stage of the algorithm (four stages in total), an appropriate partition of the mesh is determined to maximize the number of stencil updates possible within each subdomain. Subdomain data is allocated to the shared memory of GPU blocks where stencil updates can be performed in parallel. We illustrate the algorithm behavior on two examples: a structured square domain and an unstructured mesh around an airfoil. The algorithm only requires knowledge of the mesh connectivity, making it applicable to general unstructured problems.</EventDescription>
    <EventParentName>73761-SESS</EventParentName>
    <EventUniqueID>73761-118739</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Scalable Resilience Against Node Failures for the Communication-Hiding Conjugate Gradient Method with Deep Pipelines</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP7</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:45:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:05:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Viktoria Mayer</EventSpeakers>
    <EventSpeakerIDs>786124</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118297</EventHandoutURL>
    <EventDescription>The growth in the number of nodes in large-scale parallel computers gives rise to two major challenges: global communication operations like global reductions are becoming major bottlenecks due to their limited scalability, and the likelihood of node failures is increasing. We study an approach for addressing these challenges in the context of solving large sparse linear systems. The communication-hiding conjugate gradient method with deep pipelines overlaps communication of global reductions with the computation of matrix-vector products over multiple iterations. We present extensions to this solver which makes it resilient against node failures while fully preserving their communication-hiding properties and thus their scalability. A few redundant copies of local vector elements are efficiently communicated to neighboring nodes with very little overhead. In case of a node failure, these redundant copies are gathered at a replacement node, which reconstructs the lost parts of the solver’s state. After that, the parallel solver can continue as in the failure-free scenario. The cost for the reconstruction depends on the length of the pipeline. Experimental evaluations of our approach illustrate on average low runtime overheads compared to the standard non-resilient algorithm if the pipeline length is not too large. This illustrates that scalable algorithmic resilience can be achieved at rather low extra cost.</EventDescription>
    <EventParentName>73822-SESS</EventParentName>
    <EventUniqueID>73822-118297</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>PDDSparse: A New Approach to Probabilistic Domain Decomposition</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP7</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:30:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jorge Morón</EventSpeakers>
    <EventSpeakerIDs>803588</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118514</EventHandoutURL>
    <EventDescription>
	Probabilistic Domain Decomposition (PDD) is a non-standard, highly-scalable algorithm for large-scale boundary-value problems (BVPs), in which the subdomains are decoupled by first calculating the solution on the fictitious interfaces thanks to probabilistic representations of BVPs. When the BVP coefficients are represented by look-up tables, however, it may happen that they exceed the memory of a single processor, thus preventing PDD from full scalability.
A new version of PDD -PDDSparse- that has been tailored to this scenario will be presented. The idea is to express the solution on an interfacial node as a linear combination of the nodal solutions around them, via formal interpolation. Diffusions from a node are confined to very small regions, ensuring they fit in the processor's memory. This leads to a highly sparse system of equations for the interfacial values only (instead of all the values as it is the case with standard domain decomposition). We will formulate the new algorithm, discuss some implementation options, and present proof-of-concept results and scalability tests performed at  the CINECA supercomputer facility, which illustrate the promise of this formulation. 



</EventDescription>
    <EventParentName>73822-SESS</EventParentName>
    <EventUniqueID>73822-118514</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>QR Decomposition on GPUs in Multiple Double Precision</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP9</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:45:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jan Verschelde</EventSpeakers>
    <EventSpeakerIDs>710229</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118124</EventHandoutURL>
    <EventDescription>The aim is to compensate the cost overhead of multiple double arithmetic
using Graphics Processing Units (GPUs) capable of teraflop performance.

Following the GPU acceleration of the blocked QR decomposition
of [Kerr, Campbell, and Richards, GPGPU'09],
the multiple double arithmetic from the QD library
[Hida, Li, Bailey, Arith-15 2001],
extended with code generated by the CAMPARY software
[Joldes, Muller, Popescu, and Tucker, ICMS 2016]
is applied to accelerate the QR decomposition on
the NVIDIA P100 and V100 GPUs.

Because the problems become compute bound,
teraflop performance is already observed in double double precision
for matrices of size 1,024.

	
</EventDescription>
    <EventParentName>73830-SESS</EventParentName>
    <EventUniqueID>73830-118124</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Subtractive Meshing</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP11</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:20:00 AM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Henrik Stromberg</EventSpeakers>
    <EventSpeakerIDs>803504</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118188</EventHandoutURL>
    <EventDescription>This paper introduces \emph{subtractive meshing}, a novel approach to compute structured and predominantly hexahedral meshes well suited for finite element methods. The main application of this method is mesh generation for structural simulations in mechanical engineering.
Geometries in mechanical engineering are to a large extent driven by manufacturability. Many parts in industrial use are manufactured with subtractive processes such as milling. Thus, the property of a part being manufactured subtractive can be exploited in meshing it. Given that a good mesh for the initial geometry – before machining – can be trivially computed as this geometry is usually sweepable this approach is promising. To leverage this potential an algorithm to perform subtractive operations on meshes was developed.
The described algorithm consists of five stages. In mesh cutting all nodes which are cut away are identified. The element segmentation stage bisects all edges of the mesh and creates sub elements for all elements, which guarantees exclusion of all cut nodes from the mesh and absence of hanging nodes. The third stage assigns mesh entities to all geometric entities using Dijkstra's algorithm and answer set programming. Then the matched mesh entities are forced onto the respective geometry in order to improve geometric accuracy. At last, the mesh is optimized by moving nodes.</EventDescription>
    <EventParentName>73764-SESS</EventParentName>
    <EventUniqueID>73764-118188</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Software as an Instrument of Science </EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP3</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  2:05:00 PM</EventStartTime>
    <EventEndTime>02/24/22  2:45:00 PM</EventEndTime>
    <EventFilter>PP22|Invited Speaker</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Anshu Dubey</EventSpeakers>
    <EventSpeakerIDs>762908</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119076</EventHandoutURL>
    <EventDescription>Ever since high-performance computing has enabled exploration of the world hitherto impossible, scientific software has been a de-facto instrument of science, but without gaining the necessary recognition as such. A culture grew where the next scientific deadline drove development, often at the cost of quality and reusability of software. The care that experimentalists expend on their instruments was almost never accorded by computational scientists to their software. This laissez faire approach to scientific software development also extended to computational experiments, where provenance and reproducibility were often an afterthought. Two external factors are causing the community to re-examine this culture. Firstly, codes and platforms are simultaneously growing in complexity. Secondly, some high visibility failures have placed a spotlight on policy and decision-making dependent upon scientific software. We have now entered an era where we are re-examining our assumptions about software and scrutinizing our sustainability and quality approaches for efficiency and efficacy. This is an opportunity for the high-performance computing community to apply lessons learned from the missteps of the past and fully recognize software as the scientific instrument it is. In this presentation I will describe my experience and wisdom gained as a software architect of a high-performance multiphysics community code. The code, FLASH, began as an astrophysics code, and then went on to serve several additional communities as diverse as solar physics, cosmology, computational fluid dynamics, and laser plasma experiments. A new code Flash-X, derived from FLASH and incorporating our acquired wisdom, is being developed under the Exascale Computing Project to continue to serve most of these communities on heterogeneous platforms. I will describe my take on the blueprint of what our future software endeavors could be if we are intentional about software quality and sustainability, and how we are applying it to Flash-X. 

	

</EventDescription>
    <EventParentName>73436-SESS</EventParentName>
    <EventUniqueID>73436-119076</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T14:05:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Supercomputing, Cloud and Edge Security Models Convergence using APIs and Container Technologies</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP6</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  8:30:00 AM</EventStartTime>
    <EventEndTime>02/26/22  9:10:00 AM</EventEndTime>
    <EventFilter>PP22|Invited Speaker</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sadaf Alam</EventSpeakers>
    <EventSpeakerIDs>804309</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119074</EventHandoutURL>
    <EventDescription>Scientific applications and workflows today are exploiting a range of distributed resources including supercomputing, cloud, and edge technologies, such as scientific instruments.  User access and security models across these resource domains vary considerably resulting in overhead for the development of scientific applications and workflow engines, often leading to productivity losses.  Recently, adoption of zero trust security models have accelerated due to prevalent usage of off-premises cloud and edge technologies. The model relies on having multiple layers of trust and verification for applications, users, devices, and networks.  HPC applications, networks, storage, and access controls have unique architectural features for delivering close-to-metal performance and to achieve scalability.  In this talk, I introduce motivating use cases, discuss challenges in adopting zero trust security models for HPC, and present solutions that we have developed at the Swiss National Supercomputing Centre.  These include a RESTful services gateway called FirecREST and a user-friendly, HPC container engine called Sarus.  FirecREST serves as a secure, web-accessing interface to HPC resources providing a coupling layer between the identity and access management of cloud and the essential POSIX interfaces used in HPC. Sarus fulfills unique HPC applications’ portability requirements offering native hardware performance while improving isolation and security in multi-tenant environments.

	
</EventDescription>
    <EventParentName>73434-SESS</EventParentName>
    <EventUniqueID>73434-119074</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T08:30:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Pumipic: Infrastructure for Unstructured Mesh Pic on GPUs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS1</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Cameron  Smith</EventSpeakers>
    <EventSpeakerIDs>731156</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118947</EventHandoutURL>
    <EventDescription>Developing particle-in-cell (PIC) based fusion particle physics applications that effectively utilize thousands of GPUs in leadership class systems requires a flexible and extensible infrastructure that gives applications control over inter- and intra-node parallelism.  PUMIPic, the parallel unstructured mesh infrastructure for PIC is a C++ Kokkos based library that provides control of inter node parallelism through fast particle load balancing methods from EnGPar, and mesh partitioning methods that support buffer layers to satisfy data dependencies of particle operations using Omega_h.  Intra-node parallelism is provided in PUMIPic through an abstraction layer that allows application developers to quickly implement GPU accelerated kernels that operate on mesh and particle data without needing to specialize the code for the underlying data structure.  The PUMIPic particle data structures group particles by the mesh element they exist within to improve locality to mesh data during particle operations and minimize data movement.  Performance results for these inter- and intra-node methods using four different particle data structures (Sell-C-Sigma, CSR, ECP-COPA Cabana, and DPS) will be presented on leadership class systems.</EventDescription>
    <EventParentName>73409-SESS</EventParentName>
    <EventUniqueID>73409-118947</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Wisteria/BDEC-01 &amp; h3-Open-BDEC: Innovative Scientific Computing by Integration of (Simulation+Data+Learning)</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS2</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kengo Nakajima</EventSpeakers>
    <EventSpeakerIDs>715772</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116914</EventHandoutURL>
    <EventDescription>We propose an innovative method for computational science for sustainable promotion of scientific discovery by supercomputers in the Exascale Era by combining (Simulation + Data + Learning (S+D+L)). In May 2021, we start operation of the Wisteria/BDEC-01 system with 33+PF at University of Tokyo. It is a Hierarchical, Hybrid, Heterogeneous (h3) system, which consists of computing nodes for CSE with A64FX and those for Data Analytics/AI with NVIDIA A100 GPU’s. We develop a software platform “h3-Open-BDEC” for integration of (S+D+L) and evaluate the effects of integration of (S+D+L) on the Wisteria system. The h3-Open-BDEC is designed for extracting the maximum performance of the supercomputers with minimum energy consumption focusing on (1) innovative method for numerical analysis with high-performance/high-reliability/power-saving based on the new principle of computing by adaptive precision, accuracy verification and automatic tuning, and (2) Hierarchical Data Driven Approach (hDDA) based on machine learning. Integration of (S+D+L) by h3-Open-BDEC enables significant reduction of computations and power consumption, compared to those by conventional simulations.
</EventDescription>
    <EventParentName>72849-SESS</EventParentName>
    <EventUniqueID>72849-116914</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Reproducibility and Transparency Practices in HPC</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS5</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tanu Malik</EventSpeakers>
    <EventSpeakerIDs>720350</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117798</EventHandoutURL>
    <EventDescription>Several systems research conferences now incorporate an artifact description and artifact evaluation (AD/AE) process as part of the paper submission. Authors of accepted papers optionally submit a plethora of artifacts: documentation, links, tools, code, data, and scripts for independent validation of the claims in their paper. An artifact evaluation committee (AEC) evaluates the artifacts and stamps papers with accepted artifacts, which then receive publisher badges. Does this AD/AE process serve authors and reviewers? Is it scalable for large conferences such as SCxy? Using the last three SCxy Reproducibility Initiatives as the basis, this talk will analyze the AD/AE process using a data-driven approach. We will distinguish studies that benefit from AD, i.e., increased transparency versus areas that benefit from AE. We will present a vision for the resulting curated, reusable research objects---how such research objects are a treasure in themselves for advancing computational reproducibility and making reproducible evaluation practical in the coming years.


</EventDescription>
    <EventParentName>73126-SESS</EventParentName>
    <EventUniqueID>73126-117798</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Lessons Learned During My 10 Year Quest to Enable Collaborative, Sustainable and Reproducible Ml Systems R&amp;D</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS5</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Grigori Fursin</EventSpeakers>
    <EventSpeakerIDs>803127</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118347</EventHandoutURL>
    <EventDescription>
In this talk, I will describe multiple problems that I have faced while trying to enable collaborative, sustainable and reproducible ML Systems research since 2010. I will also present the Collective Knowledge framework (CK) that I have developed to address some of these problems with the help of the community.

The CK concept is to organize all research projects as a database of reusable components, automation recipes and portable workflows with common APIs and extensible meta descriptions based on DevOps and FAIR principles.

Such approach helped researchers and practitioners to decompose complex ML Systems into reusable automation tasks and meta packages for ML models, data sets, AI frameworks, libraries and compilers. We could then assemble portable ML workflows to automatically build, test, benchmark and auto-tune diverse ML Systems across continuously changing software and hardware.
Furthermore, the community could collaboratively fix reproducibility issues in common CK workflows and components encountered during crowd-benchmarking of ML Systems thus enabling their sustainable and evolutionary development.

I will conclude with lessons learned when using the CK technology to automate and reproduce the MLPerf(tm) benchmark - a recent community effort to develop a common ML benchmark that provides consistent, reproducible, and fair measurements of accuracy, speed, and efficiency across diverse ML models, data sets, hardware, and software.
 </EventDescription>
    <EventParentName>73126-SESS</EventParentName>
    <EventUniqueID>73126-118347</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Leveraging Mixed Precision to Accelerate High-Order Finite Element Methods on Gpus</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS11</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Natalie Beams</EventSpeakers>
    <EventSpeakerIDs>796365</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118709</EventHandoutURL>
    <EventDescription>For high-order finite element methods, matrix-free approaches to applying the resulting operators achieve more efficient performance
than building a sparse matrix explicitly.  libCEED, a product of the Center for Efficient Exascale Discretizations (CEED),
offers a flexible, low-level specification for high-order matrix-free finite element methods. libCEED has several GPU backends targeting different architectures.  In the case of tensor-product basis functions, its kernels are memory bound, preventing full use of the hardware's computational peak.
When lower precision can be tolerated by the application (e.g. for preconditioning or when the computation is known to be stable
and full double-precision accuracy in the final result is not necessary), a mixed-precision framework offers a way to break through the double-precision
memory bandwidth barrier.  This can entail using lower precision in memory movement between kernels, and potentially also in computation.
Non-tensor bases involve more computationally-heavy matrix-matrix multiplications, which can also be accelerated through lower computational precision, sometimes in a hardware-specific way.  
This talk will discuss the efforts to provide low- and mixed-precision functionality to libCEED's GPU backends, along with performance implications for CEED benchmarks and applications.

</EventDescription>
    <EventParentName>73362-SESS</EventParentName>
    <EventUniqueID>73362-118709</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Asynchrony and Failure Masking via Pseudo-Local Process Recovery in MPI Stencil Applications</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS14</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hemanth Kolla</EventSpeakers>
    <EventSpeakerIDs>753207</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118744</EventHandoutURL>
    <EventDescription>Previous work has demonstrated improved scalability of resilience when only failed processes roll back to a checkpoint while other processes continue execution. This technique enables stencil solvers (with only neighbor communication) to exhibit ``failure masking' due to limited propagation of local recovery delays. However, localizing the impact of process recovery after a hard failure is challenging because such failures are not transparent to the MPI runtime. In this talk, we describe techniques to perform checkpointing and recovery ``pseudo-locally' (along with requisite message logging and replay) and maintain asynchronous progress in MPI applications by adapting the Fenix library, built on User Level Failure Mitigation (ULFM). For a prototype stencil solver with emulated hard failures, results show clear evidence of failure masking in this new setting, which should enable applications to withstand much higher failure rates than possible with global recovery. Our work shows how existing fault-tolerance infrastructure designed for global checkpoint/restart can be repurposed to enable greater efficiency in a resilience-aware application. [SNL is managed and operated by NTESS under DOE NNSA contract DE-NA0003525.]</EventDescription>
    <EventParentName>73369-SESS</EventParentName>
    <EventUniqueID>73369-118744</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>PETSc and Kokkos-Kernels Integration</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS15</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Junchao Zhang</EventSpeakers>
    <EventSpeakerIDs>783179</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118675</EventHandoutURL>
    <EventDescription>PETSc is a widely used math library for scientific applications modeled by partial differential equations. Traditionally, PETSc relies on MPI for portable and scalable computation between CPU processes and single-threaded C programming for portable and efficient CPU execution within a process. This strategy has served us very well until recently a variety of computer architectures,  represented by graphic processing units (GPUs), became the main compute power provider for scientific computations. In preparing PETSc for exascale computing, we met challenges of how to make PETSc still portable and efficient on new architectures, but also avoid code duplication. In this presentation, we report on the effort we use Kokkos and Kokkos-Kernels as a portability layer within PETSc. We'll talk about our design, lessons learned and will also show some application performance results.</EventDescription>
    <EventParentName>73357-SESS</EventParentName>
    <EventUniqueID>73357-118675</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Deep Learning in Adaptive Domain Decomposition Methods</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS16</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Axel Klawonn</EventSpeakers>
    <EventSpeakerIDs>720131</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118663</EventHandoutURL>
    <EventDescription>The convergence rate of domain decomposition methods is generally determined by the eigenvalues of the preconditioned system. For second-order elliptic partial differential equations, coefficient discontinuities with a large contrast can lead to a deterioration of the convergence rate. A remedy can be obtained by enhancing the coarse space with elements, which are often called constraints, that are computed by solving small eigenvalue problems on portions of the interface of the domain decomposition, i.e., edges in two dimensions or faces and edges in three dimensions. In general, it is difficult to predict where these constraints have to be computed, i.e., on which edges or faces. Here, a machine learning based strategy using neural networks is suggested to predict the geometric location of these edges or faces in a preprocessing step. This reduces the number of eigenvalue problems that have to be solved during the iteration. Numerical experiments for model problems and realistic microsections using regular decompositions as well as those from graph partitioners are provided, showing very promising results.</EventDescription>
    <EventParentName>73347-SESS</EventParentName>
    <EventUniqueID>73347-118663</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Machine-Learning-Based Search for Autotuning</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS17</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Prasanna Balaprakash</EventSpeakers>
    <EventSpeakerIDs>791468</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118617</EventHandoutURL>
    <EventDescription>Autotuning has the potential to significantly improve the performance portability of scientific applications on the diverse exascale hardware. Within the DOE exascale project, we have been developing YTOPT/SuRF, a search software package for autotuning. It comprises machine-learning-based search methods that consist of sampling a small number of input parameter configurations, evaluating them, and progressively fitting a surrogate model over the input-output space until exhausting the user-defined time or maximum number of evaluations. In this talk, we will present recent developments on sampling-based Bayesian optimization under algebraic constraints and Monte Carlo tree search for LLVM/Polly’s composable loop optimization transformations.   </EventDescription>
    <EventParentName>73329-SESS</EventParentName>
    <EventUniqueID>73329-118617</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Integration of Ai/ml Methods with the Task Scheduling in Parsec</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS17</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Qinglei Cao</EventSpeakers>
    <EventSpeakerIDs>790117</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118618</EventHandoutURL>
    <EventDescription xsi:nil="true" />
    <EventParentName>73329-SESS</EventParentName>
    <EventUniqueID>73329-118618</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Runtime System for Orchestrating Data Movement in Heterogeneous Memory and Storage Hierarchies</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS18</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Suren Byna</EventSpeakers>
    <EventSpeakerIDs>803729</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118592</EventHandoutURL>
    <EventDescription>With the emergence of heterogeneous computing elements, such as CPUs, GPUs, and special-purpose FPGAs, data movement among their memories in high performance computing (HPC) systems is complex. In addition, storage devices that are used for storing data for longer term than an application’s runtime are becoming heterogeneous, with new non-volatile and storage-class memories. Moreover, traditional parallel file systems are here to stay as capacity storage. Software that glue these various levels of memory and storage devices are diverse. For instance, various memory devices are managed by processors or operating systems, while storage devices are managed by multiple layers of I/O libraries and parallel file systems. 

In these complex HPC environments, runtime systems play a critical role in orchestrating data movement with the goal of achieving superior performance and keeping the data closer to processing. With this objective, we have been developing systems, such as Proactive Data Containers (PDC), and enhancing existing libraries, such as HDF5, with asynchronous data movement, caching, and prefetching techniques. In this talk, we will describe our efforts, success stories, lessons learned, and future plans of a robust data movement runtime system. </EventDescription>
    <EventParentName>72963-SESS</EventParentName>
    <EventUniqueID>72963-118592</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Scalable Algorithms for Learning Graph Representations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS19</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Nesreen Ahmed</EventSpeakers>
    <EventSpeakerIDs>785429</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118488</EventHandoutURL>
    <EventDescription>Graphs arise as a natural representation and modeling tool of complex systems across the sciences, such as social, biological, and technological systems, where vertices represent the components of the system, and edges represent their observed interactions. However, the growing data volumes and rates often require a significant boost in the performance of graph algorithms. In particular, graph representation learning is a very promising emerging research area with many industrial applications and it would benefit significantly from more scalable algorithms. In this talk, we will address the challenges of learning structured representations from massive graphs, and we will discuss scalable algorithms for learning structured features and training graph neural networks. 
</EventDescription>
    <EventParentName>73138-SESS</EventParentName>
    <EventUniqueID>73138-118488</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>High Resolution Two-phase Flow Simulations based on a Weakly Compressible Scheme with an Interface-Adapted AMR Method</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS20</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Shintaro Matsushita</EventSpeakers>
    <EventSpeakerIDs>803785</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118613</EventHandoutURL>
    <EventDescription>A fully explicit weakly compressible scheme for solving low Mach number gas-liquid twophase flows with interface-adapted AMR method is proposed to achieve large-scale and high-resolution simulation by avoiding solving pressure Poisson equation. Fully explicit time integration is developed by solving independent hyperbolic pressure evolution derived from isothermal Navier-Stokes equation instead of pressure Poisson equation which has a severe scalability problem. We have developed a GPU code of the tree-based AMR method, which can significantly reduce the computational cost to assign high-resolution mesh to the region of moving interfaces. Many high-resolution applications such as oil jets and flows with liquid films have been successfully calculated.</EventDescription>
    <EventParentName>73162-SESS</EventParentName>
    <EventUniqueID>73162-118613</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Distributed, In-Situ Spatial Inference in Climate Simulations with Sparse Gaussian Processes</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS22</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Michael Grosskopf</EventSpeakers>
    <EventSpeakerIDs>803307</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117840</EventHandoutURL>
    <EventDescription>As memory and storage cost for extreme-scale physics simulation becomes the greatest performance bottleneck, the ability to access full simulation data for post-hoc statistical analysis is becoming increasingly limited. The capability to perform in-situ statistical inference of state variables is critical for comprehensive utilization of information generated by these simulations. In this presentation, we report results fitting spatial extreme value models underpinned by scalable Gaussian process regression to the state information of an expensive simulation in-situ. The inference was performed using Julia coupled to the E3SM climate model and utilizes the parallel, distributed nature of the climate computation for scaling the inference. The demonstrated capability for in-situ climate analysis of the simulation unlocks new tools for UQ in extreme-scale scientific computing. </EventDescription>
    <EventParentName>73104-SESS</EventParentName>
    <EventUniqueID>73104-117840</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Uniform-H: Bridging the Gap Between H and H²</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS23</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ronald Kriemann</EventSpeakers>
    <EventSpeakerIDs>780186</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117643</EventHandoutURL>
    <EventDescription>While H²-matrices enable significant storage improvements compared to H-matrices due to shared and nested bases for the low-rank blocks, matrix arithmetic is much more complicated. This is mainly due to the only implicitly defined data of the bases and the potentially large number of blocks in the matrix affected by a single low-rank update.

In uniform H-matrices, only low-rank blocks in a single block row or column share a basis, i.e., without basis nestedness. This brings memory consumption close to H²-matrices. Furthermore, matrix arithmetic is much simpler as all data is explicitly stored and low-rank updates also affect only a small number of matrix blocks.

We will introduce the Uniform-H-matrix format together with a matrix arithmetic and show numerical examples for several different model problems to demonstrate the various properties of this H-matrix format.
</EventDescription>
    <EventParentName>73075-SESS</EventParentName>
    <EventUniqueID>73075-117643</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Preparing Trilinos Solvers for Exascale Wind Farm Simulations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS24</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jonathan Hu</EventSpeakers>
    <EventSpeakerIDs>712687</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117816</EventHandoutURL>
    <EventDescription>Scalable solvers such as multigrid are a key enabling technology for applications seeking to exploit exascale
supercomputers.  In this presentation we will discuss recent advances in multigrid preconditioners from the
Trilinos project in support of large scale simulations conducted by the ExaWind project (part of the
Exascale Computing Project).  We will focus on the adaptation of aggregation-based algebraic multigrid within Trilinos
to GPU architectures; discuss progress, ongoing work, and challenges; and present numerical results from the
Oak Ridge supercomputer Summit.






</EventDescription>
    <EventParentName>72942-SESS</EventParentName>
    <EventUniqueID>72942-117816</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Multigrid Methods for Block Operators on Structured Meshes</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS24</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Matthias Bolten</EventSpeakers>
    <EventSpeakerIDs>720359</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117819</EventHandoutURL>
    <EventDescription>Simulations involving PDEs in computational science and engineering are often carried out on structured meshes if the application permits to do so. The use of structured meshes is beneficial not only to define the grid hierarchy but also to efficiently implement the solvers on modern architectures. Further, the discretization on structured meshes leads to structured matrices that can be analyzed well. Multigrid methods for structured matrices stemming from the discretization of scalar problems have been analyzed in depth in the literature. Very little work exists for block matrices. Block matrices appear in different cases: In the simplest case they appear when scalar PDEs are discretized using higher order discretizations. Of larger interest is the case of systems of PDEs, where the unknowns represent different physical quantities, e.g., in elasticity, fluid flow simulations, and coupled multiphysics problems. Using the well-known variational theory for multigrid method, e.g. [McCormick and Ruge, SIAM J. Numer. Anal., 1982], as well as recent results from [Notay, Numer. Math., 2016] we are able to analyze multigrid methods for both cases, yielding optimal convergence, similar to the results for scalar problems [Aric\`o, Donatelli, Numer. Math., 2007]. These theoretical findings allow for efficient multigrid methods for structured meshes on modern parallel computers. In the talk the obtained results are presented and discussed.

</EventDescription>
    <EventParentName>72942-SESS</EventParentName>
    <EventUniqueID>72942-117819</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>WarpX Progress</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS25</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 12:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22 12:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Rémi Lehe</EventSpeakers>
    <EventSpeakerIDs>785706</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118958</EventHandoutURL>
    <EventDescription xsi:nil="true" />
    <EventParentName>73411-SESS</EventParentName>
    <EventUniqueID>73411-118958</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Scalable Statistical Learning from Dependent Data</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS27</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:20:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:40:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Michael Schweinberger</EventSpeakers>
    <EventSpeakerIDs>803155</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117761</EventHandoutURL>
    <EventDescription>Network, spatial, and temporal data are dependent, high-dimensional, and structured data, presenting challenges as well as opportunities for statistical learning. My talk will highlight some of the computational and statistical challenges and opportunities arising from dependent and structured data in high-dimensional settings.

</EventDescription>
    <EventParentName>73114-SESS</EventParentName>
    <EventUniqueID>73114-117761</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A space-filling curve for pyramids</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS28</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:45:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>David Knapp</EventSpeakers>
    <EventSpeakerIDs>790206</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118612</EventHandoutURL>
    <EventDescription>We present a new space-filling curve for pyramidal adaptive mesh refinement. Developments in the recent years have extended the scalable and efficient tree-based AMR methods from quadrilaterals/hexahedra to various element shapes such as triangles, tetrahedra or prisms. To support the full range of element shapes used in common unstructured meshes, additional support for pyramids is required.

To this end, we develop a new Morton type space-filling curve for this kind of element and integrate it into our AMR framework t8code. This development enables us to apply tree-based AMR to fully hybrid input meshes. Two special challenges for the SFC development and AMR library arise from the fact that one pyramid refines into 6 pyramids and 4 tetrahedra. The first one is not using a usual 2^n refinement pattern, making adaptations of the AMR software to allow for flexible new numbers of elements when refining necessary. The second one is that elements of one shape (pyramids) refine into elements of a different shape (tetrahedra).

We present general solutions for both challenges that are also applicable to non-pyramid elements and conclude with benchmarks on the JUWELS supercomputer using up to 50 billion elements on 50k MPI ranks.
</EventDescription>
    <EventParentName>73163-SESS</EventParentName>
    <EventUniqueID>73163-118612</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName> New Developments of the FROSch Domain Decomposition Solver Package</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS30</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:20:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:40:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Alexander Heinlein</EventSpeakers>
    <EventSpeakerIDs>803849</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118668</EventHandoutURL>
    <EventDescription>Schwarz methods are an algorithmic framework for a large class of domain decomposition methods. The software FROSch (Fast and Robust Overlapping Schwarz), which is part of the Trilinos package ShyLU, provides a highly scalable implementation of the Schwarz framework, and the resulting solvers are based on the construction and combination of the relevant Schwarz operators. FROSch currently focusses on Schwarz operators that are algebraic in the sense that they can be constructed from a fully assembled, parallel distributed matrix. This is facilitated by the use of extension-based coarse spaces, such as generalized Dryja-Smith-Wildund (GDSW) type coarse spaces.

This talk gives an overview of the FROSch software framework as well as current developments in improving the performance of the solvers, for instance, due to the use of inexact subdomain and coarse solvers. Moreover, recent parallel results for challenging applications, such as coupled multiphysics simulations of land ice in Greenland and Antarctica, are presented.</EventDescription>
    <EventParentName>73348-SESS</EventParentName>
    <EventUniqueID>73348-118668</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Leveraging Single Node Linear Algebra GPU Solvers from a Multi-Nodes User Code with Alien</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS31</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:15:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Cédric Chevalier</EventSpeakers>
    <EventSpeakerIDs>763842</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118678</EventHandoutURL>
    <EventDescription>Alien is a C++ framework that provides high-level APIs to solve linear problems for distributed-memory applications. Alien does not implement a linear solver but provides unified access to several libraries such as Hypre, PETSc, or Trilinos. However, Alien also extends the capabilities of smaller footprint libraries such as Ginkgo or ViennaCL. Alien manages data redistribution to use them from a distributed memory context, allowing a legacy MPI application to leverage modern GPU solvers.

We will describe how the different solvers using different low-level technologies are plugged in Alien. We will illustrate how Alien's unified API helps us efficiently exploit different computer hardware by selecting the most adapted solver library.


</EventDescription>
    <EventParentName>73358-SESS</EventParentName>
    <EventUniqueID>73358-118678</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Porting Hypre to Heterogeneous Computer Architectures: Strategies and Experiences</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS31</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:45:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ruipeng Li</EventSpeakers>
    <EventSpeakerIDs>770972</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118704</EventHandoutURL>
    <EventDescription>The high performance preconditioner library {\em hypre} provides a variety of interfaces and linear solvers, including various multigrid methods, that have achieved good scalability on a variety of homogeneous parallel computer architectures. Heterogeneous architectures with nodes that have both CPUs and accelerators provide new challenges, since they require more fine-grained parallelism and reduced data movement between different memories on a single node as well as across nodes. In this talk, we will discuss our experiences and strategies to port {\em hypre} to heterogeneous computers with accelerators, including the design of a new memory model, the use of abstractions, the BoxLoop macros in the structured and semi-structured interfaces, and the restructuring of algebraic multigrid (AMG) into modular components. We present numerical experiments comparing CPU and GPU performance for several test problems.
</EventDescription>
    <EventParentName>73358-SESS</EventParentName>
    <EventUniqueID>73358-118704</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Convergent Orthogonality Preserving Approximations of the Kohn-Sham Orbitals</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS32</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:45:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Xiaoying Dai</EventSpeakers>
    <EventSpeakerIDs>751672</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118687</EventHandoutURL>
    <EventDescription>To obtain convergent numerical approximations without orthogonalization operations is of great importance in electronic structure calculations. In this paper, we propose and analyze a class of iteration schemes for the discretized Kohn-Sham model, which preserves the orthogonality of the Kohn-Sham orbitals automatically. With our schemes, the iterative approximations are guaranteed to converge to the Kohn-Sham orbitals without any orthogonalization operations when the initial orbitals are orthogonal. We prove the convergence and get the local convergence rate of the numerical approximations. In addition, we present two approaches to get suitable time step sizes. This is a joint work with Liwei Zhang and Aihui Zhou.</EventDescription>
    <EventParentName>73360-SESS</EventParentName>
    <EventUniqueID>73360-118687</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Arkouda: Interactive Supercomputing for Data Analytics Made Possible by Chapel</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS34</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:45:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>2900</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Michael Merrill</EventSpeakers>
    <EventSpeakerIDs>803831</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118720</EventHandoutURL>
    <EventDescription>Exploratory data analysis (EDA) is a prerequisite for all data science, as illustrated by the ubiquity of Jupyter notebooks, the preferred interface for EDA among data scientists. The operations involved in exploring and transforming the data are often at least as computationally intensive as downstream applications (e.g. machine learning algorithms), and as datasets grow, so does the need for HPC-enabled EDA. However, the inherently interactive and open-ended nature of EDA does not mesh well with current HPC usage models. Meanwhile, several existing projects from outside the traditional HPC space attempt to combine interactivity and distributed computation using programming paradigms and tools from cloud computing, but none of these projects have come close to meeting our needs for high-performance EDA.

To fill this gap, we have developed a software package, called Arkouda, which allows a user to interactively issue massively parallel computations on distributed data using functions and syntax that mimic NumPy, the underlying computational library used in the vast majority of Python data science workflows. The computational heart of Arkouda is a Chapel interpreter that accepts a pre-defined set of commands from a client (currently implemented in Python) and uses Chapel's built-in machinery for multi-locale and multithreaded execution. Arkouda has benefited greatly from Chapel's distinctive features and has also helped guide the development of the language.</EventDescription>
    <EventParentName>73366-SESS</EventParentName>
    <EventUniqueID>73366-118720</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Search Space Reduction Through Analytical Modeling</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS8</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tze Meng Low</EventSpeakers>
    <EventSpeakerIDs>780272</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118605</EventHandoutURL>
    <EventDescription>Increasing complexity in computer architecture means that the space of high performance implementation is also increasing rapidly. This makes it impractical to search the space exhaustively for good implementations. In this talk, we discuss using analytical models to reduce the search space of possible implementations. Our analytical models capture knowledge made by experts in optimizing for different hardware features. This reduces the search space to parameters where models are too complicated, inaccurate, or not available; and auto-tuning or ML-based approaches may be applied. This can result in decreased search-time and potentially reduced size of the ML models.</EventDescription>
    <EventParentName>73328-SESS</EventParentName>
    <EventUniqueID>73328-118605</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Adaptive Nonlinear Preconditioning for PDEs with Error Bounds on Output Functionals</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS9</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>David Keyes</EventSpeakers>
    <EventSpeakerIDs>8363</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118655</EventHandoutURL>
    <EventDescription>Nonlinear preconditioning refers to transforming a nonlinear algebraic system to a form for which Newton-type algorithms have improved success through quicker advance to the domain of quadratic convergence. We place these methods, which go back at least as far as the Additive Schwarz Preconditioned Inexact Newton (ASPIN, 2002) in the context of a proliferation distinguished by being left- or right-sided, multiplicative or additive, and partitioned by subdomain, field type, or other criteria. We present the Nonlinear Elimination Preconditioned Inexact Newton (NEPIN, 2021), which is based on a heuristic “bad/good” heuristic splitting of equations and corresponding degrees of freedom. We augment basic forms of nonlinear preconditioning with two features of practical interest: an adaptive switchover to ordinary Newton as the domain of convergence is approached and error bounds on output functionals of the solution. Various nonlinearly stiff algebraic and model PDE problems are considered for insight and we illustrate performance advantage and scaling potential on challenging two-phase flows in porous media.  </EventDescription>
    <EventParentName>73346-SESS</EventParentName>
    <EventUniqueID>73346-118655</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Domain Decomposition Solvers for Nonlocal Equations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS9</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christian Glusa</EventSpeakers>
    <EventSpeakerIDs>768555</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118657</EventHandoutURL>
    <EventDescription>We present a domain decomposition method for the efficient simulation of nonlocal problems. Our approach is based on a multi-domain formulation of a nonlocal diffusion problem where the subdomains share ``nonlocal' interfaces of the size of the nonlocal horizon. This system of nonlocal equations is first rewritten in terms of minimization of a nonlocal energy, then discretized with a meshfree approximation and finally solved via a Lagrange multiplier approach in a way that resembles the finite element tearing and interconnect method. Specifically, we propose a distributed projected gradient algorithm for the solution of the Lagrange multiplier system, whose unknowns determine the nonlocal interface conditions between subdomains. Several two-dimensional numerical tests \new{on problems as large as 191 million unknowns}  illustrate the strong and weak scalability of our algorithm, which outperforms the standard approach to the distributed numerical solution of the problem. This work is the first rigorous numerical study in a two-dimensional multi-domain setting for nonlocal operators with finite horizon and, as such, it is a fundamental step towards increasing the use of nonlocal models in large scale simulations.</EventDescription>
    <EventParentName>73346-SESS</EventParentName>
    <EventUniqueID>73346-118657</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Overlapping Domain Decomposition Methods: New Algebraic Coarse Spaces</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS9</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Pierre Jolivet</EventSpeakers>
    <EventSpeakerIDs>768192</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118658</EventHandoutURL>
    <EventDescription>The design of robust multilevel preconditioners often relies on the choice of appropriate transfer operators. When it comes to overlapping domain decomposition methods, there is a wide range of choices, but the construction of these operators is often based on extra information not always available algebraically such as rigid body modes or per-element matrix assembly.
In this talk, I will present the construction of new algebraic coarse spaces for different linear systems, such as sparse least-squares problems, symmetric positive definite systems, or nonsymmetric problems.
Furthermore, the integration of this work in PETSc will be presented and showcased for different applications, using the proposed methodology as a black-box solver, with comparisons against other state-of-the-art solvers such as GAMG and BoomerAMG.
</EventDescription>
    <EventParentName>73346-SESS</EventParentName>
    <EventUniqueID>73346-118658</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Large-Scale Optimization with Structured Hessian Estimates</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS10</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Johannes Brust</EventSpeakers>
    <EventSpeakerIDs>797065</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118700</EventHandoutURL>
    <EventDescription>For certain large-scale optimization problems, in which the coupling of variables results in a sparse block arrowhead Hessian matrix, exploiting structure is important. Even with sparse blocks using back-substitution for the arrowhead pattern yields a dense intermediate matrix. In order to overcome forming a dense system we instead approximate it using the compact representation of a structured BFGS (S-BFGS) quasi-Newton matrix. Because the S-BFGS matrix is of advantageous structure (i.e., multiple of identity plus low-rank), the Sherman-Morrison-Woodbury inverse is used to solve with it. The proposed methods are described for stochastic optimization when decomposed into scenarios, yet structured approximations may be more widely applicable.
</EventDescription>
    <EventParentName>73351-SESS</EventParentName>
    <EventUniqueID>73351-118700</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Randomized Sketching for Low-Memory Dynamic Optimization</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS10</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Drew Kouri</EventSpeakers>
    <EventSpeakerIDs>751676</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118701</EventHandoutURL>
    <EventDescription>In this talk, we develop a novel limited-memory method to solve dynamic optimization problems. The memory requirements for such problems often present a major obstacle, particularly for problems with PDE constraints such as optimal flow control, full waveform inversion, and optical tomography. In these problems, PDE constraints uniquely determine the state of a physical system for a given control. While the control is often low dimensional, the state is typically more expensive to store.  To reduce the memory requirements, we employ randomized matrix approximation to compress the state as it is generated.  The compressed state is then used to compute approximate gradients and to apply the Hessian to vectors. The approximation error in these quantities is controlled by the target rank of the compressed state. This approximate first- and second-order information can readily be used in any optimization algorithm. As an example, we develop a sketched trust-region method that adaptively learns the target rank using a posteriori error information and provably converges to a stationary point of the original problem.  To conclude, we apply our randomized compression to the optimal control of a linear elliptic PDE and the optimal control of fluid flow past a cylinder.
</EventDescription>
    <EventParentName>73351-SESS</EventParentName>
    <EventUniqueID>73351-118701</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Sketching with Efficient Tensor Network Embeddings</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS10</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Linjian Ma</EventSpeakers>
    <EventSpeakerIDs>790109</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118703</EventHandoutURL>
    <EventDescription>Tensor network structured data is commonly seen in multiple applications, including quantum simulation, statistical learning and data mining. We introduce tensor network embeddings, which are efficient for the dimensionality reduction of tensor network structured data. We show that the binary tree structured embedding is efficient in terms of both the output sketch size and the leading order computational cost to multiply the embedding with the data. Moreover, we provide algorithms to construct computationally efficient and accurate binary tree embeddings for given tensor network structured data.</EventDescription>
    <EventParentName>73351-SESS</EventParentName>
    <EventUniqueID>73351-118703</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>On Portability and Performance Versatility in Nonlinear Solid and Fluid Mechanics Using libCEED and PETSc</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS11</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Leila Ghaffari</EventSpeakers>
    <EventSpeakerIDs>795393</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118710</EventHandoutURL>
    <EventDescription>libCEED is an open-source mathematical software library that is layered in abstractions that handle complexities like multigrid solvers and nonlinearity in real-world simulations without loosing performance on different computational device types, including CPUs and GPUs. In this talk, we explore and compare the performance spectrum of some physical scenarios on CPU and GPU machines, as problem size, polynomial degree, machine scale, and physical complexity vary. In doing so, we have used libCEED and PETSc to develop examples in computational fluid dynamics (CFD) and solid mechanics, with which we perform trade-off studies between accuracy, cost, and execution time.
</EventDescription>
    <EventParentName>73362-SESS</EventParentName>
    <EventUniqueID>73362-118710</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Exascale Particle-in-Cell Simulations with PIConGPU</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS12</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Michael Bussmann</EventSpeakers>
    <EventSpeakerIDs>790276</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118951</EventHandoutURL>
    <EventDescription>Exascale Particle-in-Cell simulations are around the corner. We present the computing challenges one faces when dealing with next generation architectures and our solutions to these challenges. We focus on our own 3D3V electromagnetic particle-in-cell code PIConGPU to highlight the major steps necessary to scale to Exascale. We then present the opportunities that open up when going full Exascale, showcase central applications from laser plasma accelerators to high energy density physics that require Exascale capabilities and finally address the big elephant in the room: What if you can make optimum use of Exascale systems and produce high quality data with it?





</EventDescription>
    <EventParentName>73410-SESS</EventParentName>
    <EventUniqueID>73410-118951</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A New Performance-Complexity Landscape: JIT-based Fusion for Matrix-Free Multigrid Solvers, with a Case Study in Bonded Granular Media</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS13</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jed Brown</EventSpeakers>
    <EventSpeakerIDs>796171</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118706</EventHandoutURL>
    <EventDescription>Matrix-free multigrid solvers are an increasingly necessary to efficiently use modern hardware for science and engineering. Just-in-Time (JIT) compilation with kernel fusion makes it possible to separate concerns while making debuggable high-performance code, but this fusion may only be beneficial when the fused kernels can be scheduled with acceptable occupancy. We'll investigate abstraction leaks in convergence and performance tuning of matrix-free multigrid solvers as the problem specification and complexity of material models changes, all via a case study in bonded granular media.</EventDescription>
    <EventParentName>73363-SESS</EventParentName>
    <EventUniqueID>73363-118706</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Online Fault Tolerance with Kokkos Resilient Execution Spaces</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS14</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Elisabeth Giem</EventSpeakers>
    <EventSpeakerIDs>803858</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118745</EventHandoutURL>
    <EventDescription>Hardware heterogeneity in HPC is increasing as we approach the dawn of the exascale era, with performance portable APIs such as Kokkos and RAJA rising to meet the challenge. Accompanying the increase in heterogeneity is an increase in transient faults and soft errors, and industry leaders have identified software resiliency as a critical research need. We propose a framework to tolerate soft errors during execution based on a primary Kokkos abstraction: the resilient execution space. The resilient execution space allows applications already using Kokkos to take advantage of our near-seamless API integration to obtain a user-specified level of resiliency in parallel execution. Soft error detection and mitigation is handled by majority vote in resilient versions of Kokkos’ parallel execution abstractions. We show performance results on various benchmarks written for Kokkos and modified to use the resilient execution space.
</EventDescription>
    <EventParentName>73369-SESS</EventParentName>
    <EventUniqueID>73369-118745</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Performance Portable Batched Sparse Linear Solvers in Kokkos Kernels</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS15</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kim Liegeois</EventSpeakers>
    <EventSpeakerIDs>780888</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118676</EventHandoutURL>
    <EventDescription>Kokkos Kernels is a library within the Kokkos ecosystem and focuses on the implementation of portable dense and sparse linear algebra algorithms. The library is typically used by higher level math libraries that provide distributed linear algebra algorithms or directly by applications. The later has revealed the need for batched interfaces of linear algebra algorithms. These are then typically used to perform many local operations on a large number of small linear algebra operators. Such use cases arise in computational mechanics to solve problems at integration points, in particle dynamics to compute collisions or when chemical reactions are occurring in fluid flows.
In this presentation we discuss the implementation and the performance gains of batched Krylov-based solvers using hierarchical parallelism in Kokkos-Kernels. We explain how the extra dimension of batched linear systems can be used to improve the throughput of the computation and demonstrate performance on multiple architectures.
</EventDescription>
    <EventParentName>73357-SESS</EventParentName>
    <EventUniqueID>73357-118676</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Some Domain Decomposition Algorithms in Machine Learning</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS16</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Xiao-Chuan Cai</EventSpeakers>
    <EventSpeakerIDs>803784</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118660</EventHandoutURL>
    <EventDescription>Domain decomposition (DD) is a general methodology for solving problems involving a large number of variables that are globally coupled. It decomposes a global problem into a set of overlapping or non-overlapping subproblems that are easier to solve than the original problem, and sometime these subproblems can be solved in parallel since they are often independent of each other. A coupling algorithm, which is often an iterative process, is then employed to form the solution of the global problem from the solutions of the subproblems. The methodology has been applied successfully for many problems described by partial differential equations. In this talk, we discuss some preliminary studies of some domain decomposed principal component analysis (PCA) and linear discriminant analysis (LDA) approaches and their applications in unsupervised and supervised learning. This is a joint work with Jingwei Li.</EventDescription>
    <EventParentName>73347-SESS</EventParentName>
    <EventUniqueID>73347-118660</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Graph Neural Network based Framework for identifying Candidate nodes of Influence Maximization Problem</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS19</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sai Munikoti</EventSpeakers>
    <EventSpeakerIDs>803305</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117835</EventHandoutURL>
    <EventDescription>A lot of Machine learning algorithms have been developed for graphs in recent times which demonstrate high efficacy in solving various graph theory problems. Especially, the graph combinatorial problems have been targeted because of the high computational complexity and scalability issues of the existing algorithms. Influence maximization (IM) is one such combinatorial problem which is typically solved with the greedy Hill-climbing algorithm (GHC). GHC and in that matter any IM algorithm iteratively search over a large pool of nodes for identifying infleuncial seed nodes. The search space increases drastically with the size of the network leading to a significant rise in the algorithm running time. In a typical graph, only a certain fraction of nodes are likely to contribute to the IM solution set and, the rest of the nodes are nonessential. 
In this talk, we will present our ongoing work on identifying such essential nodes known as candidate nodes. Specifically, we have framed this identification problem as a binary node classification problem where node classes are "candidate" and "non-candidate". A Graph neural network model is trained to predict node classes and thereafter, only candidate nodes are searched to identify Influencial seed nodes. We will demonstrate the substantial reduction of algorithm running time without compromising the quality of seed nodes. 


</EventDescription>
    <EventParentName>73138-SESS</EventParentName>
    <EventUniqueID>73138-117835</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Fusanov - a Task-Based, Fused Rusanov Solver over Dynamically Adaptive Meshes</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS20</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Holger Schulz</EventSpeakers>
    <EventSpeakerIDs>793413</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117917</EventHandoutURL>
    <EventDescription>Dynamic adaptive mesh refinement is not a direct fit to accelerators (GPUs) which are optimised streaming architectures, i.e. perform the better the more uniform and regular all operations. Block-structured AMR accommodates this fact by embedding regular patches into the AMR (meta-)mesh yet forces us to compromise between mesh granularity and pre-patch efficiency. We propose an alternative strategy which combines the advantages of large patches with totally dynamic, fine-granular AMR: Enclave tasking identifies cells (aka tasks) within the compute mesh that can be treated with low priorities. Our proposal is to collect these cells into a batches of (uniform) tasks at runtime, and then to schedule them in one go as one compute kernel on a GPU. Such a batch allows us to write compute kernels which are optimised over multiple patches (which can be spatially disconnected). We use this approach to deliver a fused Rusanov solver over multiple cells: a Fusanov solver.</EventDescription>
    <EventParentName>73162-SESS</EventParentName>
    <EventUniqueID>73162-117917</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>UX-Focused Reproducible Analysis Toolkit for Mars 2020</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS21</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Chase Million</EventSpeakers>
    <EventSpeakerIDs>803119</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117790</EventHandoutURL>
    <EventDescription>The Mars 2020 Mast Camera Zoom (Mastcam-Z) is a stereoscopic multispectral camera pair mounted on the Perseverance rover. A Mastcam-Z multispectral observation generally consists of 12 stereo-paired 4-megapixel images taken through standard RGB bayer and narrowband filters optimized for the Martian surface. These data are both dense and urgent. They contain a large amount of high-dimensional information that must be analyzed in hours to days to inform tactical decisions, like where to drive the rover or what to target for sample collection.

We developed a suite of tools to improve speed and consistency of last-mile reduction of these data. The tools automate many best practices and provide a minimal interface to enable others. Scientists who are not software engineers can interrogate and modify the tools: their operations are defined in markup files that can be edited without reading the entire source. They are fast, easy to use, and create many distinct, fully-reproducible views on the data in formats suitable for both archival and interchange (including at web endpoints). By providing both variety and consistency, they reduce cognitive overhead and facilitate collaboration.

We will outline our approaches to project planning, user research, and implementation, and discuss how they could be used to produce software that is durable, reproducible, and responsive to research needs -- even in domains less alien than Martian robotics. 


</EventDescription>
    <EventParentName>73127-SESS</EventParentName>
    <EventUniqueID>73127-117790</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Productive Performance Portability: Building in Rust with PETSc and libCEED</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS21</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jeremy Thompson</EventSpeakers>
    <EventSpeakerIDs>783663</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117795</EventHandoutURL>
    <EventDescription>C/C++ and Fortran are mainstays of high performance computing, but modern languages such as Julia and Rust are gaining popularity for HPC applications. PETSc, the Portable, Extensible Toolkit for Scientific Computation, is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. libCEED provides fast algebra for element-based discretizations, designed for performance portability, run-time flexibility, and clean embedding in higher level libraries and applications. In this talk, we introduce the PETSc and libCEED Rust interfaces as well as a mini-application based upon these interfaces. Via this application, we will demonstrate the productivity benefits when compared to our C based mini-applications, particularly with respect to debugging, documentation, and testing.

</EventDescription>
    <EventParentName>73127-SESS</EventParentName>
    <EventUniqueID>73127-117795</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Korali: Efficient and Scalable Software Framework for Bayesian Uncertainty Quantification and Stochastic Optimization</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS22</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sergio Martin</EventSpeakers>
    <EventSpeakerIDs>803797</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117841</EventHandoutURL>
    <EventDescription>We present Korali, an open-source framework for large-scale Bayesian uncertainty quantification and stochastic optimization. The framework relies on non-intrusive sampling of computational models and enables their exploitation for optimization and decision-making. In addition, its distributed sampling engine makes efficient use of massively-parallel architectures while introducing novel fault tolerance and load balancing mechanisms. We demonstrate these features by showing experimental results where we interface Korali with existing high-performance CPU and GPU-based simulation libraries. These results showcase a highly efficient scaling on up to 512 nodes of the CSCS Piz Daint supercomputer. Finally, we present benchmarks demonstrating that Korali outperforms related state-of-the-art software frameworks. 
</EventDescription>
    <EventParentName>73104-SESS</EventParentName>
    <EventUniqueID>73104-117841</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Calibration of a Computationally Intensive Model with Parallel Computing Aspects</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS22</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ozge Surer</EventSpeakers>
    <EventSpeakerIDs>799823</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117881</EventHandoutURL>
    <EventDescription>Calibration of a computer model widely requires multiple parallel runs of computationally challenging simulations of the same model with different parameter settings. The coordination of parallel runs is important to accelerate the calibration process and efficiently utilize from the computational resources. In case of failures, an early detection mechanism is required to leave resources idle due to the imbalance in the run times of the parallel tasks. In other situations, irregular sending/receiving of data between manager and workers is necessary to sequentially update the surrogate and calibration model. In this talk, we provide algorithmic insights on the coordination of parallel runs using surmise, a Python package that is designed to provide a surrogate model interface for calibration, uncertainty quantification, and sensitivity analysis, and illustrate these by calibrating computationally expensive low-energy physics code with surrogate-assisted methods under surmise.</EventDescription>
    <EventParentName>73104-SESS</EventParentName>
    <EventUniqueID>73104-117881</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Estimation of Galaxy Redshift with Probabilistic Neural Networks</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS22</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Nesar Ramachandra</EventSpeakers>
    <EventSpeakerIDs>796515</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118578</EventHandoutURL>
    <EventDescription>Astrophysical data analyses like estimations of galactic distances often rely on representative data from telescope observations. Data-driven methods of this type are subject to a number of potential deficiencies, such as sample incompleteness and noise. In addition, redshift estimations often struggle with scalability to millions of galaxies and robust uncertainty quantifications. Motivated by these considerations, we propose an approach for augmenting data-based protocols for training machine learning models by using physically motivated synthetic data. A suitable statistical model can then be trained with a matched distribution and realistically modeled synthetic data in hand. A neural network for conditional density estimation was utilized for this purpose, demonstrating a superior accuracy over traditional redshift estimation techniques and interpretable error estimations. With this example, we will discuss some of the crucial aspects of scientific machine learning, with an emphasis on adaptability to high performance computing systems and importance of uncertainty quantification.</EventDescription>
    <EventParentName>73104-SESS</EventParentName>
    <EventUniqueID>73104-118578</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Performance Evaluation of Iterative Refinement for Symmetric Eigenvalue Problem on Supercomputer</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS35</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:45:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yuki Uchino</EventSpeakers>
    <EventSpeakerIDs>802778</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118621</EventHandoutURL>
    <EventDescription>This study considers a refinement algorithm for the eigenvalue decomposition of a real symmetric matrix. Various numerical algorithms have been developed for standard eigenvalue problems. The accuracy of approximate eigenvectors obtained using a numerical algorithm decreases with the gap between the associated eigenvalue and its closest one. Therefore, using iterative refinement methods is useful for obtaining the eigenvalue decomposition with high accuracy. Recently, Ogita and Aishima proposed an efficient refinement algorithm for improving approximate eigenpairs obtained using a numerical algorithm. Since that algorithm is based on Newton's method, that converges quadratically if an initial guess is moderately accurate. The refinement algorithm is constructed via four high-accuracy matrix multiplications. Thus, that may yield high-accuracy results faster than those obtained using multiple-precision arithmetic. In this study, we implemented that refinement algorithm on a supercomputer using high-accuracy matrix multiplications proposed by Ozaki et al. and Double-Double arithmetic. We improved the accuracy of approximate eigenpairs provided by PDSYEVD from ScaLAPACK. Numerical results are presented showing the efficiency of the refinement algorithm implemented on a supercomputer.</EventDescription>
    <EventParentName>73330-SESS</EventParentName>
    <EventUniqueID>73330-118621</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Characterizing Failures in HPC using Benford’s Law</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS38</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kurt Ferreira</EventSpeakers>
    <EventSpeakerIDs>786170</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118750</EventHandoutURL>
    <EventDescription>To better understand the potential impacts of failures on next-generation systems, significant effort has been devoted to collecting, characterizing and analyzing failures on current systems. These studies typically require large volumes of data and complex analysis in an attempt to identify statistical properties within the data to exploit.  In this talk, we examine failures on a number of systems and demonstrate that for many systems, the time between failures obeys Benford’s law. This law applies to a number of naturally occurring collections of numbers and states that the leading digit is more likely to be small, for example a leading digit of 1 is more likely than 9.  Additionally, we outline how we might utilize this property in the failure data to lower mitigation overheads.
</EventDescription>
    <EventParentName>73371-SESS</EventParentName>
    <EventUniqueID>73371-118750</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Family of MAC-Like Discretizations for Hierarchically Refined Meshes</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS39</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  3:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tobin Isaac</EventSpeakers>
    <EventSpeakerIDs>752526</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118609</EventHandoutURL>
    <EventDescription>The Marker-and-Cell (MAC) discretization for Navier-Stokes equations is famously efficient and super-convergent for uniform grids.  Practitioners of finite difference, finite volume, discontinuous Galerkin, and mixed-finite element methods all have ways of deriving the MAC scheme within their favored frameworks.  But while these derivations coincide on a uniform grid, they diverge into different methods in the presence of the coarse-fine boundaries that occur in adaptively refined meshes, each with their own merits.

Inspired by local codifferential discretizations in other work (which have appeared in the multipoint flux mixed finite element method of Wheeler &amp; Yotov and the more recent augumented serendipity elements of Lee &amp; Winther), we show how the MAC scheme can be framed as a mixed discretization of the vector Lapacian in which the codifferential is locally eliminated.  From this starting point, we can generalize to AMR meshes: quadtrees and octrees, but beyond that to any hierarchically refined mesh.  The generalization retains the formal order of accuracy and sparsity of the original and respects the Helmholtz decomposition.  We will also discuss the finite difference stencils generated by this approach, and the possibilities of combining this approach with higher order methods.</EventDescription>
    <EventParentName>73164-SESS</EventParentName>
    <EventUniqueID>73164-118609</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>AMR at Scale for Radiation and Combustion in Uintah</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS39</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:45:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Martin Berzins</EventSpeakers>
    <EventSpeakerIDs>712891</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118610</EventHandoutURL>
    <EventDescription>AMR has been used in the Uintah code in many different physically
applications including phase field models and combustion.
Its use in combustion has been in conjunction with Ray tracing
radiation models so far.
In this talk the scalability of the AMR radiation approach is shown on a
variety of systems such as Fronterra Summit and Lassen.
We hope to include full scale summit results and to compare GPU and CPU
performance.

At the same time the Uintah combustion users have been reluctant to use
AMR for their standard combustion solver. We will explore the algorithmic
issues they have raised and discussed how they may be resolved.

Finally we will describe a Uintah combustion benchmark for exascale and
discuss what may be the challenges an getting it to run on Aurora and
Frontier. </EventDescription>
    <EventParentName>73164-SESS</EventParentName>
    <EventUniqueID>73164-118610</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>HP-Adaptive Finite Element Methods for Large-Scale Parallel Computations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS39</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:35:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Wolfgang Bangerth</EventSpeakers>
    <EventSpeakerIDs>804302</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119069</EventHandoutURL>
    <EventDescription>hp-finite element methods -- where both the mesh size and the polynomial degree used on every cell are chosen adaptively -- have been known to have excellent convergence properties since the 1980s. Yet, they are not widely used, possibly because the large finite element libraries do not provide implementations. We have undertaken the task of providing comprehensive support for hp methods for parallel computations in the deal.II library where this support has already been available for sequential computations since around 2007. In this talk, we will discuss what algorithmic infrastructure is necessary for efficient, large-scale parallel support for hp-FEM. We will then illustrate our approach with performance and scalability data for all parts of realistic computations on up to many thousands of processors. </EventDescription>
    <EventParentName>73164-SESS</EventParentName>
    <EventUniqueID>73164-119069</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Scaling Shrinkage Estimates of Large Covariance and Correlation Matrices</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS40</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:45:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Alina Peluso</EventSpeakers>
    <EventSpeakerIDs>796107</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117765</EventHandoutURL>
    <EventDescription>Many real-world data applications require the estimation of correlation/covariance matrices, and when dealing with large datasets, empirical correlation/covariance matrices may be ill-conditioned. However, shrinkage-estimated covariance matrices are always positive definite, well conditioned (so that the inverse always exists), and show small mean squared error. Nevertheless, the latter requires access to the full data matrix and when both $n$ and $p$ are large this can be problematic. In particular, forming the covariance matrix may require enormous amounts of memory, far exceeding the capacity of a typical user’s workstation. Operating out of core is one possible solution, but because disk I/O speeds are slow, this is an unappealing solution for large datasets. Therefore, by exploiting in-house high-performance computing (HPC) capabilities we scaled our implementation of a shrinkage approach for the efficient estimation of correlation/covariance matrices. We apply our correlation/covariance matrix shrinkage approach to the uncertainty quantification for DL models by estimating an “a priori” abstaining classifier for multitask convolutional neural networks (MTCNN).</EventDescription>
    <EventParentName>73115-SESS</EventParentName>
    <EventUniqueID>73115-117765</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Fine Tuning the Grouping Approach to Parallelization of Statistics/Machine Learning Methods</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS40</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Norman Matloff</EventSpeakers>
    <EventSpeakerIDs>728972</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118010</EventHandoutURL>
    <EventDescription>A common approach to parallelization in data science is to randomly group the rows of the data, apply a serial algorithm to each group, then apply averaging or voting to the groups.  This approach can be highly effective, even producing superlinear speedup (Matloff, JSS, 2016). However, it raises an issue regarding tuning parameters.  Since the latter depend on the number of data points, the best parameter values for the grouped approach may differ substantially from the best set for the ungrouped data. A case study will involve parallelizing the FOCI method of variable selection.
</EventDescription>
    <EventParentName>73115-SESS</EventParentName>
    <EventUniqueID>73115-118010</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Supercomputing-Driven Climatic Clustering and Longitudinal Analysis with Impacts on Food, Bioenergy, and Pandemics</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS40</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:35:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daniel Jacobson</EventSpeakers>
    <EventSpeakerIDs>795195</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118705</EventHandoutURL>
    <EventDescription>Predicted growth in world population will put unparalleled stress on the need for sustainable energy and global food production, as well as increase the likelihood of future pandemics. In this work, we identify high-resolution environmental zones in the context of a changing climate to predict longitudinal processes relevant to these challenges. We develop novel climatic similarity methods to find high-order relationships between climate zones at high geospatial resolution. We run these methods at massive scale using two of the world's top computing systems. We also perform unbiased, data-driven climatic analyses at unprecedented scale and accuracy compared to previous approaches. Our application runs for this work result in one of the largest matrix product calculations ever performed (168.7 x 10^21 operations), as well as the highest operation rate within an application ever reported (9.37 x 10^18 operations/sec). The methods discussed herein are also applicable to correlation analyses in other diverse fields such as systems biology, ecology, materials science, carbon cycles, biogeochemistry, additive manufacturing, and zoonosis research.
</EventDescription>
    <EventParentName>73115-SESS</EventParentName>
    <EventUniqueID>73115-118705</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Proximal Algorithms for Algebraic Optimization</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS42</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:45:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1200</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daniel Bienstock</EventSpeakers>
    <EventSpeakerIDs>704448</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117441</EventHandoutURL>
    <EventDescription>Algebraic optimization problems are optimization problems where the constraints and the objective function are algebraic (i.e., polynomial) inequalities.

In this talk, we are interested in numerically fast algorithms for computing \textit{good} solutions for such problems, where by 'good' we mean solutions with small infeasibility and also objective value that is small compared to the optimum.  In this category, the best algorithms are the log-barrier methods; excellent implementations are given in Knitro and IPOPT ("merit function" and "filter", respectively).

We consider alternative methods that are based on prox-function methods and which emulate the so-called 'pump-feasibility' heuristics in the linear mixed-integer programming literature.  We are interested, primarily, in large-scale cases rather than low-dimensional.

Joint work with Chen Chen (Ohio State) and G. Munoz (U. O'Higgins, Chile).</EventDescription>
    <EventParentName>73019-SESS</EventParentName>
    <EventUniqueID>73019-117441</EventUniqueID>
    <STATUS>inactive</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Reducing Data Movement in Mixed Precision LU Factorization with GPU Tensor Cores</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS45</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:35:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:55:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Theo Mary</EventSpeakers>
    <EventSpeakerIDs>794954</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116811</EventHandoutURL>
    <EventDescription>Mixed precision accelerators such as GPU tensor cores can significantly improve both the performance and accuracy of the solution of linear systems, in particular in the context of iterative refinement based on mixed precision LU factorization. The simplest and safest implementation of LU factorization with tensor cores is to store the matrix in simple, rather than half, precision. However, this approach is expensive in terms of storage and data movement. We will present an new LU factorization algorithm that works on a matrix stored almost entirely in half precision, thereby halving storage and data movement, and achieving a speedup of nearly a factor two. A careful rounding error analysis allows us to do this without endangering the numerical stability of the factorization and thus preserving the fast convergence of iterative refinement.


</EventDescription>
    <EventParentName>72752-SESS</EventParentName>
    <EventUniqueID>72752-116811</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Online and Offline Precision Tuning in Hardware Accelerators</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS45</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:30:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>George Constantinides</EventSpeakers>
    <EventSpeakerIDs>741562</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116968</EventHandoutURL>
    <EventDescription>In recent years, there has been considerable interest in application-specific or domain-specific hardware accelerators for computational kernels. The ability to define the hardware architecture to suit the numerical application at hand gives rise to considerable potential sources of energy and cost efficiency, including through the development of custom and mixed-precision arithmetics.

In this talk, I will summarise two of our recent research contributions in this space to highlight the possibilities. In one approach, we have developed a hardware accelerator using redundant number representations that customises the precision of its computation on-the-fly, computing in an 'online' manner; I will present some analysis of the convergence of such computations. In another approach, we have utilised recent developments in satisfiability modulo theories to estimate the probabilistic impact of low precision computation on small computational kernels suitable for a hardware accelerator.

I hope to end the talk with a summary of some of the key areas where I believe the NA community and the hardware accelerator community should prioritise working together over the next few years. </EventDescription>
    <EventParentName>72752-SESS</EventParentName>
    <EventUniqueID>72752-116968</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Distributed-Memory Optimization Solver for Two-Stage Stochastic Programming Problems</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS47</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1200</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jingyi Wang</EventSpeakers>
    <EventSpeakerIDs>802987</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117450</EventHandoutURL>
    <EventDescription>We present a scalable optimization algorithm and its parallel implementation for large-scale two-stage stochastic programming problems. The application includes the security constrained optimal power flow (SCACOPF) models routinely used in electrical power grid operations. Such problems are often nonsmooth, nonconvex in two-stage formulation and can be prohibitively expensive to solve on an industrial scale with the traditional methods. 
To overcome these challenges, the distributed algorithm proposed in this work solves the first-stage master problem with local quadratic approximation of the second-stage optimization solution. It is shown that under reasonable assumptions and smoothing, global convergence can be achieved. The second-stage subproblems, which require highly varying execution times, are scheduled asynchronously and solved in parallel. This is crucial in achieving good load balancing and parallel efficiency. The algorithm draws similarity to bundle methods, in avoiding line search to reduce the computation time. The step size is instead controlled by the quadratic approximation coefficient. The asynchronous scheduling is implemented in C++ via MPI non-blocking calls to overlap computations with communication. 
The convergence and scaling results are provided through numerical experiments, in synthetic and SCACOPF problems, conducted on Summit and Lassen  supercomputers. We observe quick converging behavior and high parallel efficiency.</EventDescription>
    <EventParentName>73020-SESS</EventParentName>
    <EventUniqueID>73020-117450</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Hierarchical Interior-Point Method for Doubly-Bordered Block-Diagonal LPs from Energy System Modeling</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS47</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1200</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Charlie Vanaret</EventSpeakers>
    <EventSpeakerIDs>783325</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117451</EventHandoutURL>
    <EventDescription>The current shift of energy systems in several European countries towards sustainable, renewable technologies comes with a massive decentralization and a concomitant increase in the scale of realistic energy models. The resulting large-scale linear programs (LPs) often contain several hundred million constraints and variables. These models can take prohibitively long to solve for state-of-the-art commercial solvers and even exceed the capacities of a single desktop machine. Against this backdrop, the research project UNSEEN has been launched to develop methods for solving currently intractable energy optimization problems that exhibit a block-diagonal structure with linking constraints and linking variables.
Our solver PIPS-IPM++ implements a parallel interior-point method (IPM) and exploits the problem structure to parallelize the linear algebra kernel. However, PIPS-IPM++'s performance crucially correlates with the number of linking constraints and variables present in the model ; linking parts form a sequential bottleneck in our algorithm.
We present the hierarchical approach, a nested Schur complement strategy that exploits the constraints between two consecutive blocks. We demonstrate how PIPS-IPM++ scales with the dimension on several real-world energy system problems, solved with the supercomputer at JSC (Juelich) and compared against commercial solvers.
</EventDescription>
    <EventParentName>73020-SESS</EventParentName>
    <EventUniqueID>73020-117451</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Development of Unstructured Mesh Pic Applications for Performant Execution on Gpu Accelerated Supercomputers</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS48</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mark Shephard</EventSpeakers>
    <EventSpeakerIDs>702311</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118449</EventHandoutURL>
    <EventDescription>Multiscale simulations using the Particle-in-Cell (PIC) method couple particle motion to fields described by PDEs discretized over meshes of the domain.  Such methods are commonly used for plasma physics simulations in fusion devices (ITER, DIII-D, etc.).  Given the complexity of the geometric models of these devices, unstructured mesh discretizations are key to efficiently capturing the model feature geometry, often using anisotropic mesh gradations, while controlling mesh quality and overall size.  Efficiently developing and running these simulations on leadership class GPU accelerated systems is a challenge that requires a flexible infrastructure that can be tuned to different system architectures by computational scientists and easy to use, stable, interfaces that application developers use to implement physics methods.  PUMIPic, the parallel unstructured mesh infrastructure for PIC, is a C++, Kokkos based, library that meets these needs.  An overview of PUMIPic and its methods will be presented along with progress on developing and verifying the edge plasma physics application, XGCm, and the impurity transport application, GITRm.</EventDescription>
    <EventParentName>72873-SESS</EventParentName>
    <EventUniqueID>72873-118449</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Recent Advances in Shared-Memory (Hyper)Graph Partitioning</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS49</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:35:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:55:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tobias Heuer</EventSpeakers>
    <EventSpeakerIDs>797676</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117701</EventHandoutURL>
    <EventDescription>(Hyper)graph partitioning is an important preprocessing step for optimizing data placement and minimizing communication volume in many high-performance computing applications. It asks for a partition of the vertex set of a (hyper)graph into $k$ blocks of bounded size while minimizing an objective function defined on the (hyper)edges that connects two or more blocks. During the last decades, sequential high-quality partitioners have evolved into sophisticated frameworks, while existing parallel approaches use comparatively weak components that are easier to parallelize.  However, to cope with ever growing problem sizes and modern architectures, it has become increasingly important to develop efficient parallel partitioning algorithms whose solution quality is competitive with existing sequential systems.  In recent years, research has begun to address this issue by parallelizing the core techniques of their sequential high-quality counterparts without compromising solution quality. In this talk, we will cover recent developments in shared-memory (hyper)graph partitioning and show that this class of new systems achieves good speedups on (hyper)graphs with more than $100$ million nodes and edges and significantly outperform previously developed parallel partitioners in terms of solution quality and running time, while producing solutions with comparable quality than their sequential counterparts.
</EventDescription>
    <EventParentName>73093-SESS</EventParentName>
    <EventUniqueID>73093-117701</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Sphynx: The First Graph Partitioner on AMD GPUs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS49</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Seher Acer</EventSpeakers>
    <EventSpeakerIDs>803101</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117702</EventHandoutURL>
    <EventDescription>Sphynx: Graph partitioning is an essential step of large-scale scientific computations to distribute the work among processing elements efficiently. New accelerators used in leadership-class supercomputers have led scientific application developers to port their libraries to those architectures. Sphynx was proposed last year to address the need of a graph partitioner which is portable across the different architectures, and it is the first multi-GPU graph partitioner useable in an MPI+CUDA setting. Sphynx relies on the Trilinos scientific software library and thanks to the portability provided by the Kokkos package, Sphynx is now the first graph partitioning tool to run on (multiple or single) AMD GPUs. In this talk, we present the performance results of Sphynx obtained on an AMD system, which is a small replica of the first exascale system, Frontier.</EventDescription>
    <EventParentName>73093-SESS</EventParentName>
    <EventUniqueID>73093-117702</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Hierarchical Partitioning for GPU-Based High Performance Computational Platforms</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS49</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christopher Brissette</EventSpeakers>
    <EventSpeakerIDs>795541</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117703</EventHandoutURL>
    <EventDescription>This talk will discuss the extension of our existing PuLP/XtraPuLP graph partitioners onto GPU-based distributed computational systems. Two primary challenges and our solutions will be discussed. First, we will discuss our implementation of an efficient graph data structure for coarsening, a key processing step of multilevel partitioning. We use this data structure as part of a novel, fast, and distributed coarsening phase. This efficient coarsening phase allows us to better exploit the massive concurrency available on our target systems, without sacrificing partitioning edge cut quality or balance. Second, we extend the subroutines within PuLP/XtraPuLP to allow for solutions to hierarchical formulations of the partitioning problem. We demonstrate considerable improvements in terms of speed and efficiency relative to our preliminary efforts. The end goal of this work is to accelerate large-scale climate simulations on leadership-class supercomputers.</EventDescription>
    <EventParentName>73093-SESS</EventParentName>
    <EventUniqueID>73093-117703</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Modularity and Composability for High Performance Computing via PETSc</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS50</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:35:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:55:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Oana Marin</EventSpeakers>
    <EventSpeakerIDs>782760</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117780</EventHandoutURL>
    <EventDescription>The Portable, Extensible Toolkit for Scientific Computation (PETSc) emerged as a linear algebra for high-performance computing (HPC) library. The composability concept in PETSc relies on the idea that different solvers can play different roles in an algorithmic sequence. For example, the conjugate gradient can be either a direct solver or a preconditioner. Based on this assertion, the user interface design allows different compositions to ensure the solvability of complex engineering problems or advance linear algebra research. To facilitate the interface to different discretizations yielding a range of matrix structures, PETSc allows for modular constructions of discrete operators. Implicit time-steppers which require linear algebraic treatments interface easily with linear solver objects (KSP) or non-linear ones (SNES).
Across HPC environments, and more so on nascent heterogeneous architectures, scalability and portability are significant concerns. For example, the convergence of iterative solvers is subject to variations in the number of iterations or computing precision. PETSc's advanced logging and testing infrastructure tracks numerical behavioral changes and assures portability. To enforce scalability, PETSc provides data management (DM) interfaces that the user can control within the solver, at runtime or delegate to PETSc to decide optimal splits for performance. </EventDescription>
    <EventParentName>73120-SESS</EventParentName>
    <EventUniqueID>73120-117780</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Extra-P Meets Hatchet: Towards Modeling in Performance Analytics</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS51</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>804</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sergei Shudler</EventSpeakers>
    <EventSpeakerIDs>803586</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118337</EventHandoutURL>
    <EventDescription>As the scale of HPC systems increases, so does the volume of performance data generated by various profiling tools. These tools capture multiple performance metrics at various levels of parallelism (e.g., threads and processes) and from different sources (e.g., CPU and GPU) resulting in performance profiles that cannot be analyzed by a simple visual inspection. Hatchet is a Python-based tool that was developed to address this challenge. It brings the power of modern data science into performance analysis by allowing filtering, pruning, and aggregating performance data programmatically. In our ongoing work, we aim to supplement the performance analytics functionality enabled by Hatchet with automated modeling capabilities. Extra-P is a unique and powerful tool for automated performance modeling. It generates human-readable models that help uncover scalability bugs and provide insight into code performance at scale. In this talk, we present our recent effort to integrate Extra-P into Hatchet. With only a few lines of code it is now possible to create insightful models for complete sequences of GraphFrames in Hatchet. We aim to make this functionality accessible to users and further explore various workflows on top of it.</EventDescription>
    <EventParentName>73130-SESS</EventParentName>
    <EventUniqueID>73130-118337</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Numerical and Computational Performance of Spectral Element Methods for Prototype Fusion Problems</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS52</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:35:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:55:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>3000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>David Moxey</EventSpeakers>
    <EventSpeakerIDs>803454</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118087</EventHandoutURL>
    <EventDescription>High-order methods are increasingly being viewed as an enabling technology for bespoke high-fidelity modelling of challenging physical systems. The use of high-order polynomials within elements, when compared to classical linear and second-order methods, offers advantages from both a numerical analysis and implementation perspective. The ‘resolution power’ of high-order discretisations is far greater than at lower orders, with favourable dispersion and diffusion characteristics meaning that complex spatial structures and phenomena spanning multiple length scales can be accurately resolved and tracked across long timescales. This accuracy is critical in highly sensitive and complex systems, such as fluid dynamics, where high-order methods are amongst the most promising methods for next generation numerical simulations of high-fidelity problems at the exascale. This talk will outline the developments currently being made in the algorithmic and software improvements required to enable high-order simulations to be performed for the problems of interest to the NEPTUNE community. In particular, we will present current developments in curvilinear mesh generation for fusion-related geometries, and on the computationally-efficient high-order implementation of prototype systems related to heat transport in highly anisotropic systems.
</EventDescription>
    <EventParentName>73205-SESS</EventParentName>
    <EventUniqueID>73205-118087</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>The Submatrix Method within Electronic-Structure Theory and Approximate Computing</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS23</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Thomas Kühne</EventSpeakers>
    <EventSpeakerIDs>803063</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117640</EventHandoutURL>
    <EventDescription>We present the submatrix method and a novel linear-scaling electronic-structure method in conjunction with approximate computing, as well as the implementation of the technique in CP2K. Even though initially proposed for inverse p-th roots, it has recently been recognized that the submatrix method represents a general method to approximate arbitrary matrix functions such as the matrix-sign function of large sparse matrices. The Matrix-sign function is the essential workhorse of linear-scaling electronic-structure theory, and we present an intuitive chemical justification for the accuracy of the submatrix method. We will discuss the efficient implementation of the submatrix method into CP2K with a special focus on limiting communication between compute nodes. The resulting compute kernel is the sign function of a relatively small but dense matrix. Our optimized implementation with a simple diagonalization-based evaluation of the sign function of the submatrices outperforms the Newton-Schulz Sign iteration in initial results, especially for larger cutoffs of matrix elements. This observation shows that the submatrix method will be a valuable tool in the context of approximate computing. 





</EventDescription>
    <EventParentName>73075-SESS</EventParentName>
    <EventUniqueID>73075-117640</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Compression Techniques for Large Three-Dimensional Boundary Element Methods</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS23</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Steffen Börm</EventSpeakers>
    <EventSpeakerIDs>790132</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117642</EventHandoutURL>
    <EventDescription>Modern algorithms for boundary element problems are frequently based on a tree-structured decomposition of the matrix into submatrices that can be approximated by low-rank factorizations. When parallelizing these algorithms, we have to take the tree structure into account, e.g., by using threads to handle subtrees. More advanced algorithms can be rewritten to reduce the number of synchronization barriers.

This talk outlines the techniques used to parallelize fast BEM algorithms and presents experiments indicating that fairly large matrices can be treated efficiently without sacrificing the theoretically optimal convergence rate.
</EventDescription>
    <EventParentName>73075-SESS</EventParentName>
    <EventUniqueID>73075-117642</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Performance Optimizations of Low-Rank Matrix Computations for 3D Unstructured Mesh Deformation</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS23</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Rabab Al-omairy</EventSpeakers>
    <EventSpeakerIDs>803336</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117894</EventHandoutURL>
    <EventDescription>We present a framework that couples the PaRSEC runtime and the HiCMA library to solve challenging 3D data-sparse problems. This work highlights how a software bundle powered by a task-based programming model can address the heterogeneous workloads engendered by compressing the dense operator. Using Tile Low-Rank approximation, our approach consists in capturing the most significant information in each tile of the matrix using a threshold. Matrix operations are performed on the compressed data layout, reducing memory footprint and algorithmic complexity. Our proposed software solution accommodates a range of data structures of linear algebra from dense and data-sparse to sparse. We introduce two runtime optimizations to address the challenges encountered when confronted with a large rank disparity: (1) a trimming procedure performed at runtime to cut away data dependencies from the directed acyclic graph discovered to be no longer required after compression and (2) a rank-aware diamond-shaped data distribution to mitigate the load imbalance overheads and reduce data movement. We assess our implementation using 3D unstructured mesh deformation based on Radial Basis Function interpolation. Our implementation shows up to 7-fold performance superiority in situations where the 3D unstructured mesh deformation application renders a matrix operator with low density. Our software framework solves a formally dense 3D problem with 52M mesh points on 65K cores in about half an hour.

</EventDescription>
    <EventParentName>73075-SESS</EventParentName>
    <EventUniqueID>73075-117894</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>The Hazards and Challenges of Low and Mixed Precision Computation</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS26</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:15:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Erin Claire Carson</EventSpeakers>
    <EventSpeakerIDs>746995</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117632</EventHandoutURL>
    <EventDescription>Within the next few years, exascale-level machines will become a reality. Making use of this available computing power in applications will require a massive effort towards optimizing parallel large-scale computations. This challenge is complicated by the increasing heterogeneity of machines at the node level. Notably is the emergence of low and mixed precision hardware, the use of which offers potentially huge performance gains.

A common technique in scientific computing is to introduce inexactness into the computation to reduce computation time, e.g. in model order reduction, sparsification, low-rank approximation, and randomized and communication-avoiding algorithms. In many cases, roundoff error is assumed to be orders of magnitude smaller than any approximation error, and thus the interaction between these different errors is often ignored. In the world of low precision computation, however, this may no longer be a valid assumption.

We therefore ask the question: When does floating-point error matter, and what can go wrong if we use low precision blindly? This talk will give examples, summarize some existing work in this area, and explain the remaining gaps in our knowledge.
</EventDescription>
    <EventParentName>73076-SESS</EventParentName>
    <EventUniqueID>73076-117632</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Mixed Precision Iterative Refinement with Approximate Factorization for the Solution of Large Sparse Systems</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS26</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:20:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:40:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Bastien Vieublé</EventSpeakers>
    <EventSpeakerIDs>794956</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117633</EventHandoutURL>
    <EventDescription>The increasing availability of low precision arithmetic in hardware and the growing popularity of numerical approximation methods pave the way to new opportunities for improving speed, memory, and energy consumption for the solution of linear systems. State of the art iterative refinement methods, three precisions LU-based and five precisions GMRES-based iterative refinement, are able to compute an accurate solution from an approximate and/or low precision factorization while preserving performance and robustness. In this talk we will showcase the effectiveness of some of these algorithms for the parallel solution of large scale sparse linear systems through an implementation that relies on the MUMPS sparse direct solver. In particular we will investigate the case of Block Low Rank factorization, static pivoting and a combination of the two on problems from a variety of real life industrial and academic applications. In addition, we provide new iterative refinement convergence conditions for a general approximate factorization.
</EventDescription>
    <EventParentName>73076-SESS</EventParentName>
    <EventUniqueID>73076-117633</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Mixed Precision Solver Capabilities for the Hypre Linear Solver Library</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS26</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 12:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22 12:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daniel Osei-Kuffuor</EventSpeakers>
    <EventSpeakerIDs>734193</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117635</EventHandoutURL>
    <EventDescription>Mixed precision strategies are a class of approximate computing techniques that are well-suited to iterative algorithms. The increasing cost of data movement, coupled with the significant discrepancy in computational throughput between the different levels of precision in HPC hardware, has led to increased interest in the development of mixed-precision strategies as an important tool for designing efficient and numerically stable linear algebra kernels and algorithms.

This talk will address some of our efforts to enable mixed-precision algorithm development within the hypre linear solver library, to improve performance and make efficient use of existing and emerging HPC hardware.
</EventDescription>
    <EventParentName>73076-SESS</EventParentName>
    <EventUniqueID>73076-117635</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>On Conjugate Bayesian Linear Regression Frameworks for High-Dimensional Spatial Modeling</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS27</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 12:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22 12:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sudipto Banerjee</EventSpeakers>
    <EventSpeakerIDs>769018</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118011</EventHandoutURL>
    <EventDescription>High-dimensional spatially oriented data sets are prevalent in the physical sciences. We seek to jointly model several variables, each indexed by a spatial location, to capture associations over space, time and among the dependent variables. Multivariate latent spatial process models have proved effective in driving statistical inference and rendering better predictive inference at arbitrary locations for the spatial process. This talk will present extensions of scalable modeling strategies for multivariate processes including for several variables. We pursue Bayesian inference, which is attractive for full uncertainty quantification of the latent spatial process. We discuss computationally efficient parallelizable approaches using conjugate Bayesian matrix-normal distributions in spatial factor models as well as graphical Gaussian processes that deliver inference over a high-dimensional parameter space and for very large numbers of variables. We illustrate the computational and inferential benefits of our algorithms over competing methods using simulation studies and an analysis of some remote sensed data sets.
</EventDescription>
    <EventParentName>73114-SESS</EventParentName>
    <EventUniqueID>73114-118011</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Challenges of Mixed-Precision Fast Fourier Transforms from the Instruction Level to at Scale Computations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS29</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:45:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Lukasz Ligowski</EventSpeakers>
    <EventSpeakerIDs>758295</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116888</EventHandoutURL>
    <EventDescription>The talk attempts to discuss challenges of creating a Fast Fourier transform implementation that fully exploits opportunities offered by state-of-the art GPUs such as reduced and mixed-precision computing using tensor cores.

Making proper use of high throughput, reduced precision compute requires careful reconsideration of tradeoffs taken by FFT implementation. Result accuracy and performance is inherently sensitive to the algorithmic decomposition of the problem and how its parts are mapped into hardware. To achieve performance improvements, new methods require striking a new balance between memory bandwidth, compute bandwidth and latency.

All of these new options add to the design space of possible implementations and the enlarged design space allows to push the boundary of the existing limitations. The talk will discuss design choices in the context of real-world examples.


</EventDescription>
    <EventParentName>72751-SESS</EventParentName>
    <EventUniqueID>72751-116888</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Mixed Precision in Linear Algebra</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS29</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:20:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:40:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jack Dongarra</EventSpeakers>
    <EventSpeakerIDs>4846</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116966</EventHandoutURL>
    <EventDescription xsi:nil="true" />
    <EventParentName>72751-SESS</EventParentName>
    <EventUniqueID>72751-116966</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Double-Word Arithmetic and Accurate Calculation of  Euclidean Norms</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS29</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 12:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22 12:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jean-Michel Muller</EventSpeakers>
    <EventSpeakerIDs>790350</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116967</EventHandoutURL>
    <EventDescription>We consider the computation of the Euclidean (a.k.a.\ L2) norm of an $n$-dimensional  vector in floating-point arithmetic. We review the classical solutions used to avoid spurious overflow or underflow. We modify a recently-published algorithm to allow for a very accurate solution, free of spurious overflows and underflows. The returned result will be within very slightly more than $0.5$ ulps from the exact result, which means that we will almost always provide correct rounding.
</EventDescription>
    <EventParentName>72751-SESS</EventParentName>
    <EventUniqueID>72751-116967</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Device Accelerated Solvers with PETSc. Current Status and Future Perspectives</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS30</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:15:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Stefano Zampini</EventSpeakers>
    <EventSpeakerIDs>768190</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118664</EventHandoutURL>
    <EventDescription>In this talk, we will review recent developments towards the exploitation of GPUs from the Portable and Extensible Toolkit for Scientific computing (PETSc). Current status and future perspectives will be discussed, with a focus on Domain Decomposition solvers, Algebraic Multigrid methods, and hierarchical matrices.
</EventDescription>
    <EventParentName>73348-SESS</EventParentName>
    <EventUniqueID>73348-118664</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Status Report on MANA-2.0: A Future-Proof Design for Long-Running Mpi-Based Simulations for Hpc</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS33</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:45:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Gene Cooperman</EventSpeakers>
    <EventSpeakerIDs>790016</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118752</EventHandoutURL>
    <EventDescription>Support for long-running computations on supercomputers has long been a pain point.  To maintain scheduling flexibility, sysadmins set a maximum resource allocation (e.g., 40 hours) for HPC jobs.  Transparent checkpointing at scale on supercomputers has been a long-time dream to please both users and sysadmins.

MANA-2.0 (MPI-Agnostic, Network-Agnostic checkpointing) is such an effort. Like the original MANA academic prototype, MANA-2.0 operates over any MPI implementation and network interconnect that supports the MPI API standard.  MANA-2.0 is also future-proof, in the sense that it runs independently of the underlying MPI and network libraries.  Details of new algorithms required for its robustness will be presented.

MANA-2.0 is being tested both on NERSC's Cori supercomputer (proprietary Cray MPI and Cray GNI network) and NERSC's Perlmutter (#5 supercomputer; proprietary Cray MPI and HPE Cray Slingshot network).  Like all large projects, this is a years-long collaboration of many participants, to be credited in the talk.</EventDescription>
    <EventParentName>73370-SESS</EventParentName>
    <EventUniqueID>73370-118752</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Cardiac Electrophysiology Simulation: Past, Present, and Future</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS36</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  3:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Raymond Spiteri</EventSpeakers>
    <EventSpeakerIDs>47064</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118771</EventHandoutURL>
    <EventDescription>Cardiac electrophysiology simulation provides a non-invasive means to better diagnose and treat cardiac pathologies.  At present, the most reliable model for cardiac electrophysiology simulation is arguably the bidomain model, which is based on a homogenization of myocardial tissue such that the intra- and extra-cellular domains are co-located. Historically, this homogenization was done mostly to make computations feasible despite some desire to model each of the heart's two billion muscle cells individually. The homogenization, however, obscures the cell membrane dynamics, thus removing the possibility of studying membrane channelopathies that contribute to cardiac pathologies such as long QT syndrome.

As computing power has increased, so have the expectations from models and simulations. The extracellular-membrane-intracellular (EMI) model resolves the detailed characteristics of ion channels on individual cell membranes, thus making it highly suitable for studying channelopathies.  The EMI model resolves the tissue-scale dynamics at about four orders of magnitude higher than the bidomain model. This increased resolution opens the door to studying a wider array of heart conditions but at a significant increase in computational cost. This presentation gives an overview of the current state of cardiac electrophysiology simulation, starting from its relatively humble beginnings and ending with a glimpse into what the future may hold for it.</EventDescription>
    <EventParentName>73365-SESS</EventParentName>
    <EventUniqueID>73365-118771</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Operator-Splitting Decompositions for Efficient Solution to the Emi Equations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS36</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:45:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kevin  Green</EventSpeakers>
    <EventSpeakerIDs>790232</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118772</EventHandoutURL>
    <EventDescription>Temporal operator-splitting is a general approach for decomposing time-dependent multi-physics problems into smaller constitutive chunks that can each be solved independently and then recombined to form an approximate solution. The general approach is typically most useful when the individual chunks function on different time-scales, each requiring specialized methods for solution. The extracellular-membrane-intracellular (EMI) model of cardiac electrophysiology---which aims to explicitly model electrical potentials of extracellular space, intracellular space, and across the cell membrane of cardiomyocytes---is a candidate multi-physics example where operator-splitting can show its benefits. 

Using a recently developed operator-splitting class within the deal.ii finite element library, we discuss the implementation of such multi-physics models in this framework and present a survey of computational results pertaining to different order operator-splitting schemes in electrophysiological modelling.

</EventDescription>
    <EventParentName>73365-SESS</EventParentName>
    <EventUniqueID>73365-118772</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Accelerating Configuration Interaction Calculations for Atomic Nuclei</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS37</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Pieter Maris</EventSpeakers>
    <EventSpeakerIDs>727519</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118691</EventHandoutURL>
    <EventDescription>Many-Fermion Dynamics—nuclear, or MFDn, is a Configuration Interaction
(CI) code for ab initio calculations of the structure of atomic
nuclei. It is a platform-independent Fortran 90 code using a hybrid
MPI+X programming model, which performs efficiently on CPU-based HPC
platforms using MPI between nodes and OpenMP for shared-memory
parallelism within a node. A typical calculations involves
constructing a large irregular sparse symmetric matrix, of which we
obtain the lowest eigenvalues and corresponding eigenvectors using the
Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG)
algorithm. Rapid convergence of this block iterative method is
achieved by a suitable choice of starting guesses of the eigenvectors
and an effective distributed preconditioner.

We have recently updated MFDn in order to take advantage of
accelerators in current and future HPC platforms using OpenACC,
targeting Perlmutter at NERSC; in addition we are exploring OpenMP
target offload as well. In this talk I describe some of the
challenges of creating an efficient GPU implementation using
directives, both for the matrix construction and for the iterative
eigensolver. Furthermore, with increasing local compute-power, data
movement and inter-node communication can easily become a bottleneck,
in particular for distributed iterative solvers; we are therefore also
implementing a matrix-free version of the solver.</EventDescription>
    <EventParentName>73361-SESS</EventParentName>
    <EventUniqueID>73361-118691</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Lossy Compression and Mixed Precision Strategies for Memory-Bound Linear Algebra</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS41</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  3:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hartwig Anzt</EventSpeakers>
    <EventSpeakerIDs>763986</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117636</EventHandoutURL>
    <EventDescription>The performance of sparse linear algebra is to a large extent constrained by
the communication bandwidth, motivating the recent investigation of
sophisticated techniques to avoid, reduce, and/or hide data transfers
in-between processors and between processors and main memory. One promising
strategy is to decouple the memory precision from the arithmetic precision,
and compress the data before invoking communication operations. While this
generally comes with a loss of information, the strategy can be reasonable
when operating with approximate objects, e.g. preconditioners used in
iterative methods or self-healing iterative solvers. We will present a
memory accessor separating the arithmetic precision from the memory
precision and mixed precision algorithms based on the strategy of employing
lower precision formats for communication and memory access without
impacting the final accuracy.
</EventDescription>
    <EventParentName>73077-SESS</EventParentName>
    <EventUniqueID>73077-117636</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Towards Real-Time ACOPF on Emerging Architectures</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS42</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1200</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daniel  Maldonado</EventSpeakers>
    <EventSpeakerIDs>795574</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117446</EventHandoutURL>
    <EventDescription>The ACOPF is an essential algorithm used to plan and optimize the dispatch of thousands of generators in the United States power grid. The ACOPF is a large, non-linear optimization problem that needs to be re-computed several times as the energy demand and generation change over the day. As renewable generation becomes an increasingly important part of the energy mix, its rapid variability requires solving the ACOPF at a much higher frequency thus becoming a "real-time" optimization problem. In this "real-time" setting, there is a requirement for the algorithm to finalize in some pre-established amount of time and to produce a feasible solution. Our talk will review the ACOPF problem which consists of a set of equality constraints representing the physics of the network, and a set of inequality constraints that represent the engineering limitations of the system. We will describe the roadblocks for real-time optimization and study how novel single-instruction multiple-dispatch (SIMD) architectures can be leveraged to overcome these hurdles.</EventDescription>
    <EventParentName>73019-SESS</EventParentName>
    <EventUniqueID>73019-117446</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Testing, Verifying, and Validating Openmp Implementations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS44</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:35:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Swaroop Pophale</EventSpeakers>
    <EventSpeakerIDs>803616</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118587</EventHandoutURL>
    <EventDescription>The validation and verification of OpenMP implementations is critical as it provides a mechanism that tests for an implementation’s conformance to the OpenMP standard, as well as highlights ambiguities in the OpenMP specification. Currently OpenMP is at Specification version 5.1 while the vendors are still at different stages of implementing 5.0 features. With difference in vendor support it is critical for applications to know 1) the status of the implementations, especially with respect to the offload features that allow computations to be performed on target devices such as GPUs; 2) the conformance of the implementations to the specification – this is the correctness aspect --which becomes increasingly complex when multiple OpenMP features are exercised together. Through the SOLLVE V&amp;V we have tried to provide the fundamental tests that check for conformance keeping in mind how applications would use these OpenMP features. This talk will focus on the how, what, and the interesting lessons learnt during the testing of different OpenMP implementations and the key takeaways from our experiences. </EventDescription>
    <EventParentName>72832-SESS</EventParentName>
    <EventUniqueID>72832-118587</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Fluid Simulations Accelerated with 16 bit: Approaching 4x Speedup on A64FX by Squeezing ShallowWaters.jl into Float16</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS45</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Milan Kloewer</EventSpeakers>
    <EventSpeakerIDs>795125</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116889</EventHandoutURL>
    <EventDescription>Most Earth-system simulations run on conventional CPUs in 64-bit double-precision floats, although the need for high-precision in the presence of large uncertainties has been questioned. Fugaku, currently the world’s fastest supercomputer, is based on A64FX microprocessors, which also support the 16-bit low-precision format Float16. We investigate the Float16 performance on A64FX with ShallowWaters.jl, the first fluid circulation model that runs entirely with 16-bit arithmetic. Techniques are implemented to address precision and dynamic range issues in 16 bit. The precision-critical time integration is augmented to include compensated summation to minimize rounding errors. Such a compensated time integration is as precise but faster than mixed-precision with 16 and 32-bit floats. As subnormals are inefficiently supported on A64FX the very limited range available in Float16 is $6 \cdot 10^{-5}$ to $65504$. We develop the analysis-number format Sherlogs.jl to log the arithmetic results during the simulation. The equations in ShallowWaters.jl are then systematically rescaled to fit into Float16, using 97\% of the available representable numbers. Consequently, we benchmark speedups of 3.8x on A64FX with Float16 and 3.6x when adding a compensated time integration. ShallowWaters.jl shares essential algorithms with large Earth-system models and therefore shows that 16-bit calculations are indeed a competitive way to accelerate Earth-system simulations on available hardware.</EventDescription>
    <EventParentName>72752-SESS</EventParentName>
    <EventUniqueID>72752-116889</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Parallel Simulation of Cancer Cell Migration</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS46</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:35:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:55:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>3000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Dung Nguyen</EventSpeakers>
    <EventSpeakerIDs>802650</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116709</EventHandoutURL>
    <EventDescription>In biological tissues, cells touch each other with minimal space in-between, limiting their relative motion. When cells migrate, they undergo a “jamming” transition from a fluid-like to a solid-like state, so called glassy dynamics. Previous studies have revealed that jamming transition is governed by active motility, interaction forces, cell shape and applied stress.  Complementing experimental techniques, numerical modeling and simulation offer significant benefits towards gaining insights into biological processes and predicting outcomes. In this talk I describe a numerical model for studying jamming and glass transitions that account for both cell shapes and motility. A hybrid vertex and deformable polygon method are introduced that accounts for the viscoelasticity of the cell membrane. The model is implemented using the particle data structure in the AMReX software framework, where each particle represents one cell with multiple vertices defining its boundary; the intra- and inter-cellular forces are represented as interactions between data members of the same particle vs of different particles, respectively, and the AMReX neighbor list construct is used to minimize the number of interactions that must be evaluated.   Exploiting the AMReX framework to achieve performance portability, this model can run efficiently on high-performance computer systems with heterogeneous architectures. 

</EventDescription>
    <EventParentName>72757-SESS</EventParentName>
    <EventUniqueID>72757-116709</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Implementation and Scalability of a High-Order Immersed Interface Method on HPC Architectures</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP1</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:20:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>James Gabbard</EventSpeakers>
    <EventSpeakerIDs>796148</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118760</EventHandoutURL>
    <EventDescription>Immersed interface methods locally modify a finite difference scheme to account for discontinuities in the solution of a PDE while preserving the accuracy of the initial scheme. They are particularly useful in simulating interface-coupled multiphysics problems such as fluid-structure interaction and multiphase flow. However, the local modifications to the discretization widen the stencil of the finite difference scheme, introducing additional communication requirements in a distributed-memory environment. In this work we identify the specific communication requirements of high-order immersed interface methods and propose a scalable approach for managing this communication on block-structured Cartesian grids. We then demonstrate its effectiveness through fourth-order accurate simulations of PDE-based interface problems on adaptive multiresolution grids.
</EventDescription>
    <EventParentName>73758-SESS</EventParentName>
    <EventUniqueID>73758-118760</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Deep Learning and Spectral Embedding for Graph Partitioning</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP3</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:45:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:05:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Alice Gatti</EventSpeakers>
    <EventSpeakerIDs>805315</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119924</EventHandoutURL>
    <EventDescription>We present a graph bisection and partitioning algorithm based on graph neural networks. For each node in the graph, the network outputs probabilities for each of the partitions. The graph neural network consists of two modules: an embedding phase and a partitioning phase. The embedding phase is trained first by minimizing a loss function inspired by spectral graph theory. The partitioning module is trained through a loss function that corresponds to the expected value of the normalized cut. Both parts of the neural network rely on SAGE convolutional layers and graph coarsening using heavy edge matching. The multilevel structure of the neural network is inspired by the multigrid algorithm. Our approach generalizes very well to bigger graphs and has partition quality comparable to METIS, Scotch and spectral partitioning, with shorter runtime compared to METIS and spectral partitioning.	
</EventDescription>
    <EventParentName>73828-SESS</EventParentName>
    <EventUniqueID>73828-119924</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Block Low Rank and Hierarchical Low Rank Solvers for Fractional Diffusion Equations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP4</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:15:00 AM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>George  Turkiyyah</EventSpeakers>
    <EventSpeakerIDs>769524</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118792</EventHandoutURL>
    <EventDescription>Significant computational challenges are encountered when solving multidimensional space-fractional diffusion equations with spatially varying diffusivity or fractional order. The difficulties are due to both the kernel singularity in the fractional integral operator and the resulting dense discretized operators, which quickly become prohibitively expensive to handle because of their memory and arithmetic complexities. In this talk, we present two alternative low rank representation techniques to handle the dense matrix discretization. One is a block low rank representation that exploits the ability to approximate uniformly-sized blocks of the matrix by low rank factorizations. A Cholesky factorization solver operates directly on this representation using the low rank blocks as its atomic computational tiles, and achieves high performance on GPU hardware with $O(N^{1.5})$ storage and $O(N^2)$ arithmetic cost. In order to reach optimal complexities at scale, we present another multi-GPU distributed hierarchical matrix representation of the discretized operator, and an AMG-preconditioned Krylov solver that operates on it. We illustrate the scalability of the representation and of the solver on examples up to 16M degree-of-freedom 2D problems on 64 GPUs.
	</EventDescription>
    <EventParentName>73760-SESS</EventParentName>
    <EventUniqueID>73760-118792</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Multifrontal Solver with Low-Rank Compression</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP4</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 12:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22 12:30:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Lisa Claus</EventSpeakers>
    <EventSpeakerIDs>795709</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119073</EventHandoutURL>
    <EventDescription>Some of the major challenges to scale parallel applications are to keep the asymptotic complexity of computation and the memory consumption low. This talk presents a multifrontal solver that leverages hierarchical and non-hierarchical matrices, using low-rank compression methods, to reduce the asymptotic complexity of computation and memory usage. The multifrontal algorithm takes advantage of the low-rank property of the blocks appearing during the factorization of sparse linear systems with the utilization of a compression algorithm. 

Previously, the hierarchically off-diagonal butterfly (HODBF) format has been leveraged to accelerate compression and factorization in the multifrontal method. HODBF turns out to be very efficient for large-sized fronts. In addition, experiments have shown a reduced flop count for the non-hierarchical Block Low-Rank (BLR) compression of medium size fronts in comparison with hierarchical rank-structured techniques.

In this talk, we discuss a hybrid version using the HODBF format in combination with the BLR format. In particular, we apply the HODBF compression to large sized fronts of the multifrontal method and apply the BLR compression to medium sized fronts. This leads to a reduction of the factorization time and memory consumption and allows us to solve larger problem sizes. We apply the factorization as a preconditioner for GMRES and illustrate the performance of this new preconditioner.

	</EventDescription>
    <EventParentName>73760-SESS</EventParentName>
    <EventUniqueID>73760-119073</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Nonblocking Execution in Graphblas</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP5</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:45:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:05:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Aristeidis Mastoras</EventSpeakers>
    <EventSpeakerIDs>804298</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119065</EventHandoutURL>
    <EventDescription>GraphBLAS is a recent standard that allows expression of graph algorithms in the language of linear algebra and enables easy code parallelization and optimization. GraphBLAS operations are executed either in blocking or in nonblocking mode. The blocking mode guarantees that the output of the operation is computed when the function returns. On the other hand, the nonblocking mode aims at performing code optimizations that improve the overall performance by delaying the execution of the operations. Thus, a function is allowed to return although the computation may not be completed. Subsequent GraphBLAS operations can be safely invoked, but they may need to delay their computations as well, until the output of all earlier operations is computed and stored in memory. There exist multiple implementations of GraphBLAS for both shared and distributed memory systems, but none of them supports full nonblocking execution. We present the design and implementation of the first full nonblocking execution in GraphBLAS for shared memory systems. Nonblocking execution is achieved by exploiting lazy evaluation to enable runtime optimizations that improve data locality. Dynamic parallelism is used to group together operations that reuse data in cache and execute them in parallel. Since GraphBLAS operations are memory bound, a preliminary evaluation confirms the importance of nonblocking execution for various datasets of two algorithms, by showing up to 2.5x speedups over blocking execution.</EventDescription>
    <EventParentName>73826-SESS</EventParentName>
    <EventUniqueID>73826-119065</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Tile Low-Rank Matrix-Vector Multiplication with Architecture-Aware Dynamic Load Balancing</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP6</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:45:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:05:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yuxi Hong</EventSpeakers>
    <EventSpeakerIDs>796717</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118472</EventHandoutURL>
    <EventDescription>We present Tile Low-Rank Matrix-Vector Multiplication (TLR-MVM) with an architecture-aware dynamic load-balancing. Our TLR-MVM implementation leverages high bandwidth memory technologies and exploits data sparsity and an efficient data layout. Load balancing being a cornerstone of performance, we design a portable MPI+OpenMP framework and balance workloads using the OpenMP dynamic scheduler for node performance. We further accelerate TLR-MVM by explicitly mapping the processes/threads onto the underlying hardware, increasing occupancy. We achieve decent scalability on several x86, ARM, and GPU-based hardware platforms. We highlight TLR-MVM performance impact on computational astronomy and seismic imaging applications.</EventDescription>
    <EventParentName>73761-SESS</EventParentName>
    <EventUniqueID>73761-118472</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Parallel Asynchronous Reinforcement Learning Algorithm for Distributed Learning</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP6</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:35:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:55:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Rama Vasudevan</EventSpeakers>
    <EventSpeakerIDs>804335</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119103</EventHandoutURL>
    <EventDescription>Reinforcement Learning (RL) has not been heavily applied to scientific simulations, partly due to computational constraints. Here, we present a distributed and accelerated RL algorithm for tackling a materials science challenge simulated via a kinetic Monte-Carlo model. 

Our parallel RL algorithm works as follows. An RL computation is composed of RL agents, simulations, and models. In the case of a single process, we place RL agents on GPUs and simulations run on the CPU, with each agent updating their model based on completed simulation runs from every other agent. In the case of distributed memory parallelism, we partition RL agents among different processes to reduce inter and intra-node communication. When trying to scale to many processes, time imbalance between different agents becomes a major bottleneck. We propose an asynchronous update algorithm, where all agents need not synchronize in every iteration.  
 
The asynchronous update algorithm shows much better load balancing, which effectively minimizes idling time as agents wait for simulations to complete before they can proceed. We find this load balancing can result in at least 20% efficiency gains for highly stochastic simulations, such as those common to kinetic modeling in materials science. We demonstrate the efficacy of our approach for the Stein variational policy gradient algorithm, although our strategy can be readily adapted to other RL algorithms and readily modified based on convergence needs.
 
</EventDescription>
    <EventParentName>73761-SESS</EventParentName>
    <EventUniqueID>73761-119103</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Chronos: a GPU-Accelerated Adaptive AMG for Extreme Size Simulations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP7</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  3:40:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Giovanni Isotton</EventSpeakers>
    <EventSpeakerIDs>798545</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118264</EventHandoutURL>
    <EventDescription>The solution of linear systems of equations is a central task in several scientific and engineering applications. For extreme-size simulations, nowadays accounting for hundreds of millions or even billions of unknowns, the algebraic multigrid (AMG) methods are the most common choice as linear solvers because of their optimal or nearly optimal convergence. In terms of HPC hardware, most of the top-ranked modern supercomputers leverage on GPU accelerators, mounting GPU cards on each compute node. So far, there are few AMG solvers that fully exploit GPUs and most are basic, not suited to solve ill-conditioned problems. The porting of complex algorithms to GPUs is not easy and needs continuous updating to preserve performance because it is a dynamically evolving hardware. In this context, Chronos is emerging as a sophisticated AMG framework that relies on several peculiarities: an adaptive test space generation, a sophisticated smoother that is effective even for severely ill-conditioned problems, an adaptive prolongation, and a filtering procedure able to preserve both interpolation and operator main properties. This communication focuses on the enhancement of Chronos by using GPU accelerators on distributed memory systems. A set of real-world problems, including structural mechanics, CFD and underground applications, are solved to assess the efficiency and robustness of Chronos.

	</EventDescription>
    <EventParentName>73822-SESS</EventParentName>
    <EventUniqueID>73822-118264</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>An Accurate Method to Select Floating-Point Precision and Control Round-off Error on Upwind Scheme</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP8</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:35:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:55:00 AM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>William Weens</EventSpeakers>
    <EventSpeakerIDs>790097</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118492</EventHandoutURL>
    <EventDescription>HPC and exaflopic machines offer computational capabilities that allow simulations to be performed on large meshes. With a large number of meshes and a large number of iterations, the weight of rounding errors on the global error increases and must be controlled so that the results do not deviate too much from the solution.
Moreover, current machine architectures perform better with single precision than with double precision. If the code implementation can take advantage of this performance, then the simulations will be able to lower their memory and energy consumption and the restitution time. The problem of measuring round-off errors is therefore critical for verifying simulation codes.

In this study, we precisely measured the effect of rounding errors on the upwind scheme on different test cases of the transport equation and derived a method to ensure that round-off errors remain small compared to discretization errors. This method allows, for example, to select a precision adapted to the calculation (single or double) and to take advantage of the HPC performances while guaranteeing the results.

	
</EventDescription>
    <EventParentName>73762-SESS</EventParentName>
    <EventUniqueID>73762-118492</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Support for Fully Parameterized Construction of $\mathcal{P}_{r}^{-}$ Finite Elements in PETSc</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP8</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:45:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Brandon Whitchurch</EventSpeakers>
    <EventSpeakerIDs>796671</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118494</EventHandoutURL>
    <EventDescription>Efficient use of the Finite Element method requires that the discrete spaces for the computation be carefully selected to match properties of the continuous problem. From a developer's standpoint this means that we should attempt to provide as many tools for constructing a wide variety of spaces as possible. We present our recent work in the PETSc code base which allows users to construct function spaces by applying operations to other function spaces which have been previously implemented. This work focuses on the implementation of the Koszul differential operator which makes possible full parametric construction of the $\mathcal{P}_{r}^{-}$ family of finite elements --- including elements such as the Raviart-Thomas and N\'ed\'elec elements as well as some more recent constructions --- which are useful for constructing $\mathcal{H}(\mathrm{div})$ or $\mathcal{H}(\mathrm{curl})$ conforming discretizations.
  
In this work, we describe the computational implementation of the Koszul differential operator.  Since the null space of the Koszul differential is not necessarily empty we have developed a procedure for identifying the null space via random sampling of the dual basis. This new procedure is also described and we provide estimates of time complexity and proofs of correctness and reliability of the algorithm.</EventDescription>
    <EventParentName>73762-SESS</EventParentName>
    <EventUniqueID>73762-118494</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Scalable Computational Framework for Wavelet-Based 3D Multiresolution Grids</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP8</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:30:00 AM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Thomas Gillis</EventSpeakers>
    <EventSpeakerIDs>796147</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118670</EventHandoutURL>
    <EventDescription>We present a wavelet-based block-structured 3D multiresolution framework for finite-difference applications, targeted to run on distributed-memory parallel architectures.

The wavelet-based multiresolution theory enables a coherent approach to identify and compress spatial scales. Of particular relevance when applied to the simulation of a large range of physical phenomena, it naturally adapts the resolution throughout the domain to reduce the overall computational cost.
We have implemented a family of second-generation interpolating wavelet filters, characterized by the interpolation order $N$ and the conservation of $\tilde{N}$ moments. We use the wavelet transform both as a criterion for refinement/compression, as well as a consistent way to interpolate across resolution levels.
Our specific focus is to provide a scalable implementation of the wavelet transform with second-generation filters in 3D, on rectangular domains with arbitrary aspect ratios. Our implementation aims to reduce the communication overhead of ghost reconstruction, while providing flexibility in the choice of the wavelet order. We demonstrate the scalability of our solver on several thousands of cores, and show the dynamic mesh adaptivity using test cases involving scalar transport and diffusion. </EventDescription>
    <EventParentName>73762-SESS</EventParentName>
    <EventUniqueID>73762-118670</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Performance of Low Synchronization Orthogonalization Methods in Anderson Accelerated Fixed Point Solvers</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP9</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:30:00 AM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Shelby Lockhart</EventSpeakers>
    <EventSpeakerIDs>773756</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119934</EventHandoutURL>
    <EventDescription>Anderson Acceleration (AA) is a method to accelerate the convergence of fixed point iterations for nonlinear, algebraic systems of equations. Due to the requirement of solving a least squares problem at each iteration and a 
reliance on modified Gram-Schmidt for updating the iteration space, AA requires extra costly synchronization steps for global reductions. Moreover, the number of reductions in each iteration depends on the size of the iteration space. In this work, we introduce three low synchronization orthogonalization algorithms into AA within SUNDIALS that reduce the total number of global reductions per iteration to a constant of 2 or 3, independent of the size of the iteration space. A performance study demonstrates the reduced time required by the new algorithms at large processor counts with CPUs and demonstrates the predicted performance on multi-GPU architectures. Most importantly, we provide convergence and timing data for multiple numerical experiments to demonstrate reliability of the algorithms within AA and improved performance at parallel strong-scaling limits.

	
</EventDescription>
    <EventParentName>73830-SESS</EventParentName>
    <EventUniqueID>73830-119934</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Batched Second-Order Adjoint Sensitivity for Reduced Space Methods</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP9</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:35:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:55:00 AM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Francois Pacaud</EventSpeakers>
    <EventSpeakerIDs>805319</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119939</EventHandoutURL>
    <EventDescription>This talk presents an efficient method for extracting the second-order sensitivities from a system of implicit nonlinear equations on graphical processing units (GPU). We design a custom automatic differentiation (AutoDiff) backend that targets highly parallel architectures by extracting the second-order information in batch. When the nonlinear equations are associated to a reduced space optimization problem, we leverage the parallel reverse-mode accumulation in a batched adjoint-adjoint algorithm to compute efficiently the reduced Hessian of the problem. We apply the method to extract the reduced Hessian associated to the balance equations of a power network, and present an application to the resolution of the optimal power flow problem.


</EventDescription>
    <EventParentName>73830-SESS</EventParentName>
    <EventUniqueID>73830-119939</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Software Ecosystem Approach to Increasing Progress and Addressing Challenges in Extreme Scale Computing and Big Data</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS2</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Michael Heroux</EventSpeakers>
    <EventSpeakerIDs>706361</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116913</EventHandoutURL>
    <EventDescription>As computational science become more multi-disciplinary, multi-scale and multi-domain, and as scientific applications must execute on increasingly diverse computational platforms, the challenges for any particular team increases dramatically.  Teams must have a variety of skills beyond a specific scientific domain: computer science, software engineering, data science and, for larger teams, social and cognitive sciences.

In this presentation we describe efforts to create ecosystems and sustainable scientific software communities and products to address these challenges.


</EventDescription>
    <EventParentName>72849-SESS</EventParentName>
    <EventUniqueID>72849-116913</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Sparse Linear Algebra for High Performance Machine Learning</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS2</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Serge Petiton</EventSpeakers>
    <EventSpeakerIDs>789960</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116916</EventHandoutURL>
    <EventDescription>Sparse linear algebra is a crucial part of several applications since decades, and is now becoming an important topic for machine learning and Big Data. Classical example are the PageRank methods and Graph Neural NetworK, and, to avoid over-smoothing, the Neural Networks are also generating such problems.

In this talk we propose, in particular, an efficient algorithm to compute sequences of sparse matrix vector products for stochastic matrices, such as matrices involved in the PageRank methods. We discuss experimental results obtain in multicore processors and in a large parallel supercomputer. Then, we discuss some sparse linear algebra methods in the same environment, such as, for example, for Graph Convolutional Network. 



</EventDescription>
    <EventParentName>72849-SESS</EventParentName>
    <EventUniqueID>72849-116916</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Algebraic Auxiliary Matrix Construction for Subspace Correction and Deflation based on Error Vector Sampling for a Sequence of Linear Systems</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS2</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Takeshi Iwashita</EventSpeakers>
    <EventSpeakerIDs>768136</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116918</EventHandoutURL>
    <EventDescription>In this talk, we focus on solving a series of linear systems with an identical (or similar) coefficient matrix. The linear systems are sequentially processed due to the dependence of the right-hand side vector on the solution vector of the prior linear system. For the problem, we investigate the subspace correction and deflation methods to accelerate the convergence of the Krylov subspace method. Practically, these acceleration methods work well when the auxiliary matrix consists of eigenvectors corresponding to small eigenvalues of the coefficient matrix. We have developed a new auxiliary matrix construction method to identify the approximation of the eigenvectors with small eigenvalues using error vector sampling in the prior solution step. Numerical tests confirm that both subspace correction and deflation methods with the generated auxiliary matrix accelerate the convergence of the iterative solver.


</EventDescription>
    <EventParentName>72849-SESS</EventParentName>
    <EventUniqueID>72849-116918</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A New Semi-Structured Algebraic Multigrid Method in Hypre</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS3</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Victor Paludetto Magri</EventSpeakers>
    <EventSpeakerIDs>790151</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117809</EventHandoutURL>
    <EventDescription>Multigrid methods are well suited for solving linear systems on large massively parallel computer architectures since they are mathematically optimal and display excellent parallelization properties. In modern HPC architectures, taking advantage of problem structure has become crucial for performance since most of the computational power originates from GPUs. The hypre software library provides high-performance multigrid preconditioners and solvers through conceptual interfaces, including a semi-structured interface that describes matrices primarily in terms of stencils and logically structured grids. In this talk, we present a new semi-structured algebraic multigrid (SSAMG) method that leverages problem structure and which is built on top of the semi-structured interface. We evaluate this method's numerical convergence and performance for a set of semi-structured problems such as structured adaptive mesh refinement test cases. SSAMG achieves comparable results to other solver options in hypre, but has better potential for GPU architectures than hypre's unstructured AMG solver (BoomerAMG). In addition, the new method can solve more complex problems than hypre's structured solvers such as PFMG and SMG.</EventDescription>
    <EventParentName>72941-SESS</EventParentName>
    <EventUniqueID>72941-117809</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Novel Solver Algorithms for Nearly Singular Linear Systems Arising in Combustion Modelling</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS3</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Paul Mullowney</EventSpeakers>
    <EventSpeakerIDs>803129</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117811</EventHandoutURL>
    <EventDescription>Direct Numerical Simulations of realistic combustion devices are extremely challenging due to the wide separation of scales in the simulation, for example an internal combustion (IC) engine chamber, and the flame thickness of a high-pressure flame. The PeleLMeX solver uses adaptive mesh refinement (AMR) to evolve multi-species reacting flows in the low Mach number limit at the Exascale and relies on an embedded boundary (EB) approach to represent complex geometries. In that framework, the EB geometries often give rise to very small cut-cells along the boundary, which translate into extreme ill-conditioning of the pressure-projection, with eigenvalues that span 15-16 orders of magnitude. In this talk, we focus on the case of a typical IC piston bowl geometry for which we present on a novel approach towards solving these nearly singular linear systems with ILU-based, C-AMG smoothers on massively parallel architectures. In particular, we use scaling and equilibration algorithms to handle the non-normality of the upper triangular factors. This enables us to approximate the highly sequential triangular solve algorithm, embedded in the AMG smoothing-solve phase, with Jacobi iterations. This approximation can be written as a convergent Neumann series whose terms are composed of highly parallel sparse matrix vector multiplications. The result is an algorithm that substantially decreases setup and solve time, compared to state-of-the-art, for these challenging linear systems.</EventDescription>
    <EventParentName>72941-SESS</EventParentName>
    <EventUniqueID>72941-117811</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Matrix-Free Approach for Algebraic Multigrid</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS3</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Graham Harper</EventSpeakers>
    <EventSpeakerIDs>795507</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117815</EventHandoutURL>
    <EventDescription>We present a matrix-free approach for algebraic multigrid for finite element methods on advanced architectures. We implement the algorithm on simple physics problems with an emphasis on the concept and extensions. We show numerical results on our advanced architecture testbeds to support the concepts and discuss room for future work.
</EventDescription>
    <EventParentName>72941-SESS</EventParentName>
    <EventUniqueID>72941-117815</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Kernel Specialization for Faster Segmented Sort on GPUs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS53</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:30:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>500</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Georgii Evtushenko</EventSpeakers>
    <EventSpeakerIDs>806186</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=121429</EventHandoutURL>
    <EventDescription>Segmented sort orders many variable-length arrays independently. Variability of array sizes obstructs efficient use of GPU resources. This talk introduces a new segmented sort algorithm that addresses this issue. The algorithm includes two stages. The first stage consists of efficient three-way segments partitioning on GPU. The second stage relies on this partitioning to apply specialized kernels for different classes of segments. This talk highlights implementation details and performance characteristics of the new segmented sort that is implemented as part of CUB.
</EventDescription>
    <EventParentName>72776-SESS</EventParentName>
    <EventUniqueID>72776-121429</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Simulating Next-Gen Dataflow Architectures for HPC</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS54</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>801</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Clayton Hughes</EventSpeakers>
    <EventSpeakerIDs>780569</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118779</EventHandoutURL>
    <EventDescription>The slowing of Moore's Law has led to an explosion of accelerator designs from both industry and academia. Many of these emerging architectures promise to provide savings in energy efficiency, area, and latency but are domain specific. Moreover, developing a simulation framework capable of exploring a design space that includes a model for each of these accelerators is infeasible. One promising family of generalizable technologies are dataflow architectures, which diverge from the traditional sequentially executed instruction model into one that reflects the inherent instruction-level parallelism in a program. These dataflow accelerators can offer better programmability and more flexibility than domain specific hardware and, in a simulation framework, be used to emulate the behavior of a range of accelerators. This talk will cover some of the research and engineering that has gone into integrating a dataflow model inside of the Structural Simulation Toolkit with a focus on AI/ML and linear algebra applications.</EventDescription>
    <EventParentName>73325-SESS</EventParentName>
    <EventUniqueID>73325-118779</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Assignment of Idle Processors to Spatial Redistributed Domains for Multigrid Reduction in Time</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS56</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:50:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ryo Yoda</EventSpeakers>
    <EventSpeakerIDs>803756</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118633</EventHandoutURL>
    <EventDescription>Parallel-in-time approaches for time-dependent PDEs attract much attention in the context of massively parallel environments. One such approach, multigrid reduction in time (MGRIT), extracts temporal parallelism by applying the reduction-based multigrid method for the temporal domain. To make efficient use of this parallelism, some processors need to be idle on a temporal coarse level of MGRIT. This talk considers an optimization of the implementation by assigning these idle processors to the redistributed spatial domain on coarse levels. It accelerates coarse-level spatial solvers and promotes early switching to parallelization-in-time by reducing its overhead. Although the spatial redistribution is contrary to the motivation of parallel-in-time approaches, it is expected to be effective when we need to balance spatial and temporal parallelism. This is because parallelization-in-time is already usually switched on when parallelization-in-space starts to saturate, so one can still benefit from additional parallelism. Numerical experiments show the effectiveness of the spatial redistribution compared with pure MGRIT, which assigns the best spatial and temporal parallelism at specific parallelism.
</EventDescription>
    <EventParentName>73331-SESS</EventParentName>
    <EventUniqueID>73331-118633</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Fully Constrained PARAFAC2 with AO-ADMM</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS57</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5100</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Marie Roald</EventSpeakers>
    <EventSpeakerIDs>797608</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118725</EventHandoutURL>
    <EventDescription>The PARAFAC2 model is a powerful method to analyse multiway measurements with variations across one mode. The model captures latent factors and allows the factors of one mode to evolve across another, which has proven beneficial for applications in several domains, such as chemometrics, neuroscience and data mining. For example, the PARAFAC2 model can accurately describe chromatographic data despite retention shifts and shape changes of the temporal profiles across samples. A challenge with PARAFAC2, however, is the constant cross-product constraint, which is necessary to ensure a unique decomposition. To impose this constraint, the evolving factor matrices are traditionally modelled implicitly as the product of a shared “coordinate matrix” and a set of orthogonal basis matrices. However, this implicit handling makes it difficult to impose constraints on the evolving factor matrices. In this work, we instead reformulate the regularised PARAFAC2 problem in a form suited for alternating optimisation (AO) with the alternating direction method of multipliers (ADMM), thus making it possible to fit PARAFAC2 models with any proximable constraint on all modes. Using simulated datasets, we show that the proposed scheme is accurate, flexible in terms of constraints and computationally efficient. Finally, we use our method in a real-world chromatography application and show that constraining all modes to be non-negative improves the interpretability of the extracted components.</EventDescription>
    <EventParentName>73352-SESS</EventParentName>
    <EventUniqueID>73352-118725</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Practical, Out-of-Memory Sparse MTTKRP on Massively Parallel Architectures</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS57</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:50:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5100</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Andy Nguyen</EventSpeakers>
    <EventSpeakerIDs>804312</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118726</EventHandoutURL>
    <EventDescription>We propose a novel frame-work for massively parallel execution of fundamental tensor decomposition operations. 
Our new Blocked Linearized CoOrdinate (BLCO) format enables out-of-memory computation of the matricized tensor times Khatri-Rao product (MTTKRP) for every tensor dimension, using a unified implementation that works on a single tensor copy. To address the substantial synchronization cost on massively parallel architectures, we introduce a hierarchical on-the-fly algorithm to efficiently discover and resolve update conflicts across threads without keeping any auxiliary information or storing non-zero elements according to specific mode orientations.
As a result, our BLCO-based MTTKRP not only handles out-of-memory tensors, but also delivers superior in-memory performance compared to mode-specific approaches. 
On the latest NVIDIA A100 GPU, BLCO achieves2.9× geometric-mean speedup over the state-of-the-art mixed-mode com-pressed sparse fiber (MM-CSF) on a range of real-world sparse tensors.</EventDescription>
    <EventParentName>73352-SESS</EventParentName>
    <EventUniqueID>73352-118726</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Machine Learning–Enabled Scalable Performance Prediction of Scientific Codes</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS58</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>804</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Stephan Eidenbenz</EventSpeakers>
    <EventSpeakerIDs>803589</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118339</EventHandoutURL>
    <EventDescription>We present the Analytical Memory Model with Pipelines (AMMP) of the Performance Prediction Toolkit (PPT). PPT-AMMP takes high-level source code and hardware architecture parameters as input and predicts the runtime of that code on the target hardware platfor. PPT-AMMP transforms the code to an (architecture-independent) intermediate representation, then (i) analyzes the basic block structure of the code, (ii) processes architecture-independent virtual memory access patterns that it uses to build memory reuse distance distribution models for each basic block, and (iii) runs detailed basic-block level simulations to determine hardware pipeline usage.

PPT-AMMP uses machine learning and regression techniques to build the prediction models based on small instances of the input code, then integrates into a higher-order discrete-event simulation model of PPT running on Simian PDES engine. We validate PPT-AMMP on four standard computational physics benchmarks and present a use case of hardware parameter sensitivity analysis to identify bottleneck hardware resources on different code inputs. We illustrate a further use case of PPT-AMMP by predicting the performance of the scientific application code SNAP. To this end, we analyze multi-variate regression models that accurately predict the reuse profiles and the basic block counts. We validate predicted SNAP runtimes against actual measured times.
</EventDescription>
    <EventParentName>73131-SESS</EventParentName>
    <EventUniqueID>73131-118339</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>An Automated Framework for Fully Matrix-Free, Hybridised, Compatible, High Order Finite Element Methods</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS59</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sophia Vorderwuelbecke</EventSpeakers>
    <EventSpeakerIDs>796711</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117785</EventHandoutURL>
    <EventDescription>One way to achieve high accuracy for simulations of physically complex problems is to discretise the partial differential equations with higher order, compatible finite element methods (FEM). Since higher order, compatible approximations result in big equation systems, performance optimisations become crucial. A speedup of the solve of the corresponding equation system can be achieved by loosening the global coupling of the FEM with a hybridization preconditioner, such that it is sufficient to execute expensive operations on smaller matrices. The preconditioner is defined through local linear algebra operations on FEM tensors and is represented in the domain specific language Slate in Firedrake.

The local linear algebra operations need further optimisations in order to achieve high performance for high order, compatible FEM, because not only the size of the global tensors, but also of the local tensors is considerably high. By employing locally matrix-free methods high storage requirements/data movement can be avoided in favor of more FLOPS. A high FLOPS to data ratio is advantageous for high performance on recent computer architectures.

I will talk about performance optimisations in form of fully matrix-free, hybridised, compatible, high order FEM in Firedrake and their portability between different problems and across architectures through automatic code generation.</EventDescription>
    <EventParentName>73121-SESS</EventParentName>
    <EventUniqueID>73121-117785</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Sparse Tensor Partitioning for Scalable Distributed CPD-ALS</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS60</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:50:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Cevdet Aykanat</EventSpeakers>
    <EventSpeakerIDs>767248</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117707</EventHandoutURL>
    <EventDescription>Sparse tensor decomposition is a fundamental tool widely used in the analysis of multi-dimensional data. The canonical polyadic decomposition (CPD) is one of the most popular decomposition methods and commonly found by the CPD-ALS algorithm. High computational and memory costs of CPD-ALS necessitate the use of a distributed-memory-parallel algorithm for efficiency. MTTKRP operation, which is the bottleneck of the CPD-ALS, requires sparse expand and sparse reduce communications as well as two dense collective communications. MTTKRP becomes bandwidth bound for large decomposition ranks, whereas it becomes latency bound for small decomposition ranks and large number of processors. Several successful hypergraph-partitioning-based tensor partitioning methods have been proposed for reducing the bandwidth requirement. However, the literature is lacking methods and models, which directly target at optimizing the latency requirement of CPD-ALS, although this is a very important aspect for scaling on large number of processors. In this talk, we will present our recently proposed framework and partitioning for reducing the latency overhead of the sparse expand and reduce communications, thus increasing the scalability of CPD-ALS significantly.</EventDescription>
    <EventParentName>73094-SESS</EventParentName>
    <EventUniqueID>73094-117707</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Accelerating a Sparse Eigensolver using OpenACC</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS61</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Chao Yang</EventSpeakers>
    <EventSpeakerIDs>701749</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118442</EventHandoutURL>
    <EventDescription>We describe the effort to accelerate a sparse eigensolver for nuclear many-body calculations on multiple GPUs. Our work involves modifying a previously developed hybrid MPI/OpenMP implementation of an eigensolver written in FORTRAN90 with OpenACC directives.  The architectural differences between a CPU and a GPU device results in some differences between the way OpenACC directives are inserted and the way OpenMP directives were used. We point out these differences and demonstrate significant speedup achieved on GPUs compared to the on-node performance of a many-core CPU.  We also show that the overall performance improvement of the eigensolver on multiple GPUs is more modest due to the communication overhead among different MPI ranks.
</EventDescription>
    <EventParentName>72874-SESS</EventParentName>
    <EventUniqueID>72874-118442</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Hybrid Direct-Iterative Method for Solving Kkt Linear Systems</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS62</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1200</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Shaked Regev</EventSpeakers>
    <EventSpeakerIDs>802988</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117452</EventHandoutURL>
    <EventDescription>We propose a method to solve sparse indefinite linear systems that arise at each iteration of an interior method for solving constrained optimization problems. The current gold standard for solving these systems is $LDL^T$ factorization.  However, $LDL^T$ requires pivoting during factorization, which substantially increases communication cost. Our novel method solves a large indefinite system by solving multiple smaller positive definite systems, using an iterative solve for the Schur complement and an inner direct solve (via Cholesky factorization) within each iteration. Cholesky is stable without pivoting, thereby reducing communication and allowing reuse of the symbolic factorization. We show that on large systems, our method can efficiently utilize GPUs and outperform $LDL^T$ factorization of the full system.</EventDescription>
    <EventParentName>73021-SESS</EventParentName>
    <EventUniqueID>73021-117452</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Tbd</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS62</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1200</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Andy Sun</EventSpeakers>
    <EventSpeakerIDs>763015</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118290</EventHandoutURL>
    <EventDescription xsi:nil="true" />
    <EventParentName>73021-SESS</EventParentName>
    <EventUniqueID>73021-118290</EventUniqueID>
    <STATUS>inactive</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Computational Optimization of Industrial High-Performance Data Analytics Applications</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS63</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:50:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Timothy Holt</EventSpeakers>
    <EventSpeakerIDs>802732</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116862</EventHandoutURL>
    <EventDescription>
High-performance data analytics applications are becoming increasingly common in industrial applications, but their deployment in a production setting on cluster-scale computers is often bottlenecked by shared memory architectures which are not optimized for the quantity of data throughput that such applications require. By analyzing an application of high-throughput electricity market simulation used by an international commodities trading company, we will show how memory bottlenecks can adversely affect the performance of high-throughput applications. To mitigate these costly bottlenecks, we will present a framework using LIKWID Performance Tools to analyze the computational nature of such applications, and strategies for achieving optimal throughput using the meta-scheduler GREASY to adjust the level of parallelism and introduce CPU affinity to ease memory congestion and improve cache locality.

</EventDescription>
    <EventParentName>72771-SESS</EventParentName>
    <EventUniqueID>72771-116862</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Should You Go with the Flow ? A New Tensor Algebra for Neural Networks</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS64</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Lior Horesh</EventSpeakers>
    <EventSpeakerIDs>726292</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118799</EventHandoutURL>
    <EventDescription>Big, high-dimensional information often involves multi-dimensional correlations that may remain latent by virtue of traditional matrix-based learning algorithms. In this study, we propose a tensor neural network framework that offers an exciting new paradigm for supervised machine learning. The tensor neural network framework is based upon the notion of tensor-tensor product, an algebraic formalism to multiply tensors, which inherits mimetic matrix properties. We further extend the framework to accommodate dynamical graphs in the form of tensor Graph Convolutional Neural Networks. We show that the proposed tensor neural network formalism is a natural high-dimensional extension of conventional neural networks. Lastly, we close with numerical experiments demonstrating the intrinsic advantages of the proposed tensorial architectures.
</EventDescription>
    <EventParentName>73377-SESS</EventParentName>
    <EventUniqueID>73377-118799</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Sparse Precision Matrix Estimation for Large-Scale Datasets</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS64</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Aryan Eftekhari</EventSpeakers>
    <EventSpeakerIDs>783678</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118801</EventHandoutURL>
    <EventDescription>The L1-regularized Gaussian maximum likelihood method is a common approach for sparse precision matrix estimation but one that poses a computational challenge for high-dimensional datasets.
We present a novel method for performant large-scale sparse precision matrix estimation utilizing the block structures and parallelism in the underlying computations for added performance and scalability. Our numerical examples and comparative results with various modern open-source packages reveal that the proposed algorithm provides orders of magnitude faster runtimes and scales to significantly higher dimensional datasets.


</EventDescription>
    <EventParentName>73377-SESS</EventParentName>
    <EventUniqueID>73377-118801</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Sparse Matrix Factorization by the Accelerator-First Principle</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS65</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:55:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daniel Thürck</EventSpeakers>
    <EventSpeakerIDs>803913</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118804</EventHandoutURL>
    <EventDescription>Within the last decade, heterogeneous computing has become mainstream. These days, fueled by the ML/AI revolution, its most common embodiment is the accelerated computing system, i.e. a system combining (a) CPU(s) with one or more compute accelerators. Most commonly, that is a GPU due to their massive FLOP count and extensive software stack. However, the number of alternative accelerators is still growing - leading to a classical chicken-egg problem: why would I buy a new compute accelerator if the software support is still scarce? In this talk, we consider sparse matrix factorization, a crucial building block of scientific applications. We analyze state-of-the art implementations using accelerators and point out their issues. By re-thinking the organization of modern multifrontal codes with n 'accelerator-first' mindset, we design METAPACK, a system that automatically synthesizes sparse matrix factorization packages for various compute accelerator architectures. We discuss the components at play, from frontend to a novel compiler pass, and how systems research have affected the design. Lastly, we use first computational results to discuss the feasibility of the approach and future of software synthesis in the HPC landscape.
</EventDescription>
    <EventParentName>73378-SESS</EventParentName>
    <EventUniqueID>73378-118804</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Computing with Biologically Inspired Dynamics</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS66</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mihai Petrovici</EventSpeakers>
    <EventSpeakerIDs>802771</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116973</EventHandoutURL>
    <EventDescription>Some neuromorphic devices are most aptly described as neuronal emulators rather than simulators, as they replicate the electrodynamics of cortical cells rather than numerically calculating solutions to the associated differential equations.
Coupled with the discrete, sparse nature of spike-based communication, the advantages of such systems become apparent in their emulation speed and power consumption.
Being designed with a high degree of configurability, many of these devices support a wide range of applications.
Here, we focus on two of these.

First, we discuss the question of credit assignment for precise spike timing and show how synaptic plasticity can implement exact gradient descent in spiking networks.
When applied to, for example, hierarchical neuronal networks, this enables exceptionally fast and efficient neuromorphic solutions to feedforward inference problems such as pattern recognition.

Second, we consider the interpretation of neuronal spikes as samples from some underlying distribution.
With appropriate neuronal parametrization, Hebbian synaptic plasticity can be used to effectively learn arbitrary shapes of this distribution.
We show how a combination of cortical dynamics including spikes, short-term synaptic plasticity and background oscillations enable fast sampling from deep, multimodal distributions, with applications ranging from generative modeling to the representation of entangled quantum systems.
</EventDescription>
    <EventParentName>72861-SESS</EventParentName>
    <EventUniqueID>72861-116973</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Developing Spike-Based Graph Algorithms for Neuromorphic Processors</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS66</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kathleen Hamilton</EventSpeakers>
    <EventSpeakerIDs>802772</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116974</EventHandoutURL>
    <EventDescription>Applications for neuromorphic hardware recast computational tasks in terms of discrete, temporal pulses (or ``spike'), transmitted between multi-level dynamical systems (neurons) via weighted connections (synapses).  The interplay of neuronal and synaptic parameters, combined with external stimuli allow for complex spiking dynamics.  We leverage the structural similarities between a weighted synaptic network, and general network models to design and implement graph-based applications.  In this talk we will present a survey of several applications that have been developed to run on near-term neuromorphic processors.  We focus on analysis of undirected graphs (community detection, centrality measure evaluation) and dynamical processes on undirected graphs (epidemic models and percolation modeling).  </EventDescription>
    <EventParentName>72861-SESS</EventParentName>
    <EventUniqueID>72861-116974</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Interactions Between Task-Based Runtime Systems and the Communication Layer</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS67</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:00:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Philippe Swartvagher</EventSpeakers>
    <EventSpeakerIDs>803041</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117560</EventHandoutURL>
    <EventDescription>With the increasing complexity of heterogeneous supercomputers,
task-based runtime systems have emerged to more easily take benefit of
the full computation power of these machines. For distributed
applications, these runtime systems can handle communications between
nodes in an implicit way by automatically detecting them. This
presentation will focus on the possible interactions between the
task-based runtime system and the communication layer (usually a
third-party library dedicated to high performance communications). Since
the runtime system and the communication library have both informations
about their internal states, there are several opportunities to make
them more collaborate. I will present our last research results,
covering both positive and negative interactions: for instance how
broadcasts can be done efficientely or the possible interferences when
executing in parallel computations and communications.
</EventDescription>
    <EventParentName>73056-SESS</EventParentName>
    <EventUniqueID>73056-117560</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Latest Progress on the PaRSEC Runtime System</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS67</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:30:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:50:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>George Bosilca</EventSpeakers>
    <EventSpeakerIDs>747244</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117719</EventHandoutURL>
    <EventDescription xsi:nil="true" />
    <EventParentName>73056-SESS</EventParentName>
    <EventUniqueID>73056-117719</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Scientific Data Refactoring and Compression on the GPU with MGARD</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS68</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:30:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:50:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ben Whitney</EventSpeakers>
    <EventSpeakerIDs>793097</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118445</EventHandoutURL>
    <EventDescription>Rapid growth in scientific data and a widening gap between computational speed and I/O bandwidth make it increasingly infeasible to store and share all data produced by scientific simulations. Hierarchical data representations hold promise as a solution to this problem, allowing for flexible conversion between different fidelities so that, for example, data can be created at high fidelity and then transferred or stored at lower fidelity via logically simple and mathematically sound operations. The effective use of these representations has been hindered by the relatively high costs of creating, accessing, reducing, and otherwise operating on them. In this talk, we describe MGARD, a multilevel data reduction framework, and present a collection of highly optimized data refactoring GPU kernels that enable efficient creation and manipulation of data in MGARD's hierarchical representation. We demonstrate that our optimized design can achieve up to 250~TB/s aggregated data refactoring throughput---83\% of theoretical peak---on 1024 nodes of the Summit supercomputer.</EventDescription>
    <EventParentName>72875-SESS</EventParentName>
    <EventUniqueID>72875-118445</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Challenges on Global Task-Based Programming Model for “Fugaku” and Beyond</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS70</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:05:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:25:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mitsuhisa Sato</EventSpeakers>
    <EventSpeakerIDs>780223</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118503</EventHandoutURL>
    <EventDescription>Recently, task-based parallel programming model is attracting attention from many researchers working on exascale computing. To realize exascale systems, there are many challenges and issues including architectures and programming models to exploit billions of parallelism. There are two major different approaches for exascale, manycore-based and accelerator-based. OpenMP task is a key to make use of manycore efficiently, and is to be evolved for exploiting global and large parallelisms and integration with communication layers for large scale system such as “Fugaku”. On the other hand, task-based programming model is also useful to combine accelerator devices by offloading functions to the devices as sperate tasks. Future systems beyond exascale are expected to be large-scale, heterogenous and complex integrating many accelerators devices so that these should be programmed by global task-based programming model. We are working on global task-based parallel programming by extending the PGAS programming language XcalableMP to tackle these challenges. In this talk, challenges for global task-based parallel programming models and languages for exascale system and beyond will be addressed with our project.
</EventDescription>
    <EventParentName>73178-SESS</EventParentName>
    <EventUniqueID>73178-118503</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>FIST-HOSVD: Fused Inplace Sequentially Truncated Higher Order Singular Value Decomposition</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS71</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:30:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:50:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5100</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Benjamin Cobb</EventSpeakers>
    <EventSpeakerIDs>803834</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118729</EventHandoutURL>
    <EventDescription>Several novel methods of improving the memory locality of the Sequentially Truncated Higher Order Singular Value Decomposition (ST-HOSVD) algorithm for computing the Tucker decomposition are presented. We show how the two primary computational bottlenecks of the ST-HOSVD can be fused together into a single kernel to improve memory locality. We then extend matrix tiling techniques to tensors to further improve cache utilization. This blocked based approach is then leveraged to drastically reduce the auxiliary memory requirements of the algorithm. Our approach's effectiveness is demonstrated by comparing benchmark results between the traditional ST-HOSVD kernels and our single fused kernel. We then compare single-node CPU runtime results of a ST-HOSVD implementation utilizing our optimizations to TuckerMPI. We demonstrate $\sim2\times$ speedup over the existing state-of-the-art for dense high rank tensors, whilst increasing the problem size that can be computed for a given memory allocation by $\sim3\times$. Finally, we demonstrate our approach's effectiveness with a motivating example of compressing Homogeneous Charge Compression Ignition (HCCI) and Stat Planar (SP) simulation datasets.
</EventDescription>
    <EventParentName>73353-SESS</EventParentName>
    <EventUniqueID>73353-118729</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Lightning Fast Streaming Graph Partitioning for Modern Architectures</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS49</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:30:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christian Schulz</EventSpeakers>
    <EventSpeakerIDs>797540</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117700</EventHandoutURL>
    <EventDescription>Partitioning a graph into balanced blocks such that few edges run between blocks is a key problem for large-scale distributed processing. If the topology of a distributed system is known, it is possible to further optimize the communication costs by mapping blocks onto processing elements. A current trend for partitioning huge graphs are streaming algorithms, which are very fast and have a small memory footprint. These algorithms receive nodes, one at a time, and their neighborhood and directly make a block assignment decision which can not be undone. Up to now, no streaming algorithm maps nodes to processing elements of distributed systems taking into account the different communication speeds of distinct communication links. In this talk, we present a number of improvements to streaming graph partitioning algorithms. We present a streaming multi-recursive partitioning scheme that performs recursive multi-sections on the fly without knowing the overall input graph. This enables us to adopt the scheme to be able to take different communication speeds of a hierarchical compute system into account. Our approach has a considerably lower running time complexity in comparison with state-of-the-art one-pass partitioning algorithms designed for graph partitioning. Our algorithms are up to two orders of magnitude faster than competing algorithms when partitioning inputs into large amounts of blocks while also computing better partitions w.r.t. a process mapping objective.  
</EventDescription>
    <EventParentName>73093-SESS</EventParentName>
    <EventUniqueID>73093-117700</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Composition and DSLs for Finite Element Computation: Pros and Cons</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS50</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:30:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Lawrence Mitchell</EventSpeakers>
    <EventSpeakerIDs>785392</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117776</EventHandoutURL>
    <EventDescription>The Firedrake system uses a multi-layered set of DSLs and
domain-specific optimising compilers to generate (hopefully) high
performance code for finite element assembly. The goal is to deliver
expert-level performance to all users, who may not be experts in the
necessary low level optimisations, and data structure design. In this
talk I will discuss some of the design choices, optimisations and
algorithmic transformations that they enable, and pros and cons of
this generative approach.

</EventDescription>
    <EventParentName>73120-SESS</EventParentName>
    <EventUniqueID>73120-117776</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Composability, Flexibility, and Ease of use for CliMA’s Next Generation Earth System Model</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS50</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Valeria Barra</EventSpeakers>
    <EventSpeakerIDs>803403</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118012</EventHandoutURL>
    <EventDescription>The Climate Modelling Alliance (CliMA) project is developing a full Earth System Model (ESM), entirely in the high-level, dynamic Julia language. The CliMA model has been developed to target both CPU and GPU architectures, using a common codebase. The high-level application programming interface (API) facilitates modularity and composition of differential operators and the definition of flexible discretizations. This, in turn, is coupled with the low-level API that supports different data layouts, specialized implementations and flexible models for threading, to better face high-performance optimization, data storage, and scalability challenges on modern HPC ecosystems. This talk will describe the design of the software and numerical aspects comprising the ClimaCore.jl low-level interface and its high-level counterparts.

</EventDescription>
    <EventParentName>73120-SESS</EventParentName>
    <EventUniqueID>73120-118012</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Speeding-Up and Improving Neptune Simulations with Novel Preconditioners</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS52</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>3000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sue Thorne</EventSpeakers>
    <EventSpeakerIDs>741955</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118088</EventHandoutURL>
    <EventDescription>At the heart of the NEPTUNE Programme is the solution of coupled partial differential equations (PDEs) that exhibit challenging properties when trying to solve them. Following discretisation of the PDE, a large linear system $Ax=b$ must be solved or a series of such large linear systems: the size of the systems to be solved mean that iterative methods are preferable over direct factorisation methods but, without preconditioning to improve rate of convergence, the number of iterations needed to solve the system is too large and it is necessary to stop the iterative process before the required levels of accuracy are met. This loss in accuracy can have a detrimental effect on the convergence properties of any overarching iterative methods that are, for example, dealing with the non-linearity of the PDEs being solved. By using a preconditioner that is suitable for exascale computations, takes into account that the matrix $A$ is not normally available and, instead, the matrix-vector multiplication is provided via a function, is relatively cheap to initialise and apply, and significantly speeds up the convergence towards the solution of $Ax=b,$ simulations should become faster to run and have improved accuracy.

In this talk, we will summarise the approaches that we have trialled, for example, the novel Markov Chain Monte Carlo Matrix Inverse and Implicit-Factorisation preconditioners, and show the successes and the pitfalls of these approaches.
</EventDescription>
    <EventParentName>73205-SESS</EventParentName>
    <EventUniqueID>73205-118088</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Exploring A Domain Specific Language For Performance Portable Particle Based Algorithms in Julia</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS52</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>3000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Will Saunders</EventSpeakers>
    <EventSpeakerIDs>803590</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118388</EventHandoutURL>
    <EventDescription>Here we present an exploratory Domain Specific language (DSL) for describing particle based algorithms. Particle based methods are widely used in many application areas and, in particular, plasma modelling. As part of the ExCALIBUR NEPTUNE project we are interested in the application of Particle In Cell (PIC) methods as a simulation technique for modelling plasma in the edge regions of fusion reactors. Implementing a PIC algorithm for new a hardware architecture requires substantial knowledge in both the physical domain and the computer science domain. However, the Julia programming language has recently emerged as a single source language that offers a solution to the two-language  problem in numerical computing.

In this work we investigate the implementation and extension of an existing abstraction for particle based operations within the Julia language. This abstraction provides a separation of concerns that allows efficient implementation of particle based operations within PIC algorithms without requiring detailed knowledge of the target hardware. We provide an overview of the particular algorithms this DSL is designed to facilitate alongside performance results. In particular we discuss applicability and efficiency of this Julia based approach on the hardware architectures of interest in exascale computing.
</EventDescription>
    <EventParentName>73205-SESS</EventParentName>
    <EventUniqueID>73205-118388</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Dynamic Load Balancing for Irregular Workloads on the GPU</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS53</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:30:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>500</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>John Owens</EventSpeakers>
    <EventSpeakerIDs>747042</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116759</EventHandoutURL>
    <EventDescription>GPUs excel at data analytics problems with ample, regular parallelism. Problems with fine-grained irregular parallelism (where neighboring data elements are assigned different amounts of work), such as those in sparse machine learning and linear algebra, numerical simulation, and graph analytics, are more challenging to map to the GPU.

Today's best GPU implementations of irregular-parallel problems employ sophisticated low-level primitives to map irregular amounts of work to the GPU's compute units. Generally, these implementations build application-specific load-balancing techniques that are tightly coupled with application logic. The result is complex code whose load-balance capabilities cannot easily be used in other applications.

We describe our implementation of a standalone fine-grained load-balancing framework for GPUs that can address these irregular problems. In our work, we focus on two primary problems: (1) an abstraction that eases programmer complexity by separating the concerns of load balancing from work processing, and (2) interfaces that enable programmers to target load-balanced applications, load-balanced kernel launches, and/or in-kernel load-balancing collectives.
</EventDescription>
    <EventParentName>72776-SESS</EventParentName>
    <EventUniqueID>72776-116759</EventUniqueID>
    <STATUS>inactive</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Delegate Centric Top-K Computation on GPUs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS53</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>500</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hang Liu</EventSpeakers>
    <EventSpeakerIDs>802671</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116761</EventHandoutURL>
    <EventDescription>Recent top-k computation projects explore the possibility of revising various sorting algorithms to answer top-k queries on GPUs.These endeavors, unfortunately, perform significantly more work than needed. This talk will present a Delegate-centric top-k system on GPUs that can reduce the top-k workloads significantly. Particularly, we will introduce a comprehensive design of the delegate-centric concept, including maximum delegate, top-k delegate-based filtering, and ß delegate mechanisms to help reduce the workload for top-k by more than 99%. We will also introduce four key designs to enable fast multi-GPU.</EventDescription>
    <EventParentName>72776-SESS</EventParentName>
    <EventUniqueID>72776-116761</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Co-Designing Data Flow Accelerators</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS54</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>801</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sivasankaran Rajamanickam</EventSpeakers>
    <EventSpeakerIDs>790203</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118600</EventHandoutURL>
    <EventDescription>
Several emerging accelerators have characteristics that are attractive for use cases in ML for science domain, traditional machine learning use cases, and in scientific simulations. As these accelerators are being developed, co-design of algorithms, architectures, and applications could lead to better hardware and software. This talk will highlight several recent work that is focused on this co-design effort. 

We will introduce a suite of mini-applications that will help in the co-design effort called Mantevo-DF focused on data flow focused use cases for scientific simulations. We will also introduce machine learning use cases that are important for ML for science use cases and their uses on modern accelerator hardware.

Finally, we will set the stage for the rest of this three part mini-symposium by describing the open challenges in co-design efforts and how the rest of the talks connect to these co-design efforts.
</EventDescription>
    <EventParentName>73325-SESS</EventParentName>
    <EventUniqueID>73325-118600</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>COMET: A Compiler Framework for Next-Generation Heterogenous Systems</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS54</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>801</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Gokcen Kestor</EventSpeakers>
    <EventSpeakerIDs>796150</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118791</EventHandoutURL>
    <EventDescription>As Moore’s Law is coming to an end, simple technology scaling cannot be relied on for performance gain, and new technologies and computing paradigms must be developed to 
achieve application performance, which is leading to an explosion of accelerator designs from both industry and academia. Leveraging the new types of compute resources will require new methods and tools to not only achieve high-performance on current heterogeneous systems but also 
interface with hardware performance modeling and simulation tools to estimate application performance on future specialized accelerators.
Compiler technologies have considerably evolved during the last decades and now support sophisticated code analysis and translation methodologies as well as 
efficient code generation for target heterogeneous systems. 
In the context of co-design, a compiler-based tool is desirable because it offers the possibility of automatically generate code that leverages 
new hardware concepts without or with little code modifications in the applications. 
In this talk, I present COMET tensor algebra compiler targeting
quantum chemistry and graph analytics applications. 
COMET provides an opportunity to perform hardware/software co-design and design space exploration efficiently and 
to assess the performance of the entire application, instead of only the innermost kernel.
</EventDescription>
    <EventParentName>73325-SESS</EventParentName>
    <EventUniqueID>73325-118791</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Multi-Scale Modeling in Storm Surge Applications with an Eye Towards HPC Environments</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS55</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:50:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kyle Mandli</EventSpeakers>
    <EventSpeakerIDs>723558</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118635</EventHandoutURL>
    <EventDescription>Coastal hazards related to strong storms are one of the most frequently recurring and widespread hazards to coastal communities today.  In particular, storm surge, the rise of the sea surface responding to wind and pressure forcing from these storms, can have a devastating effect on the coastline.  Furthermore, with the addition of climate change related effects, the ability to predict these events quickly and accurately is critical to the protection and sustainability of these coastal areas.

This talk will focus on the description and approach to the optimization of coastal protection in the presence of climate change impacts.  This includes a multimodel approaches, the use of multiscale techniques to represent difficult to resolve features, and reduce order models for uncertainty quantification.</EventDescription>
    <EventParentName>73337-SESS</EventParentName>
    <EventUniqueID>73337-118635</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Exahype 2 Finally Solve Some Astrophysics Problems</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS55</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tobias Weinzierl</EventSpeakers>
    <EventSpeakerIDs>762655</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118637</EventHandoutURL>
    <EventDescription>ExaHyPE is an engine to simulate hyperbolic equation systems given in first order formulation. 
A fundamental idea behind ExaHyPE - currently written in its second generation - is the strict
separation of what, how, when, where: Users specify their physics via a few PDE terms, while the
engine decides how to translate these terms into a numerical scheme, when to launch the individual
steps of this scheme, and where to execute the computation's steps. The focus on particular 
numerical schemes, i.e. this particular software design, distinguishes ExaHyPE from other solutions in the field and facilitates a 
tight co-orchestration and tuning of all engine ingredients - the AMR, the timestepping, the 
I/O, and so forth.

We give a tour de force through ExaHyPE, which can be controlled exclusively through a Python
(code generation) interface, and discuss some particular HPC techniques under the hood: The 
underlying domain decomposition, ExaHyPE's task formalism, and its ability to deploy tasks to 
accelerators. Eventually, we will demonstrate how to use the code to simulate a binary black
hole merger.
</EventDescription>
    <EventParentName>73337-SESS</EventParentName>
    <EventUniqueID>73337-118637</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>High-Order Matrix-Free GPU-Accelerated Linear Solvers with Applications to Fluid Flow</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS55</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Will Pazner</EventSpeakers>
    <EventSpeakerIDs>774866</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118647</EventHandoutURL>
    <EventDescription>In this talk, we will present a GPU-accelerated incompressible flow solver using a high-order finite element discretization. As in most projection-type methods, solving a Poisson problem for the pressure at every time step represents the dominant computational cost. We will present matrix-free linear solvers for the ill-conditioned linear systems that result from high-order finite element discretizations. At high orders, the cost of assembling the system matrix becomes infeasible, necessitating the use of matrix-free preconditioners that can be built without access to the matrix entries. In this talk, we will consider several matrix-free approaches, including $hp$-multigrid and low-order refined preconditioning. Both the application of the preconditioners and the matrix-free operator evaluation are performed on the GPU.
</EventDescription>
    <EventParentName>73337-SESS</EventParentName>
    <EventUniqueID>73337-118647</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Code Generation for Matrix-Free Iterative Linear Solvers</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS59</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Peter Bastian</EventSpeakers>
    <EventSpeakerIDs>724709</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117783</EventHandoutURL>
    <EventDescription>Fast matrix-free iterative linear solvers play an important role in
high-performance simulations on modern computer architectures because
they allow to overcome the bandwidth limitation of iterative solvers
based on assembled matrices. High compute intensity at reduced
computational complexity can be achieved in the context of
high-order finite element discretizations by exploiting 
tensor product structure of the finite element basis. 
Robustness of iterative solvers with respect to the mesh size
as well as model coefficients is achieved by a two-level preconditioner
combining a local matrix-free preconditioner on the high-order part
with global matrix-based preconditioners in a low-order subspace.

In this talk we focus on code-generating all required components of such
preconditioners for high-order discontinuous Galerkin discretizations.
These components are matrix-free operator application, matrix-free
block SSOR smoothers as well matrix generation for the low-order subspace.
For all components it is possible to generate high-performance code
specifically tailored to the current kernel and hardware through
SIMD vectorization and code transformations based on auto-tuning.
As an example application we consider viscous fingering in miscible
displacement with operator splitting techniques.
</EventDescription>
    <EventParentName>73121-SESS</EventParentName>
    <EventUniqueID>73121-117783</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Transforming (not just) DG-FEM Array Expressions (not just) on GPUs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS59</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:50:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kaushik Kulkarni</EventSpeakers>
    <EventSpeakerIDs>785013</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117792</EventHandoutURL>
    <EventDescription>While domain-specific languages for finite-element-based PDE solvers have
enjoyed considerable success, composing them with supporting computations (e.g.
chemistry, neural networks, non-variational discretization schemes) has
proved challenging.

We demonstrate a fully generic array package with numpy-like semantics that is
able to represent and transform the action of complex discontinuous Galerkin
(DG) FEM operators for near-roofline performance while straightforwardly
incorporating non-FEM computations on the degrees of freedom. Our array package
(“pytato”) can straightforwardly be extended to transform other workloads and
allows code generation into an imperative, scalar, polyhedrally-based
Intermediate Representation (IR), based on our “Loopy” tool for loop
transformations.

With our implementation, we also provide a transformation pipeline tuned for a
general DG-FEM operator evaluation. Based on the metadata attached to various
IRs in the lowering pipeline, optimization passes such as kernel fusion, loop
fusion/distribution, common sub-expression elimination and loop tiling get
triggered.



</EventDescription>
    <EventParentName>73121-SESS</EventParentName>
    <EventUniqueID>73121-117792</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Acyclic Partitioning for Mapping and Scheduling</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS60</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yusuf Ozkaya</EventSpeakers>
    <EventSpeakerIDs>803104</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117706</EventHandoutURL>
    <EventDescription>With the disproportional improvements of computational and communication efficiency in computing architectures, data movement remains as the most important challenge in efficient execution. A good trade-off between load balance and data locality is necessary when scheduling computations. Directed Acyclic Graphs (DAGs) have been the de-facto abstraction to model computational tasks and their dependencies. Acyclicity provides many benefits on the ordering and mapping of vertices of the DAG (i.e., tasks) to computational resources. Thus, it is desirable to preserve acyclicity at different granularities of computation. In this talk, we show how acyclic partitioning of DAGs, i.e. partitioning in which the inter-part edges between the vertices from different parts should preserve an acyclic dependency structure among the parts, can be explored to minimize redundant data movement in two different computational models: on a single processor with two-level memory setting and on distributed memory setting. We demonstrate the usefulness of our acyclic partitioning using a high-performance distributed quantum circuit simulation application. To address the limitations of the graph model, we also develop and analyze the performance of acyclic partitioning approaches for directed hypergraphs and discuss and evaluate the scenarios where each can be preferable.
</EventDescription>
    <EventParentName>73094-SESS</EventParentName>
    <EventUniqueID>73094-117706</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Toward Performance-Portable PETSc for GPU-Based Exascale Systems</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS61</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:50:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Richard Mills</EventSpeakers>
    <EventSpeakerIDs>732925</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118697</EventHandoutURL>
    <EventDescription>The Portable Extensible Toolkit for Scientific computation (PETSc) library provides scalable solvers for nonlinear time-dependent differential and algebraic equations and for numerical optimization. The PETSc design for performance portability addresses fundamental GPU accelerator challenges and stresses flexibility and extensibility by separating the programming model used by the application from that used by the library, enabling application developers to use their preferred programming model, such as Kokkos, RAJA, SYCL, HIP, CUDA, or OpenCL, on upcoming exascale systems. We will describe some of the major challenges GPUs pose for scientific software libraries, discuss design decisions we have made to address these, provide a blueprint for using GPUs from PETSc-based codes, and present case studies emphasizing the flexibility and high performance achieved on current GPU-based systems.  Additionally, we will discuss recent developments in PETSc's communication module, PetscSF, that enable flexibility and scalable performance on GPU-based systems.</EventDescription>
    <EventParentName>72874-SESS</EventParentName>
    <EventUniqueID>72874-118697</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>The Open Science Pool – An OSG DHTC Service for the S&amp;E Community</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS63</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Miron Livny</EventSpeakers>
    <EventSpeakerIDs>802731</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116861</EventHandoutURL>
    <EventDescription>Researchers who harness the capacity of the OSG operated Open Science Pool (OSPool) are likely to have access to accounts on campus clusters, allocations at national facilities, or CloudBank accounts.  When they place their HTC workload on an Access Point provided by the Partnership to Advance Throughput Computing (PATh) project, they would like to steer the execution of some or all the jobs in the workload to these resources. Doing so improves the throughput of the workload and can address the needs of jobs with special resource requirements like large memory or long execution times.   PATh has been developing technologies and services that allow a researcher to Bring Your Own Resource (BYOR) to the Access Point and to direct the matching of jobs to resources. We will present the capabilities available today and outline our future plans to support BYOR for HTC workloads. Provisioning and scheduling of BYORs is a major challenge as mistakes can be extremely costly to the researcher. 
</EventDescription>
    <EventParentName>72771-SESS</EventParentName>
    <EventUniqueID>72771-116861</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Handling Runtime Variability at Scale: Elasticity for Continuously Changing Graphs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS64</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kasimir Gabert</EventSpeakers>
    <EventSpeakerIDs>790678</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118800</EventHandoutURL>
    <EventDescription>Today's massive data are increasingly represented as graphs.
Trillions of edges are common resulting in a growing importance for distributed graph systems.

Most large graphs do not appear overnight and remain static.
Instead, they are built up over time from continuous processes, e.g., users visiting web pages, product purchases, vehicles traveling on roads, and more.
Such changing graphs are called dynamic and their algorithms operate on batches of changes instead of re-computing from scratch.

An understudied but crucial property of dynamic graph systems is their high runtime variability, arising from three main areas:
the graph change rate, e.g., web traffic increases during shopping holidays;
the per-batch processing time, which can vary from constant to the full cost of re-computation;
and the user query rate, e.g., hotspots on popular queries.
High variability limits dynamic graph use for interactive applications.

ElGA is a recent high-performance elastic, dynamic graph system.
In this talk, we extend ElGA to address per-batch variability.
We develop heuristics to predict batch runtime and then preemptively, automatically, scale out.
We reactively adjust after observing iteration runtimes.
This reduces runtime variability, enabling previously out-of-reach interactive applications on massive, dynamic graphs.

SNL is managed and operated by NTESS under DOE NNSA contract DE-NA0003525.</EventDescription>
    <EventParentName>73377-SESS</EventParentName>
    <EventUniqueID>73377-118800</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Beyond a Hybrid Parallel GraphBLAS</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS65</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:05:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:25:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Albert Jan Yzelman</EventSpeakers>
    <EventSpeakerIDs>804303</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119070</EventHandoutURL>
    <EventDescription>GraphBLAS is a recent standard for writing graph algorithms in the language of linear algebra. While it has an accepted C API, we previously presented an alternative specification in line with standard C++ making use of its more recent features. Additionally, we added the notion of performance semantics and backends. An auto-vectorising and auto-parallelising implementation runs on both shared- and distributed-memory architectures, on Intel and ARM.

In this talk we refine the notion of performance semantics, present a mechanism that ensures interoperability with existing parallel frameworks such as MPI or Spark, and explore how GraphBLAS relates to existing programming models that have been highly successful within data science, such as MapReduce or vertex-centric programming. In particular, we shall demonstrate how with minor extensions that remain truthful to the idea of programming using algebraic concepts, a thus-extended GraphBLAS in fact generalizes some of these earlier well-known programming models. Whether such an `algebraic programming model' could form a basis of workloads beyond just graph computing depends also on its performance, which we shall present last.</EventDescription>
    <EventParentName>73378-SESS</EventParentName>
    <EventUniqueID>73378-119070</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Hand Tuned Spiking Networks for Composition</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS66</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>James Plank</EventSpeakers>
    <EventSpeakerIDs>802770</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116972</EventHandoutURL>
    <EventDescription>This work focuses on composing small spiking neural network to create larger neural networks.  The motivation is to ease the burden on training, by either hand-tuning networks or training small networks with machine learning and then composing them for larger networks.  We present results in both categories.  </EventDescription>
    <EventParentName>72861-SESS</EventParentName>
    <EventUniqueID>72861-116972</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>The Chunks and Tasks Programming Model</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS67</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:05:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:25:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Emanuel Rubensson</EventSpeakers>
    <EventSpeakerIDs>750626</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117718</EventHandoutURL>
    <EventDescription>We present Chunks and Tasks, a programming model that makes it easier to parallelize dynamic hierarchical algorithms on computer clusters with distributed memory. Contrary to conventional programming models there are no explicit communication calls in user code and the library takes care of the distribution of both work and data. We showcase what can be achieved with the model by looking at the example of parallelization of sparse matrix operations such as matrix-matrix multiplication and inverse factorization. We present recent changes to the Chunks and Tasks C++ interface that simplifies the handling of hierarchical data for the user, by moving responsibilities to the runtime library.
</EventDescription>
    <EventParentName>73056-SESS</EventParentName>
    <EventUniqueID>73056-117718</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Teaching Weather(ed) Code New Languages: DSLs and Source-to-Source Translation for ECMWF's Integrated Forecasting System</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS73</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:30:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:50:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Balthasar Reuter</EventSpeakers>
    <EventSpeakerIDs>798614</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118765</EventHandoutURL>
    <EventDescription>The need to fully exploit the advantages of modern accelerators in HPC architectures plays a vital role in achieving the next step change in predictive skills for weather and climate models. However, the diversification of HPC hardware continues and incurs a recurring need to adapt and transform existing model software to new programming models, which is becoming increasingly impractical using a single code base.

All current strategies tackling this issue involve some form of abstraction that separates numerical algorithms from hardware-specific implementation. ECMWF is aiming to achieve both compatibility and performance portability for accelerators and future architectures through a combination of DSLs, domain-specific libraries and bespoke source-to-source translation tools that allow an incremental adaptation of the code base alongside scientific development.

In this talk we will describe algorithmic components of the Integrated Forecasting System (IFS) and highlight key elements of our roadmap for adapting them to GPU architectures. We discuss how a combination of flexible data structures, an extended use of library interfaces and the use of source-to-source translation tools is envisaged to allow the incremental adaptation of the code base. The second part will focus on Loki, an in-house developed tool that allows bespoke source-to-source
transformations to be devised to adapt individual model components to novel programming paradigms or community-driven DSLs. </EventDescription>
    <EventParentName>73373-SESS</EventParentName>
    <EventUniqueID>73373-118765</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Diagonal SDC Preconditioning</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS74</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:55:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ruth Schöbel</EventSpeakers>
    <EventSpeakerIDs>781852</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118642</EventHandoutURL>
    <EventDescription>The spectral deferred correction method (SDC) is an iterative solver for time-dependent differential equations and can be interpreted as a preconditioned Picard iteration for the collocation problem. The key to an efficient iteration process is the choice of the preconditioner that defines the speed of convergence. While the de-facto standard choice is a fast-converging, yet serial preconditioner, our goal is to find a fast-converging, diagonal and therefore parallel one. 

Depending on the parameters of the equation at hand, we first try to find “optimal” diagonal preconditioners for Dahlquist’s equation. Using spectral methods, we then apply these results to partial differential equations with a linear stiff part that we treat in a decoupled and implicit way with the previously optimized parallel preconditioners, whereas nonlinear parts are treated explicitly in real space. 

In this talk we present different approaches to find such optimal diagonal preconditioners, including mathematical optimization and reinforcement learning. We shed light on the pros and cons of these different strategies and show results for linear and nonlinear PDEs.

</EventDescription>
    <EventParentName>73332-SESS</EventParentName>
    <EventUniqueID>73332-118642</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Understanding the Design Space of Sparse/Dense Multiphase Dataflows for Mapping Graph Neural Networks on Spatial Accelerators</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS76</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:55:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>801</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Raveesh Garg</EventSpeakers>
    <EventSpeakerIDs>804284</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119055</EventHandoutURL>
    <EventDescription>Graph Neural Networks (GNNs) have garnered a lot of recent interest because of their success in learning representations from graph-structured data across several critical applications in cloud and HPC. Owing to their unique compute and memory characteristics that come from an interplay between dense and sparse phases of computations, the emergence of reconfigurable dataflow (aka spatial) accelerators offers promise for acceleration by mapping optimized dataflows (i.e., computation order and parallelism) for both phases. The goal of this work is to characterize and understand the design-space of dataflow choices for running GNNs on spatial accelerators in order for the compilers to optimize the dataflow based on the workload. Specifically, we propose a taxonomy to describe all possible choices for mapping the dense and sparse phases of GNNs spatially and temporally over a spatial accelerator, capturing both the intra-phase dataflow and the inter-phase (pipelined) dataflow. Using this taxonomy, we do deep-dives into the cost and benefits of several dataflows and perform case studies on implications of hardware parameters for dataflows and value of flexibility to support pipelined execution.</EventDescription>
    <EventParentName>73326-SESS</EventParentName>
    <EventUniqueID>73326-119055</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Formulation and Solvers for Kinetic-Fluid Models for Plasma Simulation</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS77</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:05:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:25:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Luis Chacon</EventSpeakers>
    <EventSpeakerIDs>730580</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118424</EventHandoutURL>
    <EventDescription>We review recent progress on the formulation and numerical solution of kinetic-fluid models and algorithms for kinetic plasma simulation. The models we are interested in combine fluid and kinetic descriptions, either from a physics perspective (e.g., hybrid methods) or from an algorithmic one (e.g., hierarchical multi-model solvers), or both. Kinetic representations are characterized by an augmented (in principle, up to 6D) phase-space representation, which includes a configuration space and a velocity space, the latter discretized either with particles or on a mesh. We will touch upon different strategies for the efficient nonlinear solution of these systems such as linear moment-based preconditioning, and nonlinear Picard-based iterative solvers exploiting high-order/low-order (HOLO) hierarchical decompositions. We will demonstrate the effectiveness of our formulations with various applications of interest.</EventDescription>
    <EventParentName>73283-SESS</EventParentName>
    <EventUniqueID>73283-118424</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Coupling Kinetic PIC to Multi-Fluid Plasma Models using Compatible Discretizations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS77</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:55:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Eric Cyr</EventSpeakers>
    <EventSpeakerIDs>735268</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118426</EventHandoutURL>
    <EventDescription>This talk investigates coupling between a particle-in-cell (PIC) plasma model and a multi-fluid continuum treatment. A central obstacle to this is evolving the stiff time scales that arise, while maintaining critical properties, like no magnetic monopoles and a discrete variant of Gauss' law. Our approach is to develop a DG discretization of the fluid equations coupled to the particles through collisional interactions, and indirectly through current coupling into Maxwell’s equations. For the later coupling, a compatible discretization is used to evolve Maxwell’s equations. This talk will demonstrate how these discretizations can be coupled while preserving the Gauss’ law involutions, and implicitly integrating the implicit modes, including the plasma and cyclotron frequencies. A discussion of the parallel solution of the systems will be included to demonstrate scalability.
</EventDescription>
    <EventParentName>73283-SESS</EventParentName>
    <EventUniqueID>73283-118426</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Scaling Data Parallel Training with PyTorch</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS80</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Shen Li</EventSpeakers>
    <EventSpeakerIDs>803843</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118737</EventHandoutURL>
    <EventDescription>Data parallelism has emerged as a popular solution for distributed training due to its broad applicability. As of v1.11, PyTorch offers three main data-parallel training paradigms, namely, DistributedDataParallel (DDP), Pipelined Data Parallelism (PDP), and FullyShardedDataParallel (FSDP). DDP replicates the model on every device to independently generate gradients and then communicates those gradients at each iteration to keep model replicas consistent. To accelerate training, DDP organizes gradients into buckets and overlaps backward computation with gradient communication. With the increase of model size, a single device might no longer host the entire states of the model and the optimizer replicas due to device memory capacity. To reduce per-device memory consumption, PDP partitions the model across multiple devices on the same machine and applies data parallelism across pipelines; and FSDP shards the states across all devices and only fetch states on-demand before usage. This presentation covers technical details and comprehensive comparisons of the three data-parallel training paradigms.
</EventDescription>
    <EventParentName>73367-SESS</EventParentName>
    <EventUniqueID>73367-118737</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Structured Matrix Approximations via Tensor Decompositions</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS81</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:40:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5100</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Arvind Saibaba</EventSpeakers>
    <EventSpeakerIDs>798231</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118734</EventHandoutURL>
    <EventDescription>We provide a computational framework for approximating a class of structured matrices (e.g., block Toeplitz, block banded). Our approach has three steps: map the structured matrix to tensors, use tensor compression algorithms, and map the compressed tensors back to obtain two different matrix representations --- sum of Kronecker products and block low-rank format. The use of tensor decompositions enable us to uncover latent structure in the matrices and lead to computationally efficient algorithms. The resulting matrix approximations are memory efficient, easy to compute with, and preserve the error due to the tensor compression in the Frobenius norm. While our framework is quite general, we illustrate the potential of our method on structured matrices from three applications: system identification, space-time covariance matrices, and test matrices from SuiteSparse collection. See preprint: https://arxiv.org/abs/2105.01170.</EventDescription>
    <EventParentName>73354-SESS</EventParentName>
    <EventUniqueID>73354-118734</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Learning Solution Adaptive Discretization Stencils from Data</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS82</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Dominik Sturm</EventSpeakers>
    <EventSpeakerIDs>804231</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118268</EventHandoutURL>
    <EventDescription>Numerical methods for approximately solving partial differential equations (PDEs) are at the core of scientific computing. Often, this requires high-resolution or solution-adaptive discretizations in order to capture spatio-temporal features in the PDE solution. Systematically deriving solution-adaptive operator stencils is a current challenge and moreover requires one to know the PDE. In this talk, we present the STENCIL-NET, an artificial neural network for data-driven learning of problem- and resolution-specific local discretizations of unknown nonlinear PDEs. STENCIL-NET achieves numerically stable discretization of an unknown nonlinear PDE by spatially and temporally adaptive parametric pooling on regular Cartesian grids, and by incorporating knowledge about discrete time integration. Knowing the actual PDE is not necessary, as solution data is sufficient to train the network to learn the stencils. A once-trained STENCIL-NET can be used to predict solutions of the PDE on larger spatial domains and for longer times than it was trained for, addressing the problem of PDE-constrained extrapolation from data. We present numerical experiments on long-term forecasting of chaotic PDE solutions on coarse grids. We also present preliminary results on using differentiable convex layers to impose discrete stencil moment constraints, guaranteeing numerical consistency of the learned stencils.
</EventDescription>
    <EventParentName>73240-SESS</EventParentName>
    <EventUniqueID>73240-118268</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Parallel Implementation of FFT in a Finite Field</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS83</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daisuke Takahashi</EventSpeakers>
    <EventSpeakerIDs>83250</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117333</EventHandoutURL>
    <EventDescription>In this talk, we propose a parallel implementation of fast Fourier transform (FFT) in a finite field.
The butterfly operation of the FFT in a finite field includes modular multiplication, which can be computed faster by using Montgomery multiplication.
We vectorized the butterfly operations using SIMD instructions and parallelized them using OpenMP.
Performance results of parallel FFT in a finite field on manycore processors are reported.</EventDescription>
    <EventParentName>72986-SESS</EventParentName>
    <EventUniqueID>72986-117333</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Comparison of Parallel Profiling Tools for Programs Utilizing the FFT</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS83</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:40:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Samar Aseeri</EventSpeakers>
    <EventSpeakerIDs>747261</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117335</EventHandoutURL>
    <EventDescription>The overhead of the performance monitoring tools Craypat, FPMP, mpiP, Scalasca and TAU, are measured using default configurations likely to be choosen by a novice user and shown to be small when profiling Fast Fourier Transform based solvers for the Klein Gordon equation based on 2decomp&amp;FFT and on FFTE. Performance measurements help explain that despite FFTE having a more efficient parallel algorithm, it is not always faster than 2decom&amp;FFT because the complied single core FFT is not as fast as that in FFTW which is used in 2decomp&amp;FFT.

This study was conducted on a Cray XC40 Shaheen II and will be implemented on Fugaku supercomputer as well and results will be displayed in this talk with the behaviour comparison of both platforms.</EventDescription>
    <EventParentName>72986-SESS</EventParentName>
    <EventUniqueID>72986-117335</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>High-Order Accurate Embedded-Boundary Discontinuous Galerkin Methods for High-Performance Computing Applications</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS84</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  3:05:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:25:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Vincenzo Gulizzi</EventSpeakers>
    <EventSpeakerIDs>795483</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118650</EventHandoutURL>
    <EventDescription>This talk presents a numerical framework for high-performance computing applications based on high-order accurate embedded-boundary methods.
The framework is based on discontinuous Galerkin formulations and suitable quadrature rules for embedded geometries, whose combined use allows resolving the curved geometries as well as solving the governing equations and enforcing boundary and interface conditions with high-order accuracy.
The method uses block-structured Cartesian grids where the geometries are represented implicitly via a level set function.
The intersection between the grids and the level set function generates the implicitly defined mesh, which consists of a collection of regular (hyper)rectangular cells plus a relatively small number of irregular cells cut by the embedded boundaries.
High-order accuracy is achieved by using high-order quadrature rules for implicitly-defined domains and boundaries, while a cell-merging strategy addresses the small-cell problem.
The space-discretized equations are then advanced in time using explicit high-order Runge-Kutta algorithms.
The framework is implemented within AMReX, a software library that provides functionalities to write massively parallel applications using adaptive mesh refinement.
Numerical test cases are presented in the areas of wave propagation and computational fluid dynamics and show the performance of the framework using various parallelization paradigms including general purpose GPU computing.
</EventDescription>
    <EventParentName>73339-SESS</EventParentName>
    <EventUniqueID>73339-118650</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Magma: Towards High Performance and Cross Platform Algorithms for Numerical Linear Algebra</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS85</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:40:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Stanimire Tomov</EventSpeakers>
    <EventSpeakerIDs>719886</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117943</EventHandoutURL>
    <EventDescription>MAGMA is an open source library for dense and sparse linear algebra algorithms on heterogeneous systems with GPUs. It provides many important algorithms such as optimized BLAS and LAPACK for GPUs, including dense matrix factorizations, linear system solvers, least square problems, eigenvalue problems, batch linear algebra, as well as sparse iterative solvers. With an emphasis on performance, MAGMA provides many routines that often outperform those provided by the vendor's numerical software. This talk provides an overview of the latest heterogeneous computing algorithms in MAGMA, and highlights the recent efforts for adding support and optimizations for AMD GPUs. The talk also presents preliminary performance results of new developments that are lined up for future releases.</EventDescription>
    <EventParentName>73179-SESS</EventParentName>
    <EventUniqueID>73179-117943</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Meeting the Computational Challenges of Giant Optical Telescopes' Real-time Control</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS86</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hatem Ltaief</EventSpeakers>
    <EventSpeakerIDs>735518</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117720</EventHandoutURL>
    <EventDescription>We present a distributed-memory implementation of a real-time controller for giant optical telescopes. Based on task-based programming models and powered by dynamic runtime systems, we leverage the computational power of the underlying massively parallel hardware resources and scale the computational astronomy application to an unprecedented level. The newly developed numerical algorithm increases the performance of the adaptive optics control loop by continuously outsmarting the atmospheric turbulence in real-time. This hardware-software solution is essential to obtain sharp images with current and future giant telescopes under construction. It may eventually help to better detect planets around other stars by pushing the limits of the largest astronomical telescopes on Earth.
</EventDescription>
    <EventParentName>73057-SESS</EventParentName>
    <EventUniqueID>73057-117720</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Template Task Graph (TTG): Flow Graph Programming for Fine-Grained Distributed Memory Parallelism</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS86</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:40:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Edward Valeev</EventSpeakers>
    <EventSpeakerIDs>803122</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117722</EventHandoutURL>
    <EventDescription>TTG is a general-purpose programming model for shared and distributed memory parallel computers. It is particularly suited for expressing irregular data-driven computation patterns (numerical calculus
on adaptively refined meshes, block sparse tensor algebra, and others) that appear in many domains of science and engineering, TTG represents a program as a graph of template tasks (hence the name); flow of
{key,value} pairs over the graph generates tasks that are executed by a low-level task runtime. By abstracting the low-level task runtime TTG aims to offer high-level declarative composition with portable execution
on distributed memory and heterogeneous platforms. This talk will highlight the initial implementation of TTG (as a C++17 library, abstracting PaRSEC and MADNESS distributed-memory tasking runtimes) and its
use for implementing prototypical algorithms in linear algebra and numerical calculus.</EventDescription>
    <EventParentName>73057-SESS</EventParentName>
    <EventUniqueID>73057-117722</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Floating Point Overflow and the Parallel Solution of Triangular Equations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS86</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  3:05:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:25:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Carl Christian Kjelgaard Mikkelsen</EventSpeakers>
    <EventSpeakerIDs>803490</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118151</EventHandoutURL>
    <EventDescription>This talk is relevant to anyone interested in the development of libraries for dense numerical linear algebra. Standard linear systems, nonsymmetric standard and generalized eigenvalue problems and Sylvester matrix equations can all be solved by reducing the matrices to triangular form. Many algorithms for solving triangular equations are based on substitution. Unfortunately, substitution is very vulnerable to floating point overflow and more so when the representational range is reduced compared with IEEE double precision floating point arithmetic. LAPACK realizes a family of algorithms for solving triangular equations without suffering from floating point overflow. The subroutines are all sequential scalar codes that can only achieve a very a small fraction of the peak flop rate. Recently, we have developed algorithms that are parallel and blocked. In this talk we exhibit well-conditioned triangular equations for which the solution exceeds the representational range. We explain how to prevent floating point overflow without sacrificing performance. The underlying principles have been integrated into the task-parallel library StarNEig for solving dense nonsymmetric standard and generalized eigenvalue problems.</EventDescription>
    <EventParentName>73057-SESS</EventParentName>
    <EventUniqueID>73057-118151</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Exploiting Parallelism in Large Scale Deep Learning Model Training: From Chips to Systems to Algorithms</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS88</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:40:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>801</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Saurabh Kulkarni</EventSpeakers>
    <EventSpeakerIDs>804331</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119102</EventHandoutURL>
    <EventDescription>We live in a world where hyperscale systems for machine intelligence are increasingly being used to solve complex problems ranging from natural language processing to computer vision to molecular modeling, drug discovery and recommendation systems etc. A convergence of breakthrough research in machine learning models and algorithms, increased accessibility of hardware systems at cloud scale for research and thriving software ecosystems are paving the way for an exponential increase in model sizes. Effective parallel processing techniques and large clusters of accelerators will be required to train these models economically.
What are some of the scale challenges we face, given the enormous cost and time to train tera-scale models? What are the compute, memory and networking requirements to implement these models? How can a novel approach with fine-grained parallelism at the hardware level help? What kind of model decomposition techniques are required to exploit the innate parallelism in deep learning models?	
Attend this session to learn about how Graphcore aims to address these challenges. Get to know our Intelligent Processing Unit (IPU) - a purpose-built hardware accelerator with a unique MIMD architecture - designed to address the most demanding compute and memory bandwidth needs of modern ML models. Our network disaggregated architecture uniquely positions us to build highly scalable systems (IPU-PODs) with thousands of accelerators aimed at exploiting parallelism.
</EventDescription>
    <EventParentName>73327-SESS</EventParentName>
    <EventUniqueID>73327-119102</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Experiments Towards Parallel Adaptive In-Situ Visualization</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP0</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  6:00:00 PM</EventStartTime>
    <EventEndTime>02/24/22  8:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tim Griesbach</EventSpeakers>
    <EventSpeakerIDs>803807</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118696</EventHandoutURL>
    <EventDescription>The visualization of simulation data that arises from large-scale numerical simulations is a challenging task. Visualization as post-processing step has the disadvantage of writing large amounts of data to disk, which can be slow to the point of impracticality. Relying on a dedicated third-party library for visualization often incurs duplicating data or converting it to a prescribed external data structure, as well as a sizable increase in code, executable, and memory complexity.
We develop algorithms to visualize the simulation data using the simulation data structure. Specifically, we work with the widely known distributed adaptive octree structure and extend it to support in-situ visualization. One current approach of ours is to revive the radiosity method, which requires only a re-projection of the radiosity results to visualize the scene from a different view point in the rendered scene. Radiosity works by computing the pairwise light transfer between surface patches in a scene, which we determine by enforcing strict local radiation balance. Our algorithm exploits the octree-based data structure by using recursive top-down search algorithms and recursively excluding non-visible surface patches. The AMR data is distributed using the Morton space-filling curve, which we use to parallelize the radiosity system setup and solver. We present the current state of our research, a parallel and natively supported radiosity solver operating on a distributed octree data structure.</EventDescription>
    <EventParentName>73765-SESS</EventParentName>
    <EventUniqueID>73765-118696</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Intelligently Offloading Tasks to Intelligent Hardware</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP0</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  6:00:00 PM</EventStartTime>
    <EventEndTime>02/24/22  8:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mark Turner</EventSpeakers>
    <EventSpeakerIDs>803917</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118806</EventHandoutURL>
    <EventDescription>Many HPC applications suffer from bandwidth and latency constraints as well as lacking message progression. These problems become particularly severe for sophisticated scientific codes with tasks, multiscale methods, adaptive meshes, multiphysics, multi-numerics, and so forth, as they induce non-deterministic, non-homogeneous data exchange. Smart network devices can serve as a man-in-the-middle between the CPU and the network and thus could champion message transferral. We present an architecture where compute ranks offload tasks to Mellanox BlueFields which either compute those tasks themselves or, where appropriate, issue those tasks to underworked ranks elsewhere in the cluster in a way that is sensitive to both workload imbalances and bottlenecks such as network congestion. Novel hardware in the form of programmable network cards offer the opportunity to offload data transfer responsibilities and thus increase code reactivity with regards to unexpected messages and allow the compute ranks to focus on compute throughput. Our work leads into a generic library for nonpersistent load balancing that is orchestrated in the fabric of cluster supercomputers: it has a minimal API and can easily be integrated into simulation codes that phrase their computations in tasks.
</EventDescription>
    <EventParentName>73765-SESS</EventParentName>
    <EventUniqueID>73765-118806</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Location Estimation from the Ground Up: A New Approach to Teaching Least-Squares Algorithms</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP0</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  6:00:00 PM</EventStartTime>
    <EventEndTime>02/24/22  8:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sivan Toledo</EventSpeakers>
    <EventSpeakerIDs>704793</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118989</EventHandoutURL>
    <EventDescription>High-performance computing uses algorithms to resolve mathematical models of a wide range of phenomena. Courses and textbooks that focus on the algorithms and performance-engineering techniques often provide little motivation because explaining the models requires domain knowledge that many students lack.

The poster will present a course and a new SIAM book that present a wide range of least-squares minimization problems and algorithms using models that require almost no background to understand, namely models that estimate location from observations. The problems and algorithms that are covered include linear least squares problem (both full rank and rank deficient), the QR algorithm and the SVD, non-linear least-squares, space-state (Kalman) problems, and mixed-integer least squares. The location-estimation problems that motivate the algorithms include essentially all the techniques used in GPS, as well as many advanced techniques used in other location-estimation systems.

Location-estimation models discussed in the book can thus motivate many types of least-square minimization problems. The book can obviously also be used in courses that expose Mathematics and Computer Science undergraduates to statistical estimation techniques and algorithms without requiring any background in electrical engineering or statistics.

The poster will spur lively discussions on teaching numerical algorithms, a topic that many SIAM PP attendees are highly interested in.
	
</EventDescription>
    <EventParentName>73765-SESS</EventParentName>
    <EventUniqueID>73765-118989</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Deep Gaussian Processes for Uncertainty Quantification in Computer Experiments</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS4</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Annie Sauer</EventSpeakers>
    <EventSpeakerIDs>803130</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117732</EventHandoutURL>
    <EventDescription>Deep Gaussian processes (DGPs) are increasingly popular as predictive models in machine learning for their non-stationary flexibility and ability to cope with abrupt regime changes in training data.  The layered structure of the DGP likelihood makes direct inference impossible.  Existing variational inference methods offer thrifty predictions but oversimplify uncertainties, particularly in the low data settings common in computer experiments.  To achieve full uncertainty quantification, we present a novel elliptical slice sampling Bayesian posterior inferential scheme for DGP surrogates.  Elliptical slice sampling is particularly suited for sampling latent Gaussian layers as it is free of tuning parameters and is able to bounce readily between multiple modes.  Efficient computation relies on parsimonious layouts of latent layers, effective mixing of MCMC chains, and careful utilization of parallel processing.  Our methods are illustrated on simulation data and real computer experiments of varying input dimensionality.  We provide an open source implementation in the ``deepgp’’ package on CRAN.</EventDescription>
    <EventParentName>73103-SESS</EventParentName>
    <EventUniqueID>73103-117732</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Multiphysics and Multiscale Statistical Emulation of Complex Computer Models</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS4</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Serge Guillas</EventSpeakers>
    <EventSpeakerIDs>748559</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117733</EventHandoutURL>
    <EventDescription>Investigating uncertainties in computer simulations can be prohibitive in terms of computational costs, whenever dealing with multi-scale and multi-physics simulations.
First, so-called linked Gaussian processes offer a way to build analytical emulators for systems of computer models such as multi-physics models. We present new exact formula linked Gaussian process under the squared exponential kernel to a class of Mat\'ern kernels, essential in advanced applications, with orders of magnitude gains. An iterative procedure to construct surrogate models for any feed-forward systems of computer models is also introduced and illustrated, with again large gains. We also introduce an adaptive design algorithm that increases the approximation accuracy of linked Gaussian process surrogates with further reduced computational costs.
Second, we present a novel approach to building an emulator using multiple levels of resolution (or fidelity). We employ an sequential design of experiments at each level, and combine training data of different degrees of sophistication in a sequential multilevel approach, using Gaussian process emulators. This dual strategy allows us to allocate efficiently limited computational resources over simulations of different levels of fidelity and build the GP emulator efficiently. We theoretically prove the validity of our approach. Gains of orders of magnitudes in accuracy for medium-size computing budgets are demonstrated in numerical examples.

</EventDescription>
    <EventParentName>73103-SESS</EventParentName>
    <EventUniqueID>73103-117733</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Parallelized Multilevel Markov Chain Monte Carlo</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS4</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Anne  Reinarz</EventSpeakers>
    <EventSpeakerIDs>765680</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117734</EventHandoutURL>
    <EventDescription>The ExaHyPE Engine is a hyperbolic PDE engine capable of solving systems of first order hyperbolic PDEs. The engine provides a space-tree discretization of the computational domain, higher-order DG schemes and a-posteriori subcell limiters. ExaHyPE was initially designed with forward modeling in mind. In order to utilize expertise from the uncertainty quantification (UQ) community we have extended it with an interface to the MIT UQ library (MUQ).

I will present a parallelization strategy for multilevel Markov chain Monte Carlo, a state-of-the-art, algorithmically scalable UQ algorithm for Bayesian inverse problems. The integration between MUQ and ExaHyPE allows for large-scale parallelism across forward model evaluations and the UQ algorithms themselves. The main scalability challenge presents itself in the form of strong data dependencies introduced by the MLMCMC method, prohibiting trivial parallelization. I will demonstrate the effectiveness of  the method by inferring the most probable locations for the initialization of a tsunami.</EventDescription>
    <EventParentName>73103-SESS</EventParentName>
    <EventUniqueID>73103-117734</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Information-Preserving Bayesian Models for Efficient and Robust Learning</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS4</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sandeep Madireddy</EventSpeakers>
    <EventSpeakerIDs>791219</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118577</EventHandoutURL>
    <EventDescription>Modern deep neural networks are in general quite brittle, and hence less robust to noise in the input or adversarial perturbation applied to them, as well as out-of-distribution data. From an information-theoretic point of view, the ability of the model to achieve better generalization and robustness will depend on whether the model learns more semantically meaningful information and compresses the nuisance information. Typically, the DNNs are trained by reducing an empirical loss function which can lead to nuisance or irrelevant information be memorized, that can have a detrimental impact on the model's robustness. To that end, we adopt an information-theoretic Bayesian approach that poses this intuition as a constrained optimization problem and hence seeks to learn a compressed (latent) encoding of input that is maximally informative about our target as measured through their mutual information. An important question is the choice of the latent dimension of the encoding so that the nuisance information is reduced. This question also appears in the context of popular generative models such as the variational autoencoders. To address this, we developed a Bayesian approach to model the joint distribution of the latent encoding dimension and the latent variable distribution through a spike and slab distribution and its variational formulation, which has the ability to systematically quantify the uncertainty across dimensions. 
</EventDescription>
    <EventParentName>73103-SESS</EventParentName>
    <EventUniqueID>73103-118577</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Toward a Better Ecosystem for Understanding and Reusing Scientific Workflows</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS5</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Matthew Wolf</EventSpeakers>
    <EventSpeakerIDs>803298</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117801</EventHandoutURL>
    <EventDescription>Workflow environments have re-emerged as a key topic in scientific computing with the advent of machine learning, multi-disciplinary code coupling, multi-site compute infrastructures, and Big Data for science. A wide range of tools and approaches have been developed and tested within organizations and communities that can help with reuse and refactorization of complex workflow components.  However, we don't have a shared ecosystem for scientific workflows that allows end users to easily reason about how to connect, adapt, and refactor existing workflow components and fragments from other communities.  In this talk, we outline some approaches for building a new systems abstraction for workflow management that leverages insights from metadata semantics, data management systems and middleware, and model-driven code generation techniques.  Building off insights from our previous and current work with computational biology, materials, and other sciences, we point to new opportunities for improving the software process for individual workflows while also supporting a broader push to develop a more open and FAIR community for scientific workflows.

</EventDescription>
    <EventParentName>73126-SESS</EventParentName>
    <EventUniqueID>73126-117801</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Scalable Preconditioner for Graph Laplacian</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS6</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Katarzyna Swirydowicz</EventSpeakers>
    <EventSpeakerIDs>785023</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117833</EventHandoutURL>
    <EventDescription>The problem of finding a solver and preconditioner for graph Laplacian linear systems is well studied. However, not much attention has been devoted to the implementation of such algorithms, especially on HPC systems. In this talk we propose a viable and practical approach for the construction of a graph Laplacian preconditioner. The proposed algorithm is effective in reducing the iteration count and utilizing parallel resources. In addition, we demonstrate how the proposed preconditioner is applied inside LOBPCG in order to find eigenvalues of the graph Laplacian. </EventDescription>
    <EventParentName>73137-SESS</EventParentName>
    <EventUniqueID>73137-117833</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Workflow Critical Path, a Runtime Metric for Dynamic Resource Management</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS7</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Karen Karavanic</EventSpeakers>
    <EventSpeakerIDs>758179</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117223</EventHandoutURL>
    <EventDescription>Optimizing scientific application performance in HPC environments is a complicated task which has motivated development of many performance analysis tools over the past decades. Many tools were designed to analyze the performance of a single parallel code using common approaches such as message passing (MPI), multithreading (OpenMP), acceleration (CUDA), or a hybrid approach. However, current trends in HPC such as the push to exascale, convergence with Big Data, and growing complexity of HPC applications, have created gaps that these performance tools do not cover, particularly involving HPC workflows comprising multiple codes, paradigms, or platforms. To address this performance monitoring gap, we define a new metric called Workflow Critical Path (WCP), a data-oriented, runtime critical path metric for Holistic HPC Workflows. WCP constructs graphs using data states as vertices and data mutations as edges. I will present a fully functional prototype called Crux, a distributed analysis tool that uses cloud-based technologies for calculating and visualizing WCP. Crux’s data store can be queried throughout the workflow runtime, thus providing the current critical path as needed for dynamic resource management.  Our experiments with a workflow simulator on AWS show Crux is scalable and capable of correctly calculating WCP for common Holistic HPC workflow patterns. I will explore the use of WCP and discuss how Crux could be used with production HPC platforms and resource managers.</EventDescription>
    <EventParentName>72962-SESS</EventParentName>
    <EventUniqueID>72962-117223</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Performance-Driven Memory Management for Disaggregated Memory Architectures</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS7</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Amro Awad</EventSpeakers>
    <EventSpeakerIDs>802902</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117228</EventHandoutURL>
    <EventDescription>
Disaggregated memory architectures promise scalable, efficient, and flexible integration of various processing elements in memory-centric architectures. Today's high-performance computing (HPC) systems suffer from severe memory underutilization, which is expected to further grow with the use of dense high-capacity emerging non-volatile memories (NVMs). Moreover, the nature of emerging workloads where large graphs or data structures are operated on collaboratively would need a scalable memory-centric architecture, which can be realized efficiently by disaggregating memory and using shared memory pools. Finally, with emerging NVMs possessing the features of storage (data persistence) and memory (byte-addressability and speed), they can be leveraged to host large files which can be accessed by many different compute nodes. Thus, hosting such files in a  location that is directly and efficiently accessible by many compute nodes is advantageous.  

In this talk, I will summarize our efforts in investigating memory management and access control mechanisms for such disaggregated memory architectures. I will also describe our ongoing efforts for further improving the performance, isolation, and security of disaggregated memory architectures. In the talk, I will describe various performance bottlenecks and challenges when deploying such architectures and the various ways to overcome them.
</EventDescription>
    <EventParentName>72962-SESS</EventParentName>
    <EventUniqueID>72962-117228</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Deep Gaussian Processes with Multi-Task and Transfer Learning for Performance Optimization</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS8</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Wissam Sid-Lakhdar</EventSpeakers>
    <EventSpeakerIDs>803747</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118604</EventHandoutURL>
    <EventDescription>Bayesian learning is a commonly used framework for the performance modeling and optimization of HPC applications. Deep Gaussian processes combine the uncertainty quantification advantage of Gaussian processes with the predictive power of deep learning. Moreover, multitask and transfer learning allow for improved prediction of machine learning models when several similar tasks are to be learned simultaneously or when previous learning are sought to help in the learning of new tasks. In this paper, we combine deep Gaussian processes with multitask and transfer learning to allow for both an improved tuning of an application parameters on problems of interest but also the prediction of parameters on any potential problem the application might encounter.</EventDescription>
    <EventParentName>73328-SESS</EventParentName>
    <EventUniqueID>73328-118604</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Analytical, Statistical, and Machine Learning Models for Compiler Optimization of Convolutional Neural Networks</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS8</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mary Hall</EventSpeakers>
    <EventSpeakerIDs>735129</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118606</EventHandoutURL>
    <EventDescription>We discuss the role of performance modeling for predicting the most effective code transformation sequence for the common problem of optimizing convolutional neural networks (CNNs) when dimensions of the network are known.  Convolution dominates performance of CNNs, and the performance of convolution improves significantly as a result of compiler optimizations including loop permutation, tiling, unroll-and-jam, parallelization and vectorization. Deriving the optimal code transformation sequence for a specific size of network is challenging.  Deep loop nests and large numbers of dimensions of inputs and output mean that there are many different choices, and the best choice is heavily influenced by network size and relative size of different dimensions,  
This talk will describe a number of approaches we have explored for predicting the best optimization sequence: an analytical model utilizing decades of prior compiler research and architecture-specific heuristics, statistical modeling combined with autotuning, and recent work on models derived from machine learning.
We present a machine learning model that has demonstrated success in selecting loop order, and how to extend that model to support additional transformations.  
We show that all approaches are effective for optimizing convolution as compared to manually-tuned libraries, and the tradeoff between approaches involves the amount of developer and system time to construct the model.
</EventDescription>
    <EventParentName>73328-SESS</EventParentName>
    <EventUniqueID>73328-118606</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Reverse-Mode Automatic Differentiation and Optimization of GPU and Heterogeneous Parallel Programs via Enzyme</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS11</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>William Moses</EventSpeakers>
    <EventSpeakerIDs>803819</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118708</EventHandoutURL>
    <EventDescription>Computing derivatives is key to algorithms in scientific computing and machine learning such as optimization, uncertainty quantification, and stability analysis. Enzyme is a LLVM compiler plugin that performs reverse-mode automatic differentiation (AD) and thus generates high performance gradients of programs in languages including C/C++, Fortran, Julia, and Rust. Enzyme is the first fully automatic reverse-mode AD tool to generate gradients of GPU kernels. Moreover, since unlike other tools Enzyme performs automatic differentiation within a general-purpose compiler, we are able to introduce several novel GPU and AD-specific optimizations. To show the generality and efficiency of our approach, we compute gradients of five GPU-based HPC applications, executed on NVIDIA and AMD GPUs. All benchmarks run within an order of magnitude of the original program's execution time. Without GPU and AD-specific optimizations, gradients of GPU kernels either fail to run from a lack of resources or have infeasible overhead. We demonstrate that increasing the problem size by either increasing the number of threads or increasing the work per thread, does not substantially impact the overhead from differentiation. We will also showcase ongoing work to allow Enzyme to be integrated into any fork-join parallel framework, including OpenMP, MPI, and RAJA, as well as any combination of parallel frameworks (e.g. GPU+MPI).</EventDescription>
    <EventParentName>73362-SESS</EventParentName>
    <EventUniqueID>73362-118708</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>alpaka, LLAMA and More – Solutions for Exascale Performance Portability</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS12</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jan Stephan</EventSpeakers>
    <EventSpeakerIDs>804164</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118952</EventHandoutURL>
    <EventDescription>Heterogeneous hardware landscapes will define the Exascale era. At the same time, keeping scientific libraries and applications portable across different
hardware setups while maintaining high performance is no trivial matter. Vendor-provided programming platforms often cannot target accelerators from other
vendors, and different hardware types like CPUs, GPUs and FPGAs require differently tuned algorithms for optimal performance.

A solution to these issues can be found in abstraction layers that provide the user with a single programming interface while still maintaining portability
and performance. In this talk, we introduce the Caravan HPC ecosystem. With the \textit{alpaka} abstraction library for accelerator programming at its core and
many sibling libraries for related use cases --- such as the memory access abstraction layer \textit{LLAMA} or the C++ primitives library \textit{vikunja} --- the Caravan
ecosystem is an ideal choice for scientists and programmers setting out to tackle the challenges of the Exascale era.

</EventDescription>
    <EventParentName>73410-SESS</EventParentName>
    <EventUniqueID>73410-118952</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Portability of Solvers for Future Architectures</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS13</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Fernanda Foertter</EventSpeakers>
    <EventSpeakerIDs>803821</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118713</EventHandoutURL>
    <EventDescription>The last decade and a half have seen the growth of architectures that favor single instruction, multiple threads (SIMT) execution models. The current HPC eco-system of mostly GPU-based architectures has demanded that solvers be built to favor shared-address threaded compute models. Some solvers however have struggled to perform under this paradigm due to data dependencies. Performance is hampered without the ability to fill GPUs with execution threads. This talk will explore what challenges will be faced by developers as hardware moves beyond von Neuman architectures. 
</EventDescription>
    <EventParentName>73363-SESS</EventParentName>
    <EventUniqueID>73363-118713</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Doubt and Redundancy Kill Soft Errors - Replication-Based Resilience in Exahype</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS14</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Michael Bader</EventSpeakers>
    <EventSpeakerIDs>730729</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118742</EventHandoutURL>
    <EventDescription>Replication is an established method to improve resiliency, but typically unwelcome in HPC, due to a substantial increase in compute resources. We will present an approach for task-based replication, implemented in the ExaHyPE engine to solve hyperbolic PDE systems, that mitigates replication overhead by sharing task outcomes between teams of MPI ranks. Thus, ideally each team handles only half of the workload.

In this setup, we use physics-based error criteria to identify soft errors. We only share task outcomes that yield small error criteria values - otherwise we consider the outcome dubious and compare it with that of the replica task. Disagreeing criteria thus indicate soft errors, but also which replica is likely correct.

By switching to the likely correct task outcomes, we obtain a self-healing, resilient algorithm that promises to compensate silent floating-point errors with low overhead  regarding performance, I/O and memory footprint. 
</EventDescription>
    <EventParentName>73369-SESS</EventParentName>
    <EventUniqueID>73369-118742</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Designing Efficient Graph Algorithms on GPUs Through Proxy-Driven Codesign</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS15</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sayan Ghosh</EventSpeakers>
    <EventSpeakerIDs>796491</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118672</EventHandoutURL>
    <EventDescription>Developing scalable graph algorithms on GPUs is challenging, due to the inherent irregularities in the graph structure and memory-access intensive computational pattern. Proxy application driven software-hardware codesign plays a vital role in driving innovation among the developments of applications, software infrastructure and hardware architecture. Proxy applications are self-contained and simplified codes that are intended to model the performance-critical computations within applications. 

In this talk, we discuss facilitating software-hardware codesign through proxy applications with the goal of improving the performance of graph analytics workflows on heterogeneous systems. However, even representative proxy applications may be insufficient to diagnose performance bottlenecks of common graph computational patterns at scale. Therefore, we also discuss the role of derivative benchmarks in enhancing graph applications on GPUs. We will drive the discussion using two applications--Graph matching and clustering, which have applications in the domains of proteomics, computational biology, cybersecurity, numerical analysis and other data science scenarios.   
</EventDescription>
    <EventParentName>73357-SESS</EventParentName>
    <EventUniqueID>73357-118672</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Sparse Factorization-Based Solvers and Preconditioners on Modern GPUs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS15</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Pieter Ghysels</EventSpeakers>
    <EventSpeakerIDs>760022</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118673</EventHandoutURL>
    <EventDescription>We present recent efforts porting the sparse direct solver and preconditioners from the STRUMPACK solver library to GPU architectures. The preconditioners are based on approximate multifrontal factorization using rank-structured matrix compression, which leads to robust and scalable methods applicable to a wide range of problems. We discuss the algorithmic changes and the implementation details that were required to make these algorithms run efficiently on modern NVIDIA and AMD GPUs, using CUDA and HIP/ROCm. We also present initial performance results obtained on Intel accelerators using SYCL and oneAPI. Our implementations rely on emerging standards for variable sized batched dense linear algebra routines. Our codes run on distributed memory and multi-GPU architectures.

</EventDescription>
    <EventParentName>73357-SESS</EventParentName>
    <EventUniqueID>73357-118673</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Two-Level Group Convolution</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS16</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Chang-Ock Lee</EventSpeakers>
    <EventSpeakerIDs>729291</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118662</EventHandoutURL>
    <EventDescription>Group convolution has been widely used in order to reduce the computation time of convolution, which takes most of the training time of convolutional neural networks. However, it is well known that a large number of groups significantly reduce the performance of group convolution. In this talk, we propose a new convolution methodology called "two-level" group convolution that is robust with respect to the increase of the number of groups and suitable for multi-GPU parallel computation. We first observe that the group convolution can be interpreted as a one-level block Jacobi approximation of the standard convolution, which is a popular notion in the field of numerical analysis. In numerical analysis, there have been numerous studies on the two-level method that introduces an intergroup structure that resolves the performance degradation issue without disturbing parallel computation. Motivated by these, we introduce a coarse-level structure which promotes intergroup communication without being a bottleneck in the group convolution. We show that all the additional work
induced by the coarse-level structure can be efficiently processed in a distributed memory system. We compare the proposed method to various approaches for group convolution in order to highlight the superiority of the proposed method in terms of execution time, memory efficiency, and performance.
</EventDescription>
    <EventParentName>73347-SESS</EventParentName>
    <EventUniqueID>73347-118662</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Large Scale Structural Graph Representation Learning</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS19</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Edoardo Serra</EventSpeakers>
    <EventSpeakerIDs>803306</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117837</EventHandoutURL>
    <EventDescription>Graph representation learning methods have attracted an increasing amount of attention in recent years. These methods focus on learning a numerical representation of the nodes in a graph. Learning these representations is a powerful instrument for scientific computing applications involving core tasks such as graph mining, visualization, hashing, etc. Graph representation learning is of particular interest because they facilitate the direct use of standard machine learning models on graphs. Graph representation learning methods can be divided into two main categories, methods preserving the connectivity information of the nodes and methods preserving nodes' structural information. Connectivity-based methods focus on encoding relationships between nodes, with connected nodes being closer together in the resulting latent space. While methods preserving structure generate a new space where nodes serving a similar structural function in the network are encoded close to each other, independently of them being connected or even close to each other in the graph. While preserving node connectivity is a well-established field, structural graph representation learning still has many open problems.
This talk provides an overview of the most recent structural representation approaches, their complexity in relation to their expressive power, and gives parallel computing direction to enhance the expressive power of graph representation learning.  
</EventDescription>
    <EventParentName>73138-SESS</EventParentName>
    <EventUniqueID>73138-117837</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Leveraging Parallel AMR Capabilities in Overset Grid Simulations of Aerospace and Wind Energy Applications </EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS20</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Andrew Kirby</EventSpeakers>
    <EventSpeakerIDs>803788</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118430</EventHandoutURL>
    <EventDescription>Many aerodynamics problems require multi-scale simulation capabilities to track turbulent vortical fluid structures such as the wakes generated by wind turbines or helicopter rotors. To resolve these complex multi-scale phenomena, parallel adaptive mesh refinement paired with overset grid technologies provides a computationally efficient and scalable solution. We highlight various AMR approaches, such as octree-based and patch-based systems, in tandem with the TIOGA overset grid assembler for simulating these relative-body motion problems. Moreover, we examine the algorithmic characteristics of each approach for providing the necessary data required by the overset grid assembler. Lastly, we'll discuss possible AMR and overset capabilities needed for leveraging next-generation heterogeneous computing systems.</EventDescription>
    <EventParentName>73162-SESS</EventParentName>
    <EventUniqueID>73162-118430</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>FAIR Research Software as a Step Towards Better Research</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS21</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daniel Katz</EventSpeakers>
    <EventSpeakerIDs>779008</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117784</EventHandoutURL>
    <EventDescription>In recent years, the scholarly community has examined its culture and practices and found a set of overlapping areas to improve. Those that involve research software include open science (making the outputs and processes of scholarly research available), reproducibility (increasing trust in scholarly results by making them repeatable), FAIR (making software findable, accessible, interoperable, and reusable), citation (making software directly citable and encouraging the practice of citation), sustainability (ensuring that software will continue to be available in the future, on new platforms, meeting new needs), and career paths (recognizing and supporting essential roles such as research software engineers, particularly in academia). While the scholarly community is generally supportive of all of these efforts, the degree of support wanes with both the amount of extra work needed and the lack of clear details on how to achieve them, along with misaligned incentives.

This talk will focus on FAIR and how it can be applied to research software. This leads to a set of distinct challenges, including scope (defining research software), principles (defining what findable, accessible, interoperable, and reusable mean for research software), implementation (developing guidelines and instructions for how to make research software FAIR), adoption (getting different stakeholders to recognize and use the principles) and metrics (measuring the adoption of FAIR for research software).
</EventDescription>
    <EventParentName>73127-SESS</EventParentName>
    <EventUniqueID>73127-117784</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Designing Task-Based Libraries in Legion</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS67</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:55:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Alex Aiken</EventSpeakers>
    <EventSpeakerIDs>803133</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117739</EventHandoutURL>
    <EventDescription>Scalable software development depends on having good facilities for developing reusable libraries. In the case of parallel, and particularly distributed, programming there is a question of how to write libraries that can deal with unknown distributions of application data, both on input and output. This talk will discuss some motivating examples of the problem and solutions developed in the context of the Legion distributed programming system.
</EventDescription>
    <EventParentName>73056-SESS</EventParentName>
    <EventUniqueID>73056-117739</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Experiences with Quasi-Newton Methods for Numerical Optimization on GPUs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS68</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:55:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Todd Munson</EventSpeakers>
    <EventSpeakerIDs>704203</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118446</EventHandoutURL>
    <EventDescription>In this talk, I will discuss our experiences with using the Toolkit for Advanced Optimization to solve unconstrained and bound constrained numerical optimizations problems using GPUs.  We focus on first order methods that only require objective and gradient evaluations and in particular the conjugate gradient and quasi-Newton methods.</EventDescription>
    <EventParentName>72875-SESS</EventParentName>
    <EventUniqueID>72875-118446</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Multi-Hetero Accelerated Computing: System and Application</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS70</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:55:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Taisuke Boku</EventSpeakers>
    <EventSpeakerIDs>780364</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117940</EventHandoutURL>
    <EventDescription>GPU is the most promised accelerating device for HPC today
and there are a number of GPU-clusters, especially for top-level
systems in the world, for high absolute performance as well as its
high performance per power consumption. However, the GPU solution is
not perfect for some applications, especially for dynamically changing
the behavior and degree of parallelism during the execution. There are
several more features which cannot be covered perfectly by GPU.

On the other hand, FPGA (Field Programmable Gate Array) is rapidly
featured not just for embedded system but also for HPC use. FPGA has
almost opposite characteristics from GPU (non-SIMD, hardware
programmability, autonomous, equipped with high speed interconnection,
etc.), so we are challenging to couple FPGA with GPU for
compensation. We call this paradigm as CHARM (Cooperative
Heterogeneous Acceleration with Reconfigurable Multi-devices), and
implemented the world first GPU-FPGA combined multi-hetero cluster,
Cygnus at Center for Computational Sciences in University of Tsukuba.

In this talk, I focus on the basic concept of CHARM and supercomputer
Cygnus as well as several supporting system environment for FPGA-FPGA
direct interconnection network, intra-node GPU-FPGA DMA, a couple of
programming method for CHARM, and the actual application which achieves
a great performance improvement by this concept.
</EventDescription>
    <EventParentName>73178-SESS</EventParentName>
    <EventUniqueID>73178-117940</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>P1673:A Proposal for a C++ Standard Linear Algebra Library</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS72</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:00:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christian Trott</EventSpeakers>
    <EventSpeakerIDs>755875</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118757</EventHandoutURL>
    <EventDescription>Linear algebra is a fundamental building block for a large range of applications, which had been standardized as far back as the early 90s through the Fortran BLAS interface. However, despite its importance linear algebra functionality never made it into the C++ standard, even though functionality of similar complexity - such as sorting - has been part of the standard library for a long time. Partly that is because BLAS was primarily used in high performance computing, which only represents a small fraction of the overall C++ user base. Consequently, linear algebra was considered a niche feature by the committee. The increasing use of high performance computing in the commercial sector, together with the advent of AI and Machine Learning changed that calculus, putting linear algebra into the list of strategic exploration topics for the C++ standard. P1673 is a Linear Algebra proposal based on the existing BLAS standard. It builds on top of mdspan - multi-dimensional arrays for C++ - and extents the BLAS capabilities by modern capabilities such as mixed precision calculations, flexibility in data layout, and a design which is easily extended to support batched BLAS. This talk will present the design, demonstrate how it improves upon the existing BLAS standard, and discuss some options for future extension.</EventDescription>
    <EventParentName>73372-SESS</EventParentName>
    <EventUniqueID>73372-118757</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Update on the Intel Onemkl Library: Performance, Interfaces, and Portability</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS72</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:05:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:25:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sarah Knepper</EventSpeakers>
    <EventSpeakerIDs>760197</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118775</EventHandoutURL>
    <EventDescription>Today’s applications are more diverse than ever, as is the hardware used to run and accelerate them. This causes a challenge for developers, as they must juggle multiple code bases, tools, programming languages, and workflows. The oneAPI initiative is designed to help overcome these challenges, by offering a unified, standards-based programming model that allows acceleration of the code on various processing architectures. The Intel® oneAPI Math Kernel Library (oneMKL) product is the Intel product implementation of the oneMKL Data Parallel C++ (DPC++) specification (with DPC++ interfaces) as well as similar functionality with C and Fortran interfaces, and is highly optimized for Intel CPU and GPU hardware. In this talk, we will look at recent developments in the library and also discuss the oneMKL interfaces project. The oneMKL interfaces are an open-source implementation of the oneMKL specification, which works with multiple hardware backends using device-specific libraries underneath.</EventDescription>
    <EventParentName>73372-SESS</EventParentName>
    <EventUniqueID>73372-118775</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Boast: A Computing Kernel Metaprogramming and Autotuning Framework for Heterogeneous Architectures</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS73</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:05:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:25:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Brice Videau</EventSpeakers>
    <EventSpeakerIDs>803868</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118764</EventHandoutURL>
    <EventDescription>In the ever-changing High-Performance Computing environment, porting and tuning applications to new platforms is of paramount importance but tedious and costly in terms of human resources. While solutions start to emerge to abstract most of the underlying programming models, the tuning of computing kernels is the work of highly trained specialists. An efficient computing kernel is often the combination of insights and techniques specific to the targeted architecture. The performance portability, and sometimes compatibility, of these specialized versions are often poor.

One way to tackle this problem is by using meta-programming approaches allowing the specialist to express optimization strategies and techniques in an orthogonal manner. Coupled with a runtime supporting many programming models found in HPC, different versions can be generated and benchmarked. The design space can then be explored to find the most suitable version for the target architecture. I will discuss how I developed and used BOAST, an autotuning framework, to address these challenges in several Heterogeneous HPC applications, using accelerators (OpenCL, CUDA, HIP) or vector architectures (AVX, NEON).</EventDescription>
    <EventParentName>73373-SESS</EventParentName>
    <EventUniqueID>73373-118764</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Automatic Code Generation from High Level Mathematical Abstractions</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS73</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:55:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Satya Pramod Jammy</EventSpeakers>
    <EventSpeakerIDs>803869</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118766</EventHandoutURL>
    <EventDescription>The hardware for scientific computing evolved significantly in the past decade, especially with many core architectures like GPGPU's. Programming the numerical methods by the modellers on these complex machines need thorough understanding of its architecture,  programming language and maintaining different code bases for each architecture. This is not sustainable, as huge time needs to be invested and capability enhancement becomes nearly impossible. 

Separation of concerns approach using Domain Specific Languages (DSL's) is a sensible option to tackle these issues. In this the numerical modeller writes a single source code using the DSL's Application Programming Interface (API) and the computer scientist's work on optimization for different architectures using source-to-source translation. However, the DSL's API's tend to be very specific and programming using DSL's is error prone. Code generation approach is a solution to overcome some of the complexities caused by ever changing architectures and programming paradigms. 

In this talk, experiences in developing automatic code generation frameworks for structured finite difference methods (OpenSBLI, https://opensbli.github.io/) and finite volume methods from high level mathematical abstractions and symbolic manipulations will be presented. Algorithmic changes that are impossible to implement by hand can be easily generated using these techniques and the speedup one can achieve will also be presented.</EventDescription>
    <EventParentName>73373-SESS</EventParentName>
    <EventUniqueID>73373-118766</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Fast Multigrid Reduction-in-Time for Advection Problems</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS74</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:00:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Oliver Krzysik</EventSpeakers>
    <EventSpeakerIDs>784885</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118638</EventHandoutURL>
    <EventDescription>Parallel-in-time integration of partial differential equations (PDEs) is an increasingly important topic in the context of modern, massively parallel computing architectures. Many parallel-in-time algorithms have been shown highly efficient for diffusion-dominated PDEs, but often are inefficient or even divergent for advection-dominated PDEs. We consider the application of the two-level Parareal algorithm, and its multilevel generalization of multigrid reduction-in-time (MGRIT) to linear advection problems. We focus on odd-order, finite-difference semi-Lagrangian discretizations of such problems, for which we present novel semi-Lagrangian-like coarse-grid operators that yield fast multilevel time integration. The coarse-grid operators occur in split form, consisting of a standard semi-Lagrangian discretization combined with a dissipative correction. </EventDescription>
    <EventParentName>73332-SESS</EventParentName>
    <EventUniqueID>73332-118638</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Time-Parallel Flow Estimation</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS74</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:30:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:50:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sebastian Götschel</EventSpeakers>
    <EventSpeakerIDs>795239</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118644</EventHandoutURL>
    <EventDescription>Deformable image registration is a key technology in medical imaging; there the goal is to compute a meaningful spatial correspondence between two or more images of the same scene. One approach is to use an optimal control formulation to compute a stationary velocity field that parameterize the deformation map. The same methods can be used to estimate the motion of contrast agents from 3d ultrasound images.

In the talk I’ll introduce the application problem and discuss computational techniques for its solution, with a focus on using parallelization in time to reduce the time-to-solution.</EventDescription>
    <EventParentName>73332-SESS</EventParentName>
    <EventUniqueID>73332-118644</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Moment Limiter for the DG Method on Adaptively Refined Triangular Meshes</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS75</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:05:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:25:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Lilia Krivodonova</EventSpeakers>
    <EventSpeakerIDs>737639</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118645</EventHandoutURL>
    <EventDescription>We will present a second-order limiter for the discontinuous Galerkin method on nonconforming triangular meshes that arise in adaptive computations. It limits the linear solution coefficients (or moments) by reconstructing the slopes along a set of directions in which the moments decouple. We perform the reconstruction of the slopes on a compact stencil consisting of eight elements. We propose a simple algorithm to update the reconstruction stencil of elements in an adaptively refined triangular mesh. Our algorithm is implemented entirely on the graphics processing unit (GPU) and avoids race conditions. We provide numerical experiments to validate the robustness of the limiter in the presence of discontinuities. Finally, we perform wall clock studies to analyze the performance of the proposed limiter for the computational cost involved in setting up the reconstruction neighborhood and executing the limiter. 


</EventDescription>
    <EventParentName>73338-SESS</EventParentName>
    <EventUniqueID>73338-118645</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>FPGA/DNN Codesign and Acceleration</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS76</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:00:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>801</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Callie Hao</EventSpeakers>
    <EventSpeakerIDs>803882</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118787</EventHandoutURL>
    <EventDescription>In this talk, I will introduce a neural network and hardware implementation co-search methodology, named NAIS, to pursue aggregated solutions of high accuracy DNN designs and efficient hardware deployments simultaneously. To enable a comprehensive co-search framework, there are three indispensable components: 1) efficient hardware accelerator design (e.g. FPGA); 2) hardware-aware neural architecture search (NAS); 3) automatic design tools to quickly deploy DNNs to hardware platforms. I will discuss each component and the integrations to support an efficient and optimal NAIS implementation.

</EventDescription>
    <EventParentName>73326-SESS</EventParentName>
    <EventUniqueID>73326-118787</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Coupling Particle-in-Cell and Continuum Methods in the Global Total-f Gyrokinetic Magnetic Fusion Code XGC</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS77</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:30:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:50:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Albert Mollén</EventSpeakers>
    <EventSpeakerIDs>803631</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118425</EventHandoutURL>
    <EventDescription>The X-point gyrokinetic code (XGC) is a 5D total-f gyrokinetic particle-in-cell (PIC) code used to study kinetic transport phenomena in magnetic fusion plasmas. In addition to PIC methods, the code evaluates dissipative operations such as Coulomb collisions and heat sources/sinks on a 5D grid at each fixed time step. This requires a PIC-continuum coupling in the form of a mapping between the marker particles and the 5D phase-space grid. If the conservation errors in the mapping are undesirably large they can cause a significant error accumulation in long-time simulations of plasmas with steep density and temperature gradients. Here we discuss a novel mapping technique in 2D velocity-space, based on the calculation of a pseudo-inverse, to exactly preserve moments up to the order of the chosen discretization space and to minimize the errors. This new particle to/from velocity grid interpolation method has been implemented in XGC, and we demonstrate its effectiveness. The mapping relies on a particle resampling technique which is used to create, annihilate or redistribute particles in configuration space while preserving a desired number of moments. We will also discuss details of the resampling and how the new mapping could be used to reduce the error when coupling XGC to other codes.


</EventDescription>
    <EventParentName>73283-SESS</EventParentName>
    <EventUniqueID>73283-118425</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Fast Parallel Solver for the Space-Time IgA-Dg Discretization of the Anisotropic Diffusion Equation</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS78</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:40:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Pietro Benedusi</EventSpeakers>
    <EventSpeakerIDs>769511</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118648</EventHandoutURL>
    <EventDescription>We consider the space-time discretization of the anisotropic diffusion equation, using an isogeometric analysis (IgA) approximation in space and a discontinuous Galerkin (DG) approximation in time. Drawing inspiration from a former spectral analysis of space-time operators in [Benedusi, 2018], we propose for the resulting space-time linear system a parallel multigrid preconditioned GMRES method, which combines a preconditioned GMRES with a standard multigrid acting only in space. The performance of the proposed parallel solver is illustrated through numerical experiments, showing its competitiveness in terms of iteration count, run- time and parallel scaling. More general parallel space-time multigrid techniques applied also to non-linear problems, will be also discussed.
</EventDescription>
    <EventParentName>73333-SESS</EventParentName>
    <EventUniqueID>73333-118648</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Generating GPU Kernels from Chapel's Features for Parallelism And Locality</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS79</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Engin Kayraklioglu</EventSpeakers>
    <EventSpeakerIDs>803870</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118767</EventHandoutURL>
    <EventDescription>Chapel is a portable parallel programming language. Since the advent of using GPUs in HPC applications, targeting GPUs natively has been one of the most sought for features by Chapel users. Chapel's high-level language constructs, such as data-parallel \emph{forall} loops that specify parallelism and \emph{on} statements that specify locality, offer a great potential in writing portable applications that can execute on CPU and/or GPU efficiently. With this vision, the Chapel team at HPE has been making significant progress in designing and implementing support for GPUs, particularly in the September 2021 release of Chapel 1.25.

In this talk, we will cover basic parallel concepts of the Chapel programming language focusing on those that are used for GPU programming. We will outline Chapel's vision of how GPUs can be targeted using those concepts in a portable manner. Then, we will dive deeper into how the Chapel compiler transforms data-parallel forall loops into GPU kernels and launches them. Finally, we will discuss near- and long-term plans for Chapel's native GPU support.


</EventDescription>
    <EventParentName>73374-SESS</EventParentName>
    <EventUniqueID>73374-118767</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Moccuda: Running Cuda Codes on Fugaku</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS79</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:40:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jens Domke</EventSpeakers>
    <EventSpeakerIDs>803872</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118769</EventHandoutURL>
    <EventDescription>In recent years, the fastest supercomputers have been dominated by GPU-accelerated node architectures, and hence a majority of scientific applications have been ported to the CUDA programming paradigm and Nvidia's performance libraries (e.g. cuBLAS), while improvements to the original CPU-version received less attention. Furthermore, the proliferation of Deep Learning (DL) resulted in frameworks which are implemented with a GPU-first or GPU-only design philosophy and the CPU version only serves for unoptimized debugging purposes. These trends cause a non-trivial challenge to novel archirectures, such as RIKEN's Arm/CPU-based Supercomputer Fugaku.
To assist the early porting efforts, we designed, and present in this talk, a framework to natively execute CUDA code on CPUs. However, manually porting existing GPU-first applications and frameworks to Fugaku is a tedious (at best) or impossible (at worst) task, especially when it comes to highly-complex DL frameworks like Tensorflow or Pytorch with specific performance goals in mind. Hence, we explore an alternative approach and present the application a virtual GPU plus re-implementations of CUDA libraries for CPU, to execute those CUDA applications natively. We demonstrate a proof-of-concept of our idea and show results of running Resnet50 with Pytorch, among other examples, on Supercomputer Fugaku.</EventDescription>
    <EventParentName>73374-SESS</EventParentName>
    <EventUniqueID>73374-118769</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Data Parallel Training in DeepSpeed with ZeRO (Zero Redundancy Optimizer)</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS80</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:40:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Samyam Rajbhandari</EventSpeakers>
    <EventSpeakerIDs>803844</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118738</EventHandoutURL>
    <EventDescription>Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. 
In this talk we introduce a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. 
In fact, ZeRO can scale to trillions of parmaeters on modern GPU clusters and can be combined with heterogeneous memory technology to scale to tens of trillions of parameters without compromising efficiency or throughput scalability. Unlike other forms of parallelism, ZeRO enables data scientists to scale their model without requiring model code refactoring.
A robust implementation of ZeRO and it’s extended family of technologies have been implemented in the DeepSpeed Library, and is used extensively by the DL community for large model training. 

</EventDescription>
    <EventParentName>73367-SESS</EventParentName>
    <EventUniqueID>73367-118738</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Memory-Efficient Tensorized Embedding Layers for Neural Networks</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS81</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5100</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Chunxing Yin</EventSpeakers>
    <EventSpeakerIDs>789345</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118732</EventHandoutURL>
    <EventDescription>Tensor-trains have been proposed to reduce the memory requirements of embedding tables in industrial-scale deep neural networks, including recommendation models and graph neural networks. This talk summarizes the tensor-trains use cases and implementation challenges when training large embeddings for such models. In this context, we are developing a new prototype infrastructure of performance-optimized kernels, TT-EmbeddingBag, that aims to reduce model sizes while preserving model accuracy and incurring only modest increases in training-time over uncompressed baselines.
</EventDescription>
    <EventParentName>73354-SESS</EventParentName>
    <EventUniqueID>73354-118732</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Parallel Memory-Efficient Computation of Symmetric Higher-Order Joint Moment Tensors</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS81</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  3:05:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:25:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5100</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Zitong Li</EventSpeakers>
    <EventSpeakerIDs>797419</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118735</EventHandoutURL>
    <EventDescription>The decomposition of higher-order joint cumulant tensor of spatial-temporal data sets is useful in analyzing multi-variate non-Gaussian statistics with a wide variety of applications (e.g. anomaly detection, independent component analysis, dimensionality reduction). Computing the cumulant tensor often requires computing the joint-moment tensor of the input data first, which is very expensive using a naïve algorithm. The current state-of-the-art algorithm for computing joint moment tensors takes advantage of the symmetric nature of a moment tensor by dividing it into smaller cubic tensor blocks and only compute the blocks with unique values and thus reducing computation. We propose an improvement over this algorithm by posing its computation as matrix operations, specifically Khatri-Rao products and standard matrix multiplications. Because this approach is much more cache efficient, we expect considerable speedup in a single processor. We implemented our algorithm in Julia and in Matlab and compared it against the state-of-the-art approach. The results show a speedup of up to 10x.</EventDescription>
    <EventParentName>73354-SESS</EventParentName>
    <EventUniqueID>73354-118735</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Narrow Band-Based Dynamic Load Balancing for Multiphase Flow Simulations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS84</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daniel Appel</EventSpeakers>
    <EventSpeakerIDs>803761</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118649</EventHandoutURL>
    <EventDescription>This talk presents a dynamic load balancing scheme for compressible two-phase flow simulations with the level-set ghost-fluid method. The proposed load balancing scheme is designed generally for dynamic workload variations on unstructured meshes using space-filling curves (SFCs). In case of the level-set ghost-fluid method, the workload imbalance arises from applying the costly interface-capturing algorithm only to the grid cells near the phase interface. To this end, we introduce an element masking (narrow band), that also enables an efficient code instrumentation through accurate, element-local wall time measurements.

We compare state-of-the-art partitioning algorithms for SFCs and investigate the strong scaling behavior for up to O(1e6) cores. The results demonstrate near optimal parallel efficiency and an average performance gain of factor ten compared to previous, unbalanced simulations. The load balancing scheme is applied to simulate complex droplet dynamics, including the well-studied interaction of a shock wave with a singular droplet.

</EventDescription>
    <EventParentName>73339-SESS</EventParentName>
    <EventUniqueID>73339-118649</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Simulating Acoustic and Gravity Waves in ForestClaw</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS84</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Donna Calhoun</EventSpeakers>
    <EventSpeakerIDs>714749</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118698</EventHandoutURL>
    <EventDescription>We present recent progress in the development the software platform ForestClaw, a 
parallel, dynamically adaptive  library for solving PDEs on a nested hierarchy of logically Cartesian grids.  In this talk, we discuss software engineering challenges related to extending ForestClaw  with the MAGIC (Model for Acoustic and Gravity wave Interations and Coupling, J. Snively, Embry-Riddle, FL) and GEMINI (Geospace Environment Model Ion-Neutral Interactions, M. Zettergren, Embry-Riddle, FL) codes.  A key challenge in this effort, funded as part of the DARPA AtmoSense program, is to factor existing MAGIC and GEMINI codes so they can take advantage of the adaptive capabilities in ForestClaw.   A second challenge is to facilitate runtime exchange of solution data between GEMINI and MAGIC.  We will show preliminary results from the AirWaves project, as well as code development with the core ForestClaw platform needed to support AirWaves.</EventDescription>
    <EventParentName>73339-SESS</EventParentName>
    <EventUniqueID>73339-118698</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Particle Resampling and Mesh Quasi-Interpolation in Delta-F Methods</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS87</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:40:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Varis Carey</EventSpeakers>
    <EventSpeakerIDs>803653</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118471</EventHandoutURL>
    <EventDescription>In $\delta f$ PIC plasma simulations, particle weight growth may lead to significant error, while marker particle motion can lead to to both inadequate phase space resolution and poor load balancing in large-scale simulations.   We introduce a class of particle resampling techniques that address these issues,  detailing the effective selection of new particle phase space position and culling low-impact particles.   In addition, we relate the resampling procedure to the general problem of particle-mesh projection, and examine how different selections impact weight positivity, projection accuracy, phase space locality, and moment preservation.  We discuss modifications to cater to code coupling and simulations using a total-$f$ approach.  Finally, we illustrate the method at scale, using  XGC, a global 5D total-f gyrokinetic  PIC code.   
</EventDescription>
    <EventParentName>73284-SESS</EventParentName>
    <EventUniqueID>73284-118471</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Impacting Software Quality and Process Through the Extreme-Scale Scientific Software Stack (E4S) and Software Development Kit (SDK) Projects</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS21</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>James Willenbring</EventSpeakers>
    <EventSpeakerIDs>728876</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117793</EventHandoutURL>
    <EventDescription>The E4S and SDK projects improve quality and efficiency within the scientific software ecosystem through containerization, build caches, continuous integration (CI) testing, improving interoperability between packages, and community policies. 
The E4S containerization effort delivers solutions that raise productivity by providing a tested environment for development and delivering tutorials. The E4S build cache increases productivity, providing thousands of pre-built binaries for a variety of platforms. The containers and build cache also enhance the reproducibility of configurations and errors. E4S promotes the sustainability of the scientific software ecosystem through a multi-platform CI testing infrastructure. 
Covering many areas including development tools, math libraries, and data and visualization, individual SDKs enhance the productivity of application developers in a variety of ways, including increasing the interoperability and portability of software within the SDK, and providing an additional, complimentary layer of CI testing. Community policies at the E4S level, and SDK-specific policies promote long-term sustainability through minimal software standards in a variety of areas, including building, testing, documentation, and error handling.
This talk will introduce many ways in which E4S and the SDK projects are increasing productivity, and improving sustainability and reproducibility, as well as touch on future efforts in these areas.
	</EventDescription>
    <EventParentName>73127-SESS</EventParentName>
    <EventUniqueID>73127-117793</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Recent GPU Developments of Multigrid Reduction in Hypre for Problems in Porous and Fractured Media</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS24</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Quan Bui</EventSpeakers>
    <EventSpeakerIDs>785798</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117818</EventHandoutURL>
    <EventDescription>In many fully-implicit multi-physics simulators, including those for porous media applications, the main computational cost typically rests on the linear solve. Because of the wide range of processes and couplings that may be involved—e.g., formation and propagation of fractures, deformation of the solid porous medium, viscous flow of one or more fluids in the pores and fractures, complicated well sources and sinks, etc.—it is difficult to develop general-purpose and scalable linear solver. This challenge is further aggravated by the range of different discretization schemes that may be adopted, which has a direct impact on the linear system structure. Recent developments of a multigrid reduction (MGR) framework allow one to explore and experiment with scalable preconditioners for a wide range of these coupled problems. In this talk, I will present new results of porting an implementation of MGR in Hypre to GPU. Several numerical examples of difficult, field-scale problems simulated by GEOSX using MGR will be presented, notably: a hybrid discretization of single-phase flow, compositional multiphase flow with and without complex wells, and hydraulic fracturing.

</EventDescription>
    <EventParentName>72942-SESS</EventParentName>
    <EventUniqueID>72942-117818</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Multiphysics Components for High Fidelity ICF Hohlraum Simulations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS24</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Bobby Philip</EventSpeakers>
    <EventSpeakerIDs>735672</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118683</EventHandoutURL>
    <EventDescription>In laser-indirect-drive (LID) ICF, laser energy is converted to x-rays by illuminating the inner
surface of a hollow metal cylinder (hohlraum) surrounding a capsule. The x-rays bathe
the capsule to compress and heat the fuel by the rocket force produced by ablation of the outer capsule
layer. Then, the fuel participates in nuclear fusion. It is now understood that departures from “hydro”
conditions exhibited by hohlraum plasmas (such as kinetic plasma interpenetration and mix), nonlinear
laser-plasma interactions (LPI), and electromagnetic (EM) effects pose major uncertainties for the
interpretation of experimental data. Integrated simulations of LID ICF implosions are remarkably difficult
as they require capturing faithfully the physical processes underlying laser propagation in the hohlraum, laser energy conversion to x-rays and delivery to the ablator surface, the ablation process, subsequent capsule implosion, and finally ignition and thermonuclear burn. All these processes occur in plasmas in different regimes of coupling and collisionality, and demand tailored simulation strategies. We will report on our ongoing efforts to develop a simulation tool capable of addressing these challenges focusing on some aspects of the full problem such as multiscale methods for kinetic plasma and radiation transport simulations, implicit EM algorithms, as well as a robust coupling strategy between these components at scale.</EventDescription>
    <EventParentName>72942-SESS</EventParentName>
    <EventUniqueID>72942-118683</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Performance Portable Additive Manufacturing Simulations with the Material Point Method</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS25</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:15:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sam Reeve</EventSpeakers>
    <EventSpeakerIDs>804167</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118955</EventHandoutURL>
    <EventDescription>PicassoMPM is a new material point method application with a focus on high-fidelity simulation of additive manufacturing (AM). We first describe the software design, where PicassoMPM is built on libraries from three parts of the Exascale Computing Project (ECP): Kokkos for performance portability (software technology), Cabana for particle algorithms and communication (co-design), and Picasso for PIC and MPM motifs (ExaAM application). This strategy enables sharing development across the widest set of applications possible, e.g. other particle methods or MPM for a different target problem. This design is also crucial for simulation across HPC architectures, including ORNL Summit and Frontier from which we show initial performance and discuss potential optimizations. Finally, we discuss the calibration, comparison, and predictive capability of PicassoMPM for laser powder bed fusion against experimental data and other simulation methods as part of ExaAM.
</EventDescription>
    <EventParentName>73411-SESS</EventParentName>
    <EventUniqueID>73411-118955</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>HACC+Cabana: Performance Portability for PIC Methods in Cosmological N-Body Simulations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS25</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:20:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:40:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Adrian Pope</EventSpeakers>
    <EventSpeakerIDs>750670</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118956</EventHandoutURL>
    <EventDescription>The Hardware/Hybrid Accelerated Cosmology Code (HACC) is a high-performance cosmological N-body structure formation code designed to run extreme-scale simulations on the largest supercomputers. Gravity has infinite extent and is unshielded, so it dominates on the largest scales. HACC uses force splitting techniques to divide the gravitational force into long-range and short-range components. The long-range component is calculated with Particle-In-Cell (PIC) methods, where a Particle-Mesh (PM) interaction deposits density onto a grid, and then particle velocities are updated with forces from a spectral Poisson solver that employs distributed-memory 3D FFTs. Careful spectral shaping is used to limit the force handover to roughly 3 grid cell lengths, limiting the number of particle-particle interactions needed for the short-range force. The short-range force kernel is the most FLOPS-intense portion of HACC, and specific implementations have been developed and optimized by hand for a variety of architectures. HACC developers are building a new Particle-Particle-Particle-Mesh (P3M) mini-app with the ECP/CoPA Cabana Particle Toolkit, using Cabana and Kokkos data structures and algorithms whenever suitable. This code is expected to facilitate prototyping algorithmic designs for HACC, exploring optimal configurations for different architectures (eg. ARM, GPUs, MPI-aware accelerators), and evaluating available backends for Kokkos and Cabana (eg. OpenMP threads, CUDA, HIP, SYCL).</EventDescription>
    <EventParentName>73411-SESS</EventParentName>
    <EventUniqueID>73411-118956</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>High Performant Scalable R-Adaptive Particle In Cell Method with Monotonic Reconstruction Remapping</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS25</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:45:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kwitae Chong</EventSpeakers>
    <EventSpeakerIDs>804168</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118957</EventHandoutURL>
    <EventDescription>Particle-in-Cell (PIC) methods for flow are good choices when complex moving interfaces and discontinuities interact with complex geometries due to the low-dissipation nature of Lagrangian advection combined with the ability of particles to interact with various geometry descriptions. In our talk, Polynomial Particle in Cell (PolyPIC) method with additional higher order polynomial description of the particle velocity mode is used to increase the numerical accuracy and stability of compressible flow simulations. Standard r-adaptive mesh redistribution is also demonstrated to improve the solution by improving mesh quality around the steep gradients of the solution through the solution of a mesh motion equation. We improve the mesh motion procedure by introducing a new monotonic reconstruction particle remapping to maintain a good distribution of particle along the computational domain, which guarantees stability and accuracy. The method is demonstrated with canonical benchmark problems including Sod Shock, Woodward-Colella wave and other Riemann problems by demonstrating adaptively populated meshes and corresponding monotonically reconstructed particles.</EventDescription>
    <EventParentName>73411-SESS</EventParentName>
    <EventUniqueID>73411-118957</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Accelerating Space and Space-Time Statistical Modeling with Mixed-Precision Arithmetic</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS27</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:15:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sameh Abdulah</EventSpeakers>
    <EventSpeakerIDs>794833</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117760</EventHandoutURL>
    <EventDescription>Geostatistics predicts desired quantities from geographically distributed data, based on statistical models. A primary computational kernel of spatial statistics is the maximum log-likelihood estimation (MLE) function, whose central data structure is a dense covariance matrix that requires two operations, i.e., inverse and determinant evaluation, with the cost of quadratic storage and cubic arithmetic complexity. To reduce this complexity, we migrate geostatistics to three precisions approximation (double/single/half) based on the distances-based structure of the application. We exploited the advantages of Nvidia Tensor Core technology to accelerate space and space-time modeling on large scale. Primary experiments on 128 Summit nodes show that our method delivers 6.5-8 Pflops/s sustained performance with mixed-precision to obtain the required application accuracy on 1M locations real datasets compared to 3.5 Pflops/s with double-precision execution of a single iteration of the MLE operation.

</EventDescription>
    <EventParentName>73114-SESS</EventParentName>
    <EventUniqueID>73114-117760</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>GPU Optimisation of Local Time Stepping on Unstructured Adaptive Meshes</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS28</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:15:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ravil Dorozhinskii</EventSpeakers>
    <EventSpeakerIDs>803350</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117915</EventHandoutURL>
    <EventDescription>The earthquake simulation software SeisSol adopts a high-order discontinuous Galerkin discretisation to solve the seismic wave equation (in various materials - elastic, acoustic, poroelastic, e.g.) on adaptive tetrahedral meshes. Cluster-oriented local time stepping, i.e., choosing the time step size adaptively depending on element size and material property is a crucial component to bring down the time to solution for realistic models. We will present approaches to optimize local time stepping on modern GPU-based supercomputers and discuss load distrubution challenges that stem from non-homogeneous performance on heterogeneous architectures.
</EventDescription>
    <EventParentName>73163-SESS</EventParentName>
    <EventUniqueID>73163-117915</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Numerical Behavior of GPU Matrix Multiply-Accumulate Hardware</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS29</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:15:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mantas Mikaitis</EventSpeakers>
    <EventSpeakerIDs>789947</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116807</EventHandoutURL>
    <EventDescription>Tensor cores are hardware units on the latest GPUs that perform matrix multiply accumulate (MMA) operation. 116 of the TOP500 supercomputers contain these units and a lot of the numerical libraries begin to utilize them in various algorithms. Tensor cores and similar arithmetic units targeted at machine learning applications are not necessarily IEEE 754-compliant and features such as rounding, normalization, order of operations, subnormal number support and others can differ from a standard software implementation of the matrix multiplication. In this talk I will discuss our recent work on determining various numerical features of MMAs, using tensor cores as an example. We determined various features by testing tensor cores with carefully constructed numerical test cases on the V100, T4 and the A100 NVIDIA GPUs.
</EventDescription>
    <EventParentName>72751-SESS</EventParentName>
    <EventUniqueID>72751-116807</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Accelerating FROSch Preconditioners using GPUs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS30</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:45:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ichitaro Yamazaki</EventSpeakers>
    <EventSpeakerIDs>790005</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118666</EventHandoutURL>
    <EventDescription>The Trilinos package FROSch (Fast and Robust Overlapping Schwarz) implements multilevel overlapping Schwarz methods within the Trilinos framework and has been used in various applications. FROSch focusses on the implementation of extension-based coarse spaces, such as GDSW (Generalized Dryja-Smith-Widlund) type coarse spaces, which can be build based on the fully assembled system matrix. Through Tpetra, FROSch can make use of the Kokkos programming model and linear algebra kernels from KokkosKernels, which allow for performance portability on different computer architectures.
&gt;&gt; In this talk, we discuss our experiences and challenges porting FROSch to run on a GPU cluster. We present several techniques used to enhance its performance on GPUs along with performance results on Summit Supercomputer; we consider different types of model problems, ranging from simple Poisson an linear elasticity problems to landice simulations of Greenland and Antarctica.</EventDescription>
    <EventParentName>73348-SESS</EventParentName>
    <EventUniqueID>73348-118666</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Utopia: A C++ Library for Parallel Multilevel Solution Methods and for Constrained Non-Linear Problems</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS31</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:20:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:40:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Patrick Zulian</EventSpeakers>
    <EventSpeakerIDs>769510</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118693</EventHandoutURL>
    <EventDescription>Utopia is a C++ embedded domain-specific language designed for parallel non-linear solution strategies. Utopia combines the advantages of high-level programming interfaces with the advantages of scripting languages. On the one hand, it allows using high-level abstractions while providing access to the native low-level data structures. On the other hand, it facilitates expressing complex numerical methods by means of few lines of code. This is achieved by separating the model from the computation, thus allowing us to keep the implementation details hidden from the code of applications such as non-linear solution algorithms and finite element assembly. This separation is realized by using C++ meta-programming and particular evaluation strategies which allow mapping an abstract representation of the computation to the actual code computing the result. In this talk, we showcase the library functionalities and their application to numerical geophysics simulations. In particular, we focus on fractured rock formations in the subsurface which are of crucial importance in a variety of reservoir applications such as geothermal energy extraction, CO2 sequestration, nuclear waste storage, and unconventional oil and gas recovery.
</EventDescription>
    <EventParentName>73358-SESS</EventParentName>
    <EventUniqueID>73358-118693</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Parallel Two-Stage, Divide and Conquer Algorithm in Slate for the Dense Symmetric Eigenvalue Problem</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS32</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 12:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22 12:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mark Gates</EventSpeakers>
    <EventSpeakerIDs>752091</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118688</EventHandoutURL>
    <EventDescription>The SLATE library provides a distributed parallel and GPU accelerated algorithm to solve the dense symmetric eigenvalue problem. We use a two stage reduction from dense to band, then band to tridiagonal, which casts most operations as Level 3 BLAS. Different algorithms can subsequently be used for the tridiagonal eigensolver. We support both the classical QR iteration and a divide and conquer approach. The two stage reduction to tridiagonal also requires two back transformations of the eigenvectors. SLATE provides portability across GPU architectures, currently supporting both CUDA and AMD ROCm platforms.</EventDescription>
    <EventParentName>73360-SESS</EventParentName>
    <EventUniqueID>73360-118688</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Extending Reinit Towards a Formal Specification and Interplay with GPUs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS33</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:15:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Giorgis Georgakoudis</EventSpeakers>
    <EventSpeakerIDs>795518</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118746</EventHandoutURL>
    <EventDescription>Reinit is a recently proposed extension to MPI to support global-restart recovery through a simple but powerful API that minimizes user involvement and changes in the source code of applications. In this talk, we will present extensions to the Reinit fault-tolerance API towards a formal specification that composes with the rest of the MPI specification and facilitates MPI+X programming models. Those extensions include supporting synchronous or asynchronous recovery modes, extending the MPI error-handler interface to select the recovery mode, adding a function to the API to explicitly trigger recovery in synchronous mode, and articulating assumptions in the fault and recovery model supported by Reinit. Further, we will present insights from applying Reinit recovery on MPI applications that use GPUs.</EventDescription>
    <EventParentName>73370-SESS</EventParentName>
    <EventUniqueID>73370-118746</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Towards Access Pattern Aware Checkpointing For Kokkos Applications</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS33</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:20:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:40:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Nigel Tan</EventSpeakers>
    <EventSpeakerIDs>803078</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118747</EventHandoutURL>
    <EventDescription>Portability of resilience despite increasing heterogeneity is vital for the future of HPC systems. While there is an active push towards performance portability, relatively little attention has been dedicated to resilience portability. This talk focuses on the challenges and opportunities of adopting checkpoint-restart techniques (commonly used to provide resilience for HPC systems) in the context of performance portable abstractions and runtimes. Starting from the problem of sparse updates and implications it has on incremental checkpointing, it generalizes the discussion towards the broader question how application data access patterns can be identified and leveraged through memory views for the purpose of automating and accelerating checkpoint-restart at scale.
</EventDescription>
    <EventParentName>73370-SESS</EventParentName>
    <EventUniqueID>73370-118747</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Non-Blocking Resilient Extensions in MPI</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS33</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 12:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22 12:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Aurélien Bouteiller</EventSpeakers>
    <EventSpeakerIDs>780470</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118756</EventHandoutURL>
    <EventDescription>The resilient extensions to the MPI Standard under investigation as part of the User Level Failure Mitigation (ULFM) propose a set of blocking capabilities to detect, notify and recover from permanent process failures. The key principle is that no MPI call (point-to-point, collective, RMA, IO, ...) can block indefinitely after a failure, but must either succeed or raise an MPI error informing the developer of a drastic change in the number of ranks in the parallel application. However, all these extensions are blocking in their nature, resulting in a recovery process that behaves sequentially. A set of new extensions, entirely non-blocking, will allow applications to construct faster-changing worlds, and fulfill the resilient needs of more dynamic applications. We will present these extensions, highlight how they impact applications and provide some early experimental data.
</EventDescription>
    <EventParentName>73370-SESS</EventParentName>
    <EventUniqueID>73370-118756</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Experiences with Chapel in Cosmology from Data Analysis to Simulations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS34</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:20:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:40:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>2900</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Nikhil Padmanabhan</EventSpeakers>
    <EventSpeakerIDs>803830</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118719</EventHandoutURL>
    <EventDescription>
Addressing problems in cosmology require analyzing large amounts of data, either from simulations or observations. At the same time, extracting the maximal amount of information from the data often requires researchers to explore new algorithms, which are most easily developed and explored on small datasets on single machines. It is therefore very useful to have the ability to (relatively) seamlessly scale up algorithms from simple serial implementations to fully distributed parallel implementations. Chapel provides a promising answer to this problem.  This talk will describe personal experiences using Chapel for research in cosmology, describing how Chapel has allowed us to easily scale up algorithms. While I will present performance numbers, I will also emphasize the productivity aspects of the language, an essential consideration for a researcher. For concreteness, I will focus on a single problem: the astrophysical simulation of ultralight dark matter, which requires solving  coupled Schrodinger and Poisson equations. I will start by describing how Chapel's distributed-parallel features allowed us to simply extend local 1D FFTs (as implemented in FFTW) to a 3D distributed setting and describe its performance. I will then turn to how we implemented our simulation and associated analysis codes in Chapel, highlighting the relative closeness between the mathematical description of the algorithm and the fully distributed-parallel code. Based on our experiences, I will attempt to draw some general lessons on Chapel's suitability for scientific analyses.

</EventDescription>
    <EventParentName>73366-SESS</EventParentName>
    <EventUniqueID>73366-118719</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Optimizations of Lattice H-Matrix-Vector Multiplication for Modern Supercomputers</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS35</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:20:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:40:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tetsuya Hoshino</EventSpeakers>
    <EventSpeakerIDs>751636</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118620</EventHandoutURL>
    <EventDescription>The Hierarchical matrices (H-matrices) are used to approximate the dense matrices that appear in the Boundary Element Method (BEM). Accelerating the Hierarchical Matrix Vector multiplication (HiMV) is important for fast solution of the simultaneous linear equations of BEM, but it has not been sufficiently studied compared to the dense matrix vector multiplication (GEMV) and the sparse matrix vector multiplication (SpMV). In this study, in order to speed up HiMV, we have optimized it for modern multi-core and many-core processors while considering the complexity of H-matrix. We also improved the scalability by developing a communication hiding method for the lattice H-matrix method, which is an extension of the H-matrix for distributed memory environments. Using H-matrices of various shapes appearing in the electrostatic field BEM analysis, HiMV was evaluated on a cluster with A64FX, Intel Xeon Phi Knights Landing and Intel Xeon Cascade Lake processors.
</EventDescription>
    <EventParentName>73330-SESS</EventParentName>
    <EventUniqueID>73330-118620</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Modern Mixed-Precision Methods in Portable C++ for Accelerated Hardware Platforms</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS72</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:30:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:50:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Neil Lindquist</EventSpeakers>
    <EventSpeakerIDs>789995</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118776</EventHandoutURL>
    <EventDescription>Mixed precision methods are growing increasingly popular for numerical linear algebra.  However, developing and testing these methods often requires repeatedly implementing the same routine with different combinations of datatypes.  Furthermore, adding support for hardware accelerators often requires reimplementing codes for the differences in API for vendor libraries.  Fortunately, C++ features, such as templating, allow for implementing a method once in a generic manner without significant loss of performance or readability.</EventDescription>
    <EventParentName>73372-SESS</EventParentName>
    <EventUniqueID>73372-118776</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Microscale Cardiac Electrophysiology on Exascale Supercomputers</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS36</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:35:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mark Potse</EventSpeakers>
    <EventSpeakerIDs>730056</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118774</EventHandoutURL>
    <EventDescription>The bidomain model of cardiac electrophysiology has made it possible
to simulate entire human hearts, by homogenizing the heart's two
billion myocytes and their interconnections and replacing them by only
a few million model elements. This simplification has allowed enormous
progress in the area of cardiac simulations with a large impact on
cardiology itself. However, bidomain models cannot represent the
electrical interactions between individual cells or small
groups of cells. These interactions are thought to play a crucial
role in arrhythmia in ageing hearts and in a swath of ischemic,
infectious, hypertrophic, and inheritable cardiomyopathies. In order
to understand these interactions and to learn to recognize their
electric signatures, we must model the heart cell by cell. This
problem is challenging due to its size and complexity, and wil require
exascale supercomputers to be solved on a whole heart. We are working
on this problem with an international, interdisciplinary consortium
involving computer scientists, applied mathematicians, biomedical
engineers, and medical researchers. In this talk I will outline our
motivation for this work and initial results of the project.
</EventDescription>
    <EventParentName>73365-SESS</EventParentName>
    <EventUniqueID>73365-118774</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Parallel Simulation of Non-Equilibrium Dynamics of a Quantum Many-Body System</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS37</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:45:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jia Yin</EventSpeakers>
    <EventSpeakerIDs>796647</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118690</EventHandoutURL>
    <EventDescription>The dynamical properties of a non-equilibrium quantum many-body system can be obtained from the solution of the Kadanoff-Baym equations (KBEs), which are a set of nonlinear integral differential equations satisfied by a two-time Green’s function. The numerical procedure for solving KBEs has a high computational complexity in both the floating point operations and memory footprint.  We will describe a parallelization strategy for solving the KBEs efficiently within a small time window and using the solution snapshots to predict long time dynamics of the Green’s function by performing a parallel dynamic mode decomposition.</EventDescription>
    <EventParentName>73361-SESS</EventParentName>
    <EventUniqueID>73361-118690</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Accelerating Scientific Computing with Dataflow at Wafer Scale</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS88</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>801</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Natalia Vassilieva</EventSpeakers>
    <EventSpeakerIDs>803877</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118780</EventHandoutURL>
    <EventDescription xsi:nil="true" />
    <EventParentName>73327-SESS</EventParentName>
    <EventUniqueID>73327-118780</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Scalable Fine-Grid Empirical Performance Modeling via Tensor Completion</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP0</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  6:00:00 PM</EventStartTime>
    <EventEndTime>02/24/22  8:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Edward Hutter</EventSpeakers>
    <EventSpeakerIDs>780246</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118740</EventHandoutURL>
    <EventDescription>Rising architectural complexity demands aggressive kernel parameterization to achieve performance portability across an increasingly diverse array of architectures.
  Latent patterns in kernel performance are often localized across configuration spaces and consequently difficult to model.
  We observe that many multivariate kernels feature execution times that exhibit low-rank structure.
  We therefore develop performance models that characterize a multivariate kernel's performance as a low-rank tensor represented explicitly as a summation of multi-dimensional outer products (i.e., CP Decomposition).
  We leverage tensor completion to learn these expansions from a sparse dataset, which generalizes parametric linear regression performance models by implicitly capturing interactions and (nonlinear) correlations among kernel parameters.
  Systematic increase in tensor rank improves prediction accuracy, which we show to be necessary to model nonlinear behavior present at fine-grained scales.
  We demonstrate that the enhanced scalability offered by low-rank representations enables performance prediction accuracy to exceed that achieved by state-of-the-art regression methods across a suite of kernels.
</EventDescription>
    <EventParentName>73765-SESS</EventParentName>
    <EventUniqueID>73765-118740</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Some HPC-Specific C++ Extensions in Llvm</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP0</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  6:00:00 PM</EventStartTime>
    <EventEndTime>02/24/22  8:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Pawel Radtke</EventSpeakers>
    <EventSpeakerIDs>803909</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118798</EventHandoutURL>
    <EventDescription>Even though DSLs gain popularity, C++ remains a dominant language for scientific computing in HPC.
Many selling points of DSLs can directly be explained from four shortcomings of
C++, where the language design aligns inaptly with the hardware.
(i) 
The C++ language yields classes (structs) with a large memory footprint.
(ii)
The C++ language naturally leads towards an AoS data design as a class 
is the primary modelling entity for programmers.
(iii) 
The C++ language does not offer built-in support for distributed 
memory parallelisation (MPI).
(iv)
The C++ language lacks support for multifaceted (floating point) data precision.
We present a pragma-based LLVM extension which allows computational scientists to annotate their
programs with data layout, MPI and precision information.
Our compiler extension then yields native MPI and C++ code.
This greatly improves the productivity of numerical developers and hides realisation complexity.
</EventDescription>
    <EventParentName>73765-SESS</EventParentName>
    <EventUniqueID>73765-118798</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>HDBSCAN* on GPUs: Can We Do It Better?</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP0</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  6:00:00 PM</EventStartTime>
    <EventEndTime>02/24/22  8:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Piyush Sao</EventSpeakers>
    <EventSpeakerIDs>790070</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119090</EventHandoutURL>
    <EventDescription>HDBSCAN* is a recent clustering algorithm for a multi-dimensional data. It addresses several drawbacks of the original DBSCAN algorithm, such as requiring expert knowledge to tune the parameters or the inability to find clusters of different densities. Despite sharing a similar name, the underlying computation is entirely different: first constructing a minimum spanning tree; then forming a dendrogram, and finally producing a flat clustering. Parallelizing each part is a challenging task, more so on GPUs. We present novel algorithms for each phase, all running on GPUs.  

We are interested in applying HDBSCAN* to detect clusters in a large cosmological data with tens of millions of data points (galaxies!). Therefore, we currently limit ourselves to the two and three-dimensional datasets for the experiments. We describe the bottlenecks we encountered, and our current solution for them. The question, however, remains: can we do any better?

</EventDescription>
    <EventParentName>73765-SESS</EventParentName>
    <EventUniqueID>73765-119090</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>SIAG/Supercomputing Career Prize: Scaling in Space and Time</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>SP2</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  9:25:00 AM</EventStartTime>
    <EventEndTime>02/25/22  9:55:00 AM</EventEndTime>
    <EventFilter>PP22|Prize Speaker</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Robert Falgout</EventSpeakers>
    <EventSpeakerIDs>708034</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119937</EventHandoutURL>
    <EventDescription>My career has centered on developing algorithms and software that scale with the next generation of high-performance computers and high-fidelity simulations. As such, multigrid methods have been a major research focus since they have the potential to exhibit optimal-order parallel and computational complexity. In this talk, I will discuss the role of multigrid methods in scientific computing, which have served mainly as fast solvers for ill-conditioned spatial systems and are now being developed in other areas such as parallel time integration, an approaching paradigm shift for computing.

	
</EventDescription>
    <EventParentName>73832-SESS</EventParentName>
    <EventUniqueID>73832-119937</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T09:25:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Multi-Model Fault-Tolerant and Malleable Mpi+x Parallel Programming with Transparent Checkpointing and Next-Generation RDMA Protocols</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS38</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  3:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Anthony Skjellum</EventSpeakers>
    <EventSpeakerIDs>803859</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118748</EventHandoutURL>
    <EventDescription>The authors detail multiple models of fault-tolerance with MPI+X (X=GPUs or threads) and discuss both explicit and transparent checkpointing as a collaborative aspect of supporting both malleability and fault-tolerance in such programs.  Discussion of collaboration between the fault-tolerant MPI program and the fault-tolerant model, as well as between the fault-tolerant model(s) and the checkpoint system are covered.  Aspects of malleability, under consideration in for MPI-5 are covered too, as complementary. Discussion of robust operation in coverage cloud-HPC environments will ephemeral/spot resources is included.   Aspects of MPI semantics provide hints as to where transparent checkpointing and malleability can most appropriately cope with MPI message activity and handle both quiescent and non-quiescent state.

Key contributions of this work include integration of CRAC+MANA from Northeastern with the UTC ExaMPI research implementation of MPI extended to support both the MPI Stages and FA-MPI models of fault-tolerant MPI.  Implications for explicit checkpointing libraries like SCR are also discussed. The potential for integrating malleability with message re-routing through advanced RDMA techniques is mentioned, particularly involve RVMA (aka, RDMA 2.0)  as a further source of resilience and reconfigurability.</EventDescription>
    <EventParentName>73371-SESS</EventParentName>
    <EventUniqueID>73371-118748</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Inverse-Deletion BFS - Revisiting Static Graph BFS Traversals with Dynamic Graph Operations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS43</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:45:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>500</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Oded Green</EventSpeakers>
    <EventSpeakerIDs>780304</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116756</EventHandoutURL>
    <EventDescription>Breadth-First Search (BFS) traversals appear in a wide range of applications and domains. BFS traversals determine the distance between key vertices and the remaining vertices in the network. The distance between the vertices often referred to as the number of hops, is the shortest path between a root and the remaining vertices in the in the graph. Given its applicability across multiple domains, BFS has received significant attention from both the theoretical and applied communities, including many algorithms for parallelizing BFS. 
BFS traversals are typically conducted on a snapshot of a graph (commonly referred to as a static graph).
In this paper, we show a novel algorithm for executing a BFS traversal. While our algorithm also uses a static graph for its input, we show how to execute a BFS traversal by using dynamic graph operations. Specifically, in each BFS level, we remove the subset of the edges that are not necessary for the next phase. These edges do not impact finding the vertices in the next level and these reduce the number of random memory accesses. We show a top-down BFS variation of our new algorithm. 
While our implementation does not outperform state-of-the-art implementations its performanance is competitive. Furthermore, it shows a novel way to implement BFS and opens up many research opportunities. 

</EventDescription>
    <EventParentName>72775-SESS</EventParentName>
    <EventUniqueID>72775-116756</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Testing Strategies in a Large Scale Scientific Code – Experiences and Aspirations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS44</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Balint Joo</EventSpeakers>
    <EventSpeakerIDs>803617</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118493</EventHandoutURL>
    <EventDescription>Maintaining correctness of Scientific Codes is important to ensure confidence in the results. In this talk I will discuss various methods of testing that I have encountered during my work with the Chroma Lattice QCD code, including algorithmic testing, regression testing, unit testing and discuss our current status and future aspirations.</EventDescription>
    <EventParentName>72832-SESS</EventParentName>
    <EventUniqueID>72832-118493</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>An Overview of Particles in Amrex</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS46</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:30:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>3000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Andrew Myers</EventSpeakers>
    <EventSpeakerIDs>785662</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116708</EventHandoutURL>
    <EventDescription>Particle methods are used to model a variety of phenomena in scientific computing, including the dynamics of charged particle beams subject to electric and magnetic fields in accelerators, dark matter in an expanding universe, the solids phase in chemical looping reactors, and many more. The AMReX block-structured adaptive mesh refinement framework provides a set of tools for implementing such methods that is particularly tailored towards particles that interact with a nested hierarchy of mesh data. The addition of particles to structured mesh calculations introduces additional challenges stemming from the irregularity and dynamic nature of the data layout, memory access patterns, and connectivity information. In this talk, I will give an overview of the particle functionality provided in the AMReX framework and demonstrate how it can be used to build scalable, performance-portable adaptive mesh refinement applications that implement a variety of particle methods across a number of different scientific domains.
</EventDescription>
    <EventParentName>72757-SESS</EventParentName>
    <EventUniqueID>72757-116708</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>CFD-Dem Simulations of Reacting Fluidized Beds with MFIX-Exa</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS46</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>3000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Roberto Porcu</EventSpeakers>
    <EventSpeakerIDs>796070</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116710</EventHandoutURL>
    <EventDescription>MFIX-Exa is a CFD-DEM HPC code under development for simulating chemically reacting multiphase flows and optimizing modern complex geometries for chemical looping reactors (CLRs). Specifically, we model the fluid using a low Mach number formation with a multicomponent ideal gas state equation, and we discretize the particle-unresolved fluid equations using an embedded boundary (EB) aware Godunov scheme with an approximate projection. We represent particles by a soft-sphere spring-dashpot model and evolve them using a forward Euler method with subcycling. The fluid and particle models are coupled through a volume fraction field and interphase mass, momentum, and energy transfer. These coupling terms are computed at the p-th particle position, mapping continuous fluid properties to the particle location through an interpolation operator. The resulting particle properties are transformed into continuous fields through a unit normal transfer kernel. We present numerical results simulating a CLR prototype and benchmark it against experimental data to validate the approach. Besides the prototype's simplifications and reduced scale, these results represent a milestone towards realistic CLR simulations.

</EventDescription>
    <EventParentName>72757-SESS</EventParentName>
    <EventUniqueID>72757-116710</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Scalable Particle-In-Cell Algorithm For Neutrino Quantum Kinetics</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS46</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>3000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Don Willcox</EventSpeakers>
    <EventSpeakerIDs>785746</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116711</EventHandoutURL>
    <EventDescription>Neutron star mergers, or kilonovae, are one of the most intense subjects of astrophysics research in recent years after the LIGO collaboration observed them to emit strong gravitational wave radiation. We have yet to understand many of the details of the merger dynamics, and one of the remaining open questions is to what extent neutrino flavor transformations determine nucleosynthesis for the heavy elements these events produce. Answering this question requires sophisticated computing resources to solve the underlying nonlinear equations for neutrinos in six-dimensional phase space. To achieve this, we developed Emu, a particle-in-cell simulation code that solves the neutrino quantum kinetics equations with arbitrary angular resolution. Emu represents the neutrino distribution using a set of computational particles, each having a unique position, momentum, and quantum state. For flexibility, Emu’s C++ kernels for evolving each particle’s quantum state are all symbolically generated using Sympy. Emu is built on top of the AMReX computational framework to take advantage of AMReX’s scalable, domain-distributed particle-mesh routines, and Emu is performance portable on CPUs and GPUs. Emu thus enables detailed multidimensional studies of the neutrino fast flavor instability, resolved in space, angle, and time. We will discuss Emu’s design and scaling on modern supercomputing platforms along with our ongoing studies of the fast flavor instability in neutron star mergers.

</EventDescription>
    <EventParentName>72757-SESS</EventParentName>
    <EventUniqueID>72757-116711</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>GPU Support in SUNDIALS Time Integrators and Nonlinear Solvers</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS48</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:30:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>David Gardner</EventSpeakers>
    <EventSpeakerIDs>763182</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118434</EventHandoutURL>
    <EventDescription>SUNDIALS is a suite of robust and scalable time integrators and nonlinear solvers utilized in a wide variety of applications on computing systems ranging from desktop machines to super computers. With the shift to heterogenous architectures in exascale systems where most of the computational power comes from GPUs there has been a significant development effort in SUNDIALS to support hardware from various vendors and a variety of programming models as well as provide interfaces to GPU-enabled algebraic solvers. In this talk we will overview our GPU porting strategy leveraging SUNDIALS’ object-oriented design to offloading computations while keeping algorithmic control on the CPU, summarize the current GPU capabilities of SUNDIALS, and highlight applications uses and recent results on GPU systems. 

This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. LLNL-ABS-826702.</EventDescription>
    <EventParentName>72873-SESS</EventParentName>
    <EventUniqueID>72873-118434</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Advances in GPU Methodologies in AMReX</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS48</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:35:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:55:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kevin Gott</EventSpeakers>
    <EventSpeakerIDs>786064</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118448</EventHandoutURL>
    <EventDescription>The AMReX software framework for massively parallel block-structured AMR applications has undergone extensive improvements to run efficiently on GPU supercomputers, especially the DOE exascale systems Perlmutter, Frontier and Aurora. The latest generation of computing technologies has led to additional studies in performance and algorithmic design with a focus on usability and scientific achievement. These advancements have demonstrated substantial gains across the AMReX suite of applications, including WarpX, Nyx, Castro, Pele, MFix-Exa and AMR-Wind.

This talk will give an overview of recent AMReX advancements in GPU design and implementation. Topics include advancements in porting to AMD and Intel software frameworks, advances and remaining deficiencies in GPU performance and new technologies explored to enhance AMReX’s capabilities and prepare for the next-generation of scientific research.</EventDescription>
    <EventParentName>72873-SESS</EventParentName>
    <EventUniqueID>72873-118448</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>FEniCSx - a Modular PDE Framework: Developing for Heterogeneous Computing</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS50</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jørgen Dokken</EventSpeakers>
    <EventSpeakerIDs>803296</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117781</EventHandoutURL>
    <EventDescription>In the latest development version of the popular FEniCS Finite Element framework, we have increased the flexibility of problem composition. In general, meshing, FEM kernel generation, assembly, and solve phases can now be considered as pluggable units. In this context, we have investigated performance portability of the central kernel generation and assembly phase with SYCL, a C++ library that allows us to easily target both CPU and GPU. With assembly "on device", this gives us the flexibility to call device-specific solvers, for example, amgX, and experiment with matrix-free methods. Obtaining the best performance requires attention to detail at the lowest level, as varying memory access and loop transformations can make large timing differences, especially for higher-order kernels. Despite recent improvements in memory capacity, GPU implementations are still limited by the amount of memory available and data transfer rates between devices. We will give an overview of the approaches we have taken to obtain the best performance we can from the available hardware. We will also discuss how carefully arranging memory transfer and allocations can reduce latency and increase throughput in different accelerators.

</EventDescription>
    <EventParentName>73120-SESS</EventParentName>
    <EventUniqueID>73120-117781</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>GPU Accelerated Online Search for Gravitational Waves</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS53</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>500</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Zhihui Du</EventSpeakers>
    <EventSpeakerIDs>802672</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116762</EventHandoutURL>
    <EventDescription>Gravitational waves (GWs) open a completely new window for us to learn about the universe. In this talk, a Graphics Processing Unit (GPU) accelerated time-domain low-latency method to search for GWs from coalescing binaries of compact objects will be introduced. The objective of this research is to facilitate fast detection of GWs with a very low delay to allow prompt electromagnetic follow-up observations. Our optimizing methods on GPU can achieve significant performance improvement by matching the application, parallel program and manycore processors well together. The proposed method and the corresponding SPIIR (Summed Parallel Infinite Impulse Response) data processing pipeline have been successfully used in LIGO's GW events detection. </EventDescription>
    <EventParentName>72776-SESS</EventParentName>
    <EventUniqueID>72776-116762</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Streaming Benchmarks on the Graphcore Intelligence Processing Unit</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS54</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:50:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>801</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Alexandra Porter</EventSpeakers>
    <EventSpeakerIDs>803741</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118785</EventHandoutURL>
    <EventDescription>The Graphcore IPU is a new massively parallel processor with architecture designed to optimize AI computations. In this talk we will discuss implementing benchmarks originally developed for traditional compute methods on IPUs. Code for IPUs is developed using the Poplar SDK, which was co-developed by Graphcore with the IPU and supports both direct programming with C++ and integration with standard machine learning frameworks. To take full advantage of the architecture of the IPU, we implement the benchmarks with maximal parallelization and seek to limit communication between pairs of processes in locations with relatively higher communication costs. Finally, we show performance results and comparisons to other implementations of the benchmarks.</EventDescription>
    <EventParentName>73325-SESS</EventParentName>
    <EventUniqueID>73325-118785</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Extensible Computational Fluid Dynamics in Julia with Trixi.jl</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS55</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Michael Schlottke-Lakemper</EventSpeakers>
    <EventSpeakerIDs>803764</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118651</EventHandoutURL>
    <EventDescription>In recent years, the Julia programming language has made considerable advances
in the scientific computing community and positions itself as a viable alternative to
established high-performance computing languages such as Fortran and C/C++. Its
paradigm of multiple dispatch with just-in-time compilation allows one to easily
reuse existing code and surgically extend its functionality without compromising
the computational performance.
In this talk, we will present some recent results on efficient
implementations of entropy stable and kinetic energy preserving discontinuous
Galerkin schemes used for the discretization of the compressible Euler
equations. These implementations are evaluated in two open source codes: Trixi.jl,
an adaptive high-order simulation framework
written in Julia, and FLUXO, a Fortran-based simulation tool for computational
fluid dynamics. We will discuss several implementation techniques for such
summation-by-parts flux differencing operators and evaluate their impact on the

execution speed. A particular focus will be on demonstrating how Julia's
extensibility features can be used to achieve these efficiency improvements
without having to change the existing implementation, and how the resulting
performance is on par with a mature Fortran code.</EventDescription>
    <EventParentName>73337-SESS</EventParentName>
    <EventUniqueID>73337-118651</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Analysis of a Three-Level Variant of Parareal</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS56</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Felix Kwok</EventSpeakers>
    <EventSpeakerIDs>797393</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118626</EventHandoutURL>
    <EventDescription>In this talk, we present a three-level variant of the parareal
algorithm that uses three propagators at the fine, intermediate and
coarsest levels.  The fine and intermediate levels can both be run in
parallel; only the coarsest level propagation is completely
sequential. We compare our algorithm with three-level MGRIT, and we
present a convergence analysis that uses parareal-type assumptions,
i.e., those that involve Lipschitz constants on the propagators. We
present numerical experiments to illustrate how sharp the estimates are
for various time dependent problems.


</EventDescription>
    <EventParentName>73331-SESS</EventParentName>
    <EventUniqueID>73331-118626</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Unified Analysis Framework for Iterative Parallel-in-Time Algorithms</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS56</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Thibaut Lunet</EventSpeakers>
    <EventSpeakerIDs>803753</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118627</EventHandoutURL>
    <EventDescription>Parallel-in-time (PinT) methods have received renewed attention in the last two decades, mainly motivated by the advent of new supercomputing resources that provide more and more concurrency. Four of the most widely studied methods are Parareal (Lions, Maday, Turinici), PFASST (Minion and Emmett), MGRIT (Falgout, Friedhoff, Kolev, MacLachlan, Schroder) and a specific form of Space-Time Multigrid (Gander and Neumueller). All those algorithms have a common aspect: they are iterative PinT "across-the-steps" methods, i.e they represent the time sequential solution process as a large linear or nonlinear system and solve it by iterating on all time steps simultaneously. While various convergence analyses exist for each algorithm separately, it is difficult to relate them and compare convergence of these iterative PinT methods when applied to various model problems, and in applications.

In this talk, we present a new approach that lets us analyze the convergence of these four iterative algorithms in a common framework. Following an idea of Hairer and Gander already used to analyze Parareal convergence, this framework is based on an abstract view of each iterative PinT algorithm and provides error bounds for the Dahlquist equation, the fundamental time-dependent test problem. Furthermore, it provides understanding of the different convergence mechanisms in each algorithm 
and its level of abstraction could eventually lead to novel PinT algorithms.</EventDescription>
    <EventParentName>73331-SESS</EventParentName>
    <EventUniqueID>73331-118627</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Asynchronous Truncated Multigrid-Reduction-in-Time (AT-MGRIT)</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS56</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jens Hahne</EventSpeakers>
    <EventSpeakerIDs>803755</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118632</EventHandoutURL>
    <EventDescription>In this talk, we present the new "asynchronous truncated multigrid-reduction-in-time" (AT-MGRIT) algorithm for introducing time parallelism to the solution of discretized time-dependent problems. The new algorithm is based on the multigrid-reduction-in-time (MGRIT) approach, which, in certain settings, is equivalent to another common multilevel parallel-in-time method, Parareal. In contrast to Parareal and MGRIT that both consider a global temporal grid over the entire time interval on the coarsest level, the AT-MGRIT algorithm uses truncated local time grids on the coarsest level, each grid covering certain temporal subintervals. These local grids can be solved completely independent from each other, which reduces the sequential part of the algorithm and, thus, increases parallelism in the method. Here, we study the effect of using truncated local coarse grids on the convergence of the algorithm, and present parallel results on challenging nonlinear problems.
</EventDescription>
    <EventParentName>73331-SESS</EventParentName>
    <EventUniqueID>73331-118632</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Robust Multigrid for Stokes with Extreme Viscosity Variation using Firedrake</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS59</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yu-hsuan Shih</EventSpeakers>
    <EventSpeakerIDs>790036</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117787</EventHandoutURL>
    <EventDescription>In this talk, I will present augmented Lagrangian Schur complement preconditioners and robust multigrid methods for incompressible Stokes problems with extreme viscosity variations, with a focus on its implementation using the open source library Firedrake. Such Stokes systems arise, for instance, upon linearization of nonlinear viscous flow problems, and they can have severely inhomogeneous and anisotropic coefficients. The augmented Lagrangian formulation for the incompressibility constraint makes the Schur complement easier to approximate, but results in a nearly singular (1,1)-block in the Stokes system. To cope with the near-singularity of the (1,1)-block, we use a multigrid scheme with discretization-dependent smoother and transfer operators. We confirm the robustness of the multigrid scheme and the overall efficiency of the solver using numerical examples with scalar and with anisotropic fourth-order tensor viscosity arising from linearization of a viscoplastic constitutive relation. With Firedrake's parallelism, we present scalability results using up to 28,672 parallel tasks for problems with up to 1.6 billion unknowns and a viscosity contrast up to ten orders of magnitude. 
</EventDescription>
    <EventParentName>73121-SESS</EventParentName>
    <EventUniqueID>73121-117787</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Partition Improvement for High-Order Unstructured Mesh and Particle-In-Cell Simulations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS60</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Gerrett Diamond</EventSpeakers>
    <EventSpeakerIDs>763345</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117705</EventHandoutURL>
    <EventDescription>One of the most common techniques for partitioning unstructured meshes is to construct a graph abstraction of the mesh and perform a graph partitioning method. When these partitioning methods do not create a sufficiently good partition of the mesh, multicriteria diffusive partition improvement methods can be employed to further improve performance in the target application. In this presentation, we explore alternative graph constructions of the unstructured mesh to specifically target the application requirements allowing better diffusive partition improvements. Diffusive load balancing is provided by the EnGPar library that employs a multihypergraph to represent the unstructured mesh. We present results of EnGPar partitioning the unstructured mesh for high-order finite element simulations and dynamic load balancing of particles in distributed unstructured mesh particle-in-cell simulations.</EventDescription>
    <EventParentName>73094-SESS</EventParentName>
    <EventUniqueID>73094-117705</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Fast High Order GPU Simulation Support for Exascale Applications</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS61</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jean-Sylvain Camier</EventSpeakers>
    <EventSpeakerIDs>758314</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118438</EventHandoutURL>
    <EventDescription>MFEM is an open-source, lightweight, flexible and scalable C++ high-order finite element methods library targeting portability, scalability and high-performance computing. This talk will present the GPU research and development activities of the MFEM library within the U.S. Department of Energy Exascale Computing Project co-design Center for Efficient Exascale Discretization.
We will discuss the GPU developments strategies in several components and our work on performance optimizations for large-scale architectures, targeting high-order finite-element algorithms for high-order applications on both NVIDIA and AMD GPU-accelerated platforms. We will report performance and capability improvements in several benchmarked proxies, miniapps and for CEED's applications on both systems.

</EventDescription>
    <EventParentName>72874-SESS</EventParentName>
    <EventUniqueID>72874-118438</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Automatic Discovery of Implementation Rules for Fast GPU + MPI Operations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS61</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Carl Pearson</EventSpeakers>
    <EventSpeakerIDs>803645</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118440</EventHandoutURL>
    <EventDescription>Developing a high-performance implementation of a distributed computational kernel for high-performacing computing is increasingly challenging.
Systems are composed of heterogenous computational resources, and limited communication performance demands an asynchronous application design.
Even if high-performance computation and communication libraries are available. the challenge becomes the best coordination of the provided operations to create an optimal result.
This work presents a system that automatically generates design rules for a high-performance implementation of a compound operation provided as a dependence graph.
The system searches among valid schedules to determine the fastest arrangement of operations.
A post-processing step on the results of the search yields interpretable design rules.
The fast implementation can be used directly, or experts can use the design rules to create a high-performance implementation.
</EventDescription>
    <EventParentName>72874-SESS</EventParentName>
    <EventUniqueID>72874-118440</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>An Inertia-Free Approach for Nonlinear Optimization on GPU</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS62</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1200</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Nai-Yuan Chiang</EventSpeakers>
    <EventSpeakerIDs>802989</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118289</EventHandoutURL>
    <EventDescription>We present a filter line-search interior-point solver that does not require inertia information of the KKT system.
The proposed approach performs curvature tests along the search step to detect negative curvature and correct it to guarantee global convergence. This feature enables the use of a wide range of linear algebra strategies and libraries, which is essential to tackle large-scale problems on modern computing architectures. Our numerical tests demonstrate the inertia-free approach on multiple CPUs (MPI) and on GPU.</EventDescription>
    <EventParentName>73021-SESS</EventParentName>
    <EventUniqueID>73021-118289</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Managing High-Throughput Application Workloads: Findings and Recommendations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS63</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Michela Taufer</EventSpeakers>
    <EventSpeakerIDs>784892</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116859</EventHandoutURL>
    <EventDescription>High-throughput applications, in which application workload consists of a large ensemble of self-contained tasks and application performance is measured by the number of tasks completed per unit of time, are vital for scientific discovery. Managing high-throughput application workloads is not an easy task: it requires automation services that include workflow capabilities for managing all the tasks and all the input/output files across different runs of a high-throughput applications ensemble. These increasing complexities hinder the ability of scientists to generate robust science, which we define as the capacity of high-throughput applications to scale in performance, to exhibit trustworthiness, and to assure reproducibility. 

This talk presents recommendations for designing and implementing software systems for robust science across critical high-throughput applications. The findings were collected through a virtual mini-workshop in May 2021 called a Virtual World Cafe (VWC). In the VWC, we engaged communities of software developers to share state of the art recommendations through structured conversational processes: participants were distributed across several breakout sessions in an online meeting, with participants switching sessions periodically and getting introduced to the previous discussion at their new session by a session lead.


</EventDescription>
    <EventParentName>72771-SESS</EventParentName>
    <EventUniqueID>72771-116859</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Yin and Yang: Balancing Cloud and HTC Workloads</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS63</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kate Keahey</EventSpeakers>
    <EventSpeakerIDs>802730</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116860</EventHandoutURL>
    <EventDescription>The NSF-funded Chameleon project (www.chameleoncloud.org) provides an OpenStack-based bare metal reconfigurable cloud that supports systems research, education, and emergent application projects. The facility has been in existence for 6+ years, and has served ~6,000 users working primarily on systems experimentation. These types of experiments require interactive access; demand tends align with the academic calendar and conference deadlines which means that during certain periods the infrastructure is underutilized. High Throughput Computing (HTC) jobs consist of independent tasks that do not require interactivity and are resilient to resource loss. Cloud and HTC workloads thus have complementary characteristics that could be leveraged and combined to ensure the best possible utilization of resources and thus the lowest possible price for interactive usage. This talk will describe the mechanisms developed by the Chameleon project to enable Open Science Grid (OSG) workloads to run on the system; these workloads are invisible to Chameleon users, do not impact their usage of the system, and utilize only the resources that are not used for experimentation. I will present the architecture and implementation of the system, the methods and policies we use to ensure efficient usage, and share preliminary results from an actual deployment.
</EventDescription>
    <EventParentName>72771-SESS</EventParentName>
    <EventUniqueID>72771-116860</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Multi-Scale Methods for Machine Learning</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS64</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:50:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Inderjit Dhillon</EventSpeakers>
    <EventSpeakerIDs>36345</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118802</EventHandoutURL>
    <EventDescription>Many modern applications require us to very quickly find relevant results from an enormous output space of potential candidates, for example, finding the best matching product from a large catalog or suggesting related search phrases on a search engine. The size of the output space for these problems can be in the millions to billions. Moreover, observational or training data is often limited for many of the so-called “long-tail” of items in the output space. Given the inherent paucity of training data for most of the items in the output space, developing machine learning models that perform well for spaces of this size is a contemporary challenge. In this talk, I will present a multi-scale machine learning framework called Prediction for Enormous and Correlated Output Spaces (PECOS). PECOS proceeds by first building a hierarchy over the output space using unsupervised learning, and then learning a machine learning model that makes predictions at each level of the hierarchy. Finally, the multi-scale predictions are combined to obtain an overall predictive model. This leads to an inference method that scales logarithmically with the size of the output space. A key to obtaining high performance is parallel computation, and leveraging sparsity to develop highly efficient sparse matrix routines.

</EventDescription>
    <EventParentName>73377-SESS</EventParentName>
    <EventUniqueID>73377-118802</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Investigations on the use of Hashing for Parallel Graph and Hypergraph Processing</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS65</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:30:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:50:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Somesh Singh</EventSpeakers>
    <EventSpeakerIDs>804304</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119071</EventHandoutURL>
    <EventDescription>Graphs and hypergraphs are widely used as models in many data science and scientific computing applications. This is so as these two discrete mathematical objects correspond naturally to sparse matrices and tensors. We investigate the use of hashing for implementing operations on graphs and hypergraphs in shared memory systems, with an eye towards linear and multi-linear algebraic operations on matrices and tensors.
</EventDescription>
    <EventParentName>73378-SESS</EventParentName>
    <EventUniqueID>73378-119071</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Assessing a Neuromorphic Platform for Scientific Random Sampling</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS66</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Darby Smith</EventSpeakers>
    <EventSpeakerIDs>782527</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116971</EventHandoutURL>
    <EventDescription>Recent advances in neuromorphic algorithm development have shown that neural inspired architectures can efficiently solve scientific computing problems including graph decision problems and partial-integro differential equations (PIDEs).  The latter requires the generation of a large number of samples from a stochastic process. While the Monte Carlo approximation of the solution of the PIDEs converges with an increasing number of sampled neuromorphic trajectories, the fidelity of samples from a given stochastic process using neuromorphic hardware requires verification. Such an exercise increases our trust in this emerging hardware and works toward unlocking its energy and scaling efficiency for scientific purposes such as synthetic data generation and stochastic simulation.

In this talk, we focus our verification efforts on a one-dimensional Ornstein-Uhlenbeck stochastic differential equation.  We sample trajectories of the stochastic process across a variety of parameters on an Intel 8-Loihi chip Nahuku neuromorphic platform.  Using relative entropy as a verification measure, we demonstrate that the random samples generated on Loihi are, in an average sense, acceptable.
</EventDescription>
    <EventParentName>72861-SESS</EventParentName>
    <EventUniqueID>72861-116971</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Accelerating Hypre with GPUs: Strategies and Performance</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS68</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:00:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ulrike Meier Yang</EventSpeakers>
    <EventSpeakerIDs>719805</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118443</EventHandoutURL>
    <EventDescription>With the increasing inclusion of accelerators into current and future high-performance computers, it has become important to enable parallel mathematical libraries to take advantage of the increased performance potential of GPUs. The hypre software library provides high performance preconditioners and solvers for the solution of large sparse linear systems on massively parallel computers with focus on algebraic multigrid (AMG) methods. However, their implementation can be complex. This talk will discuss strategies in achieving portability, new developments in hypre and performance results on GPUs.</EventDescription>
    <EventParentName>72875-SESS</EventParentName>
    <EventUniqueID>72875-118443</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Bridging the Gap: Complex Models and Advanced Uncertainty Quantification</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS69</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:05:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:25:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Linus Seelinger</EventSpeakers>
    <EventSpeakerIDs>781263</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117796</EventHandoutURL>
    <EventDescription>Mathematical models of complex real-world phenomena result
in computational challenges, necessitating advanced software and High Performance Computing systems. Uncertainty quantification (UQ) on such models is even more challenging, since uncertainties essentially increase dimensionality of the problem at hand.

We propose a new approach to coupling arbitrary model codes with advanced uncertainty quantification packages, making it easy to apply advanced UQ methods on challenging models. Further gains are portability of models, separation of concerns between model experts and UQ developers, and the availability of standardized UQ benchmarks.

Finally, we show a demonstration of the approach by applying a massively parallelized Multilevel Markov Chain Monte Carlo method on a shallow water PDE modelling the 2011 Tohoku tsunami, determining its source from buoy data.</EventDescription>
    <EventParentName>73122-SESS</EventParentName>
    <EventUniqueID>73122-117796</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Customizable Physics Kernels in the Glacier Flow Modeling Package Icepack</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS69</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:55:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daniel Shapero</EventSpeakers>
    <EventSpeakerIDs>755456</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117799</EventHandoutURL>
    <EventDescription>In this talk, I'll describe some of the features of the glacier flow modeling package \emph{icepack}. Icepack is built on the finite element modeling library Firedrake, which uses the Unified Form Language to specify variational forms of partial differential equations and implements a compiler from this specification to efficient low-level C code. Firedrake assumes that end users have a concrete idea in mind of what problems they wish to solve, but for domain scientists, the real service that a software package like icepack can provide is the flexibility to try many different problem formulations. For example, two key problems at the forefront of glaciology are (1) how fracture and damage to the ice affects large-scale flow and (2) how the state of the hydrological system beneath a glacier affects sliding, both of which require adding new physics to the system and including more physical fields in the problem formulation. In this talk I'll describe how physics models are specified in icepack and how end users can alter components of these physics models, even to the point of adding new physical fields.
</EventDescription>
    <EventParentName>73122-SESS</EventParentName>
    <EventUniqueID>73122-117799</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Designing an Intelligent Runtime System for Extremely Heterogeneous Computing</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS70</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:00:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jungwon Kim</EventSpeakers>
    <EventSpeakerIDs>803352</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117938</EventHandoutURL>
    <EventDescription>Across embedded, mobile, enterprise, and HPC systems, computer architectures are becoming more heterogeneous and complex. This complexity is causing a crisis in programming systems and performance portability. Several programming systems are working to address these challenges, but the increasing architectural diversity is forcing software stacks and applications to be specialized for each architecture.

This talk argues that a more agile, proactive, and intelligent runtime system is essential to increase performance portability and improve user productivity. To this end, this talk introduces a new runtime system called IRIS. IRIS enables programmers to write portable and flexible programs across diverse heterogeneous architectures from edge to exascale by orchestrating multiple programming platforms in a single execution and programming environment. This talk will present the challenges we have faced while designing IRIS and how we addressed the challenges.
</EventDescription>
    <EventParentName>73178-SESS</EventParentName>
    <EventUniqueID>73178-117938</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Parallel Randomized Algorithms for Tucker Decompositions</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS71</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:05:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:25:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5100</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Rachel Minster</EventSpeakers>
    <EventSpeakerIDs>784918</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118728</EventHandoutURL>
    <EventDescription>The Tucker decomposition is a low-rank tensor decomposition generalizing the matrix SVD to higher dimensions. Traditional algorithms used to compute Tucker decompositions can be computationally expensive as they involve computing the SVD of mode unfolding, which can be quite large. Existing parallel and randomized algorithms have reduced this cost, but computational challenges remain. We propose new parallel randomized algorithms to address these challenges. Using randomized matrix techniques, we accelerate a distributed-memory implementation of the Tucker decomposition to obtain an efficient algorithm that avoids communication. Specifically, we employ a new sketching method that exploits Kronecker structure to accelerate a key computation. We also present probabilistic analysis of the error resulting from the algorithm, as well as numerical results demonstrating the computational benefits of this approach.
</EventDescription>
    <EventParentName>73353-SESS</EventParentName>
    <EventUniqueID>73353-118728</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Numerical Study of a Micro-Macro MGRiT Method for Scale-Separated SDEs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS74</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:05:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:25:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ignace Bossuyt</EventSpeakers>
    <EventSpeakerIDs>803757</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118639</EventHandoutURL>
    <EventDescription>Time-parallel methods can significantly reduce wall clock time for accurate numerical solutions by parallelising across the time dimension. We present and test a micro-macro version of a two-level MGRiT method, in which the fine propagator is based on a (high-dimensional, slow-fast) stochastic microscopic model, and the coarse propagator is based on a low-dimensional approximate effective dynamics at slow time scales. At the microscopic level, we use an ensemble of Monte Carlo particles, whereas the approximate coarse propagator uses the (deterministic) Fokker–Planck equation for the slow degrees of freedom. The required coupling between microscopic and macroscopic representations of the system introduces several design options, specifically on how to generate a microscopic probability distribution consistent with a required macroscopic probability distribution and how to perform the coarse-level updating of the macroscopic probability distribution in a meaningful manner. We numerically study how these design options affect the efficiency of the algorithm in a number of situations.
</EventDescription>
    <EventParentName>73332-SESS</EventParentName>
    <EventUniqueID>73332-118639</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Flexible and Generic Software Design for Lattice Boltzmann Methods on CPU and GPU with Walberla and Lbmpy</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS75</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:00:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Markus Holzer</EventSpeakers>
    <EventSpeakerIDs>803759</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118643</EventHandoutURL>
    <EventDescription>As more and more heterogeneous hardware is used in high-performance computing like GPUs as accelerators, it becomes a demanding challenge to provide compute kernels optimised explicitly for each architecture. Furthermore, in the context of lattice Boltzmann methods, a wide variety of collision models, stencils, boundary conditions, and equilibria formulations exist, making the situation even more demanding. In this talk, the code generator lbmpy is introduced to tackle the problem. It allows defining lattice Boltzmann methods on a symbolic mathematical description. From this description, highly-optimised compute kernels, communication schemes, and boundary conditions automatically adapted to the model's description are generated. The generated compute kernels can run independently on a single machine, or they can be combined with the massively parallel multiphysics framework waLBerla to use domain decomposition and allow for extensive MPI-parallel simulations.</EventDescription>
    <EventParentName>73338-SESS</EventParentName>
    <EventUniqueID>73338-118643</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Structure-Preserving Particle Discretizations of Coulomb Collisions on GPUs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS77</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:00:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Eero Hirvijoki</EventSpeakers>
    <EventSpeakerIDs>803629</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118423</EventHandoutURL>
    <EventDescription xsi:nil="true" />
    <EventParentName>73283-SESS</EventParentName>
    <EventUniqueID>73283-118423</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Comparing the PFASST, MGRIT and Parareal Methods</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS78</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jordi Wolfson-Pou</EventSpeakers>
    <EventSpeakerIDs>803758</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118641</EventHandoutURL>
    <EventDescription>PFASST, MGRIT and Parareal are three of the most popular parallel-in-time methods.  However, these three methods have not been carefully compared in the literature.  One reason for this is that the performance of each method depends on many factors, including the PDE being solved, the implementation specifications, and many method-specific parameters.  In this work, we compare these three methods with the goal of investigating the performance of the algorithms rather than implementations.  We have implemented all three methods in the LibPFASST library to reduce the effect on performance of using different software libraries.  We distill the parameter space down to those that have the greatest impact on the reduction of the error: for MGRIT and Parareal, we vary the time discretization scheme and for PFASST, we vary the number of SDC sweeps.  We conduct experiments on the Cori supercomputer at NERSC using two test problems.  We look at three important results for each method: error versus wall-clock time, error versus number of time steps, and strong scaling.
</EventDescription>
    <EventParentName>73333-SESS</EventParentName>
    <EventUniqueID>73333-118641</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Using Macros to Assemble and Generate Specialized Code for Multiple Devices from a Single Source</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS79</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Thomas Klosterman</EventSpeakers>
    <EventSpeakerIDs>803871</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118768</EventHandoutURL>
    <EventDescription>Scientific software used on high performance computing (HPC) platforms is in a phase of transformation because of a combined increase in the heterogeneity and complexity of models and hardware platforms. Because separate implementations for different platforms is not an option, the computational science community has been looking for mechanisms to express code through abstractions that can be specialized for different platforms. Some approaches have met success through the use of template meta-programming in C++. However, these approaches are inherently inaccessible to non C++ codes. In this presentation, we describe a language agnostic methodology that mimics the behavior of templates as applied in the abstractions described above. We also include an example of code refactoring utilizing this approach.</EventDescription>
    <EventParentName>73374-SESS</EventParentName>
    <EventUniqueID>73374-118768</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Transformation from DSL to Source Code for the Multi-Physics Application Flash-X</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS79</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  3:05:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:25:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Johann Rudi</EventSpeakers>
    <EventSpeakerIDs>752522</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118770</EventHandoutURL>
    <EventDescription>We propose a code transformation toolchain for Flash-X that realizes the
"recipe-to-source" code transformation for Flash-X simulations and that is one
crucial component in order to achive performance portability.  We design a
high-level language to express operations of simulations in so-called
"recipes," which are given as input to the toolchain.  The tools of the code
transformation pipeline include tree-based source code representation
techniques and code orchestration and generation based on control flow graphs.
The generated source code utilizes a new runtime, developed for Flash-X, that
orchestrates dynamic and asynchronous data movement and task execution.
</EventDescription>
    <EventParentName>73374-SESS</EventParentName>
    <EventUniqueID>73374-118770</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Improving Large Scale Data Parallel Training Performance of Horovod</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS80</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  3:05:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:25:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Joshua Romero</EventSpeakers>
    <EventSpeakerIDs>804273</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119051</EventHandoutURL>
    <EventDescription>Horovod is a popular software library for communication in data parallel deep learning applications. While originally written by Uber to enable straightforward data parallel conversion of TensorFlow models to run across a handful of server nodes, the library has grown to support all major frameworks and has been used to run deep learning workloads on some of the world’s largest supercomputing clusters. Additionally, many new features and enhancements have been incorporated into the library to continually improve performance and usability. This talk will provide an overview of Horovod’s framework agnostic design and the implementation of several performance improvements to the original library to improve scalability, illustrated with real-world use cases.</EventDescription>
    <EventParentName>73367-SESS</EventParentName>
    <EventUniqueID>73367-119051</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Massively Parallel Solvers for Buoyancy-Driven Convection in Stokes Flow</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS82</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Nils Kohl</EventSpeakers>
    <EventSpeakerIDs>780404</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118814</EventHandoutURL>
    <EventDescription>Challenges in Earth mantle convection motivate the development of massively parallel solvers for advection dominated transport in 
viscous fluids. We present our hierarchical hybrid grids (HHG) based approach that couples a geometric multigrid solver for the Stokes equations to an Eulerian-Lagrangian method for the  advection-diffusion equation on block-structured tetrahedral meshes. 

Here, we use a matrix-free approach, where the stencil-based kernels are partly generated from an abstract description in Python. The parallel performance of the implementation is demonstrated for Stokes systems with trillions ($&gt;10^{12}$) of unknowns via the notion of textbook multigrid efficiency, and for advection dominated transport problems that are simulated with more than ten billion particles  ($&gt;10^{10}$). Both approaches are run on up to 147,456 parallel processes, and coupled via a predictor-corrector scheme to simulate buoyancy-driven convection in Stokes flow.
</EventDescription>
    <EventParentName>73240-SESS</EventParentName>
    <EventUniqueID>73240-118814</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Beyond 2D Parallelization of Multi-Dimensional FFTs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS83</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  3:05:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:25:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Doru Thom Popovici</EventSpeakers>
    <EventSpeakerIDs>798421</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117691</EventHandoutURL>
    <EventDescription>Any multi-dimensional Fourier transform is typically decomposed as multiple 1D transforms applied in the different dimensions of the data set. Hence, most Fourier packages that implement parallel multi-dimensional Fourier transforms, focus on the parallelism across the 1D transforms. However, most of the existing frameworks do not offer flexible solutions to exploit the parallelism within the 1D FFT and also do not allow for changes of the data layout to exploit the parallelism. In the era of exascale, with systems with thousand of nodes and intricate network topologies, flexibility and parallel efficiency are key aspects all multi-dimensional DFT frameworks need to have in order to map and scale the computation appropriately. In this work, we present a framework that enables the development of a family of parallel FFT algorithms by 1) using different data layouts to distribute the data across the network, 2) exploiting both parallelism schemes, and 3) unifying the two parallelization schemes within a single framework. We show that the flexibility of choosing between different parallel multi-dimensional DFT algorithms allows for almost linear strong scaling results for problem sizes of 1024^3 on two supercomputers, namely the RIKEN's K-Computer and Oakridge's Summit.</EventDescription>
    <EventParentName>72986-SESS</EventParentName>
    <EventUniqueID>72986-117691</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Hierarchical Mesh Based Coupling of Solution Methods for the Simulation of Multiphase Flows</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS84</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:40:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daniel Lauwers</EventSpeakers>
    <EventSpeakerIDs>803806</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118652</EventHandoutURL>
    <EventDescription>The simulation of multiphase flows based on Eulerian-Eulerian or Eulerian-Lagrangian methods on high performance computers requires an efficient coupling and a simultaneous execution of the numerical solvers for the fluid and particles or gas phase.
Especially, communication overhead and load imbalances can lead to a low computational efficiency.
In this presentation an efficient coupling strategy based on hierarchical Cartesian meshes is presented that enables large-scale multiphase simulations.

A method that has previously been successfully used for a multiphysics application, i.e., a computational fluid dynamics method coupled with a computational aeroacoustics method, is extended to the modeling of gas-liquid bubbly flows.
The accurate description of such multiphase flows is essential for process engineering or the machinery industry.
Simulations of a bubbly turbulent channel flow, which mimics the transport phenomena in an electrochemical machining (ECM) process, demonstrate the applicability of the method.</EventDescription>
    <EventParentName>73339-SESS</EventParentName>
    <EventUniqueID>73339-118652</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Modeling the Solar Corona: Magnetohydrodynamics and Beyond</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS87</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Gregory Szypko</EventSpeakers>
    <EventSpeakerIDs>803632</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118427</EventHandoutURL>
    <EventDescription>Understanding the Sun's corona, its outer layer consisting of high-temperature, low-density magnetized plasma, is crucial to understanding the Sun's complex interactions with the rest of the Solar System. Magnetohydrodynamics (MHD), where the plasma is treated as a conducting fluid, is a common framework for modeling behavior in the corona. However, additional mechanisms of note in coronal plasmas, such as electron-mediated thermal conduction and magnetic reconnection, often necessitate treatment of temporal and spatial scales outside of the strict MHD regime. This necessity of multi-scale treatment poses a challenge from a computational standpoint. In this presentation, we discuss our current efforts at numerically modeling the solar atmosphere in an MHD framework, as well as the potential for integrating kinetic-scale physics to better capture the behavior of the corona.</EventDescription>
    <EventParentName>73284-SESS</EventParentName>
    <EventUniqueID>73284-118427</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Conservation and Monotonicity in Plasma Kinetic Simulations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS87</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  3:05:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:25:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Daniel Finn</EventSpeakers>
    <EventSpeakerIDs>796612</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118482</EventHandoutURL>
    <EventDescription>The Landau collision operator provides an accurate model for small-angle dominated Coulomb collisions in hot plasmas.  Here we review a new discrete gradient dependent, marker-particle scheme for simulating the standard Landau operator.  The inclusion of discrete gradients, a first integral preserving integrator historically used for Hamiltonian systems, ensures conservation of mass, momentum, and kinetic energy, as well as preservation of the positive semi-definite production of entropy in long-time simulations.  This new scheme was tested on standard test problems, such as Landau Damping and Two-Stream Instability, using particle-in-cell (PIC) simulations written using the PETSc library.
</EventDescription>
    <EventParentName>73284-SESS</EventParentName>
    <EventUniqueID>73284-118482</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Communication-Centric Approach for Designing Flexible Accelerators</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS88</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>801</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tushar Krishna</EventSpeakers>
    <EventSpeakerIDs>803742</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118786</EventHandoutURL>
    <EventDescription>Deep Neural Networks (DNN) have demonstrated highly promising results across computer vision and speech recognition, and are becoming foundational for ubiquitous AI. The computational complexity of these algorithms and a need for high energy-efficiency has led to a surge in research on hardware accelerators. To reduce the latency and energy costs of accessing DRAM, most DNN accelerators are spatial in nature, with hundreds of processing elements 
operating in parallel and communicating with each other directly.

DNNs are evolving at a rapid rate - leading to myriad layer/operator types of varying shapes. Given a DNN there can be myriad computationally efficient implementations (e.g., via pruning) - leading to structured and unstructured sparsity. Finally, a given DNN can be tiled and partitioned in myriad ways to exploit data reuse. All of the above can lead to irregular dataflow patterns within the accelerator substrate. Getting high mapping efficiency for all these cases is highly challenging in accelerators today that are often tightly coupled 2D grids with rigid near-neighbor connectivity.

In this talk, we will demonstrate a systematic methodology for understanding data reuse opportunities within the algorithm, determine the cost vs benefit for efficiently exploiting them in hardware and a communication-centric methodology for accelerator design, that can provide ~100% mapping efficiency for arbitrary DNNs shapes, sparsity ratios and mappings.

</EventDescription>
    <EventParentName>73327-SESS</EventParentName>
    <EventUniqueID>73327-118786</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Distributed Memory Tensor Completion For Generalized Loss Functions</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP0</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  6:00:00 PM</EventStartTime>
    <EventEndTime>02/24/22  8:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Navjot Singh</EventSpeakers>
    <EventSpeakerIDs>790176</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117624</EventHandoutURL>
    <EventDescription>Tensor completion is a useful tool applied in many fields such as data science, machine learning, signal processing on large scale data. Recent works have shown that using a specific loss function based on insights about the data distribution of the tensor may lead to a better model for completion. We provide novel algorithms and systems infrastructure which enable efficient parallel implementation of algorithms for tensor completion with generalized loss functions. Specifically, we consider alternating minimization, coordinate minimization, and a quasi-Newton (generalized Gauss-Newton) method. We compare these methods to stochastic gradient descent method and show that using second order information leads to more accurate factors in relatively less time in a parallel setting. To make possible tensor completion for very sparse tensors, we introduce new multi-tensor primitives, for which we provide specialized parallel implementations and compare these to the state of the art. We provide microbenchmarking results on the Stampede2 supercomputer to demonstrate the efficiency of the new primitives.
	
</EventDescription>
    <EventParentName>73765-SESS</EventParentName>
    <EventUniqueID>73765-117624</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Extreme-Scale Grid Optimization with the Exascale Grid Optimization (ExaGO) Toolkit</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS42</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  3:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1200</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Shrirang Abhyankar</EventSpeakers>
    <EventSpeakerIDs>795067</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117440</EventHandoutURL>
    <EventDescription>In this talk, we introduce Exascale Grid Optimization (ExaGO), an open-source package for large-scale power grid optimization on parallel, distributed, and heterogeneous GPU architectures. We will discuss ExaGO's architecture, formulation, and features along with presenting its performance on large-scale grid optimization problems.</EventDescription>
    <EventParentName>73019-SESS</EventParentName>
    <EventUniqueID>73019-117440</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Take-Away Impartial Combinatorial Game on Different Geometric and Discrete Structures</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP0</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  6:00:00 PM</EventStartTime>
    <EventEndTime>02/24/22  8:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>T. H. Molena</EventSpeakers>
    <EventSpeakerIDs>802691</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118005</EventHandoutURL>
    <EventDescription>Following from the winning strategy for a Take-Away Impartial Combinatorial Game on only Oddly Uniform or only Evenly Uniform Hypergraphs in the Ph.D. Dissertation of Dr. Kristen Barnard (an Assistant Professor of Mathematics at Berea College), T. H. Molena found the new winning strategy for a Take Away Game on neither Oddly nor Evenly Uniform Hypergraphs during her Undergraduate Independent Research opportunity. However, those neither Oddly nor Evenly Uniform Hypergraphs must meet the given special requirements. In a Take-Away Game on hypergraphs, two players take turns to remove the vertices and the hyperedges of a hypergraph. In each turn, a player must remove either only one vertex or only one hyperedge. When a player chooses to remove one vertex, all of the hyperedges that contain the chosen vertex are also removed. When a player chooses to remove one hyperedge, only that one chosen hyperedge is removed. Whoever removes the last vertex wins the game. All of the new theorems in this research paper are in agreement with the previous theorems in Dr. Kristen Barnard’s Ph.D. Dissertation.</EventDescription>
    <EventParentName>73765-SESS</EventParentName>
    <EventUniqueID>73765-118005</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Parallel Finite Element Solver for Skeletal Muscle Deformation</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP0</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  6:00:00 PM</EventStartTime>
    <EventEndTime>02/24/22  8:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Javier Almonacid</EventSpeakers>
    <EventSpeakerIDs>803727</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118714</EventHandoutURL>
    <EventDescription>Recent advances in understanding how skeletal muscle deforms have led to a new, more informative multiphysics model that views muscles as composite biomaterials, drawing a connection to the deformation of nonlinear solids and active materials. This model yields a three-dimensional, highly nonlinear system of partial differential equations (PDEs). On the other hand, the presence of important physiological entities across multiple length scales and highly heterogeneous material coefficients require finite element meshes to be particularly refined. Therefore, traditional computers may not be able to store all this information, or it may take too long to solve the resulting linear systems of equations. In this work, a parallel solver for skeletal muscle deformation is developed. The code is designed to run primarily on a distributed architecture and contains Message Passing Interface (MPI) capabilities. In particular, the decomposition of the mesh is achieved through the p4est library, while the linear algebra components are handled using the Trilinos library. The core of the code has been developed using deal.II, a C++ library for the finite element solution of PDEs. Scaling analyses show that this distributed implementation represents a step above an existing shared-memory implementation in which the design pattern WorkStream is used to enable multithreading.

	</EventDescription>
    <EventParentName>73765-SESS</EventParentName>
    <EventUniqueID>73765-118714</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>On Centrality Resilience of Complex Networks</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP0</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  6:00:00 PM</EventStartTime>
    <EventEndTime>02/24/22  8:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Fariba Afrin Irany</EventSpeakers>
    <EventSpeakerIDs>803845</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118741</EventHandoutURL>
    <EventDescription>
The resilience of a complex network relates to how well certain of its properties are retained under attacks. In this paper, we study centrality resilience, that is, resilience to path-based centralities: closeness and betweenness centralities. Our goal is to develop attack models that can disrupt the rank of the top path-based centrality nodes.  Attack models are useful in disrupting flow through a network with minimum disruption, such as in avoiding the spread of epidemics by vaccinating/quarantining super-spreaders. 

However, finding high centrality nodes in a large network, although polynomial rime is extremely expensive for large-scale networks. We show that there are several expander-like subgraphs that are embedded in complex networks, and influential nodes with high centrality are located within these expander graphs. We present an efficient method for identifying these regions and thereby the influential nodes, using snowball sampling and the core-periphery structure of the sampled graph. We further demonstrate that the resilience of the network is dependent on the number and size of the expander-like subgraphs. Based on these observations we develop attack models to disrupt the flow through the network. 

	</EventDescription>
    <EventParentName>73765-SESS</EventParentName>
    <EventUniqueID>73765-118741</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Interoperability of Resilience Strategies for Simplicity and Performance</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP0</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  6:00:00 PM</EventStartTime>
    <EventEndTime>02/24/22  8:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Matthew Whitlock</EventSpeakers>
    <EventSpeakerIDs>790179</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119082</EventHandoutURL>
    <EventDescription>Resilience at high scales necessitates minimizing and localizing the impact of failures and their recovery. Current techniques for resilience in high performance computing (HPC) applications use highly efficient, well tested libraries, but rely on a global tear-down and re-launch which places an unnecessary burden on the recovery process. However, these resilience strategies are simple for developers to implement and are already present in many current applications. Work on fault tolerant MPI specifications - such as User Level Fault Mitigation (ULFM) - has enabled more localized recovery, but at the cost of high development costs. We present the integration of existing, high performance recovery libraries with Fenix, a library built on top of ULFM to simplify recovery and emulate a more typical global tear-down process without paying the costs. Using Fenix to bridge the gap between local and global recovery processes lets the users step-by-step localize recovery with a series of minor changes to their application code. This integration leads to better performance in failure recovery and serves as a simplified bridge to the types of software resilience that may be needed to enable performant and energy-efficient future hardware. 

[SNL is managed and operated by NTESS under DOE NNSA contract DE-NA0003525.]

</EventDescription>
    <EventParentName>73765-SESS</EventParentName>
    <EventUniqueID>73765-119082</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>SIAG Best Paper Prize: Optimizing Communication-Avoiding Sparse Lu Factorization on Multi-Gpu Clusters</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>SP1</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  8:30:00 AM</EventStartTime>
    <EventEndTime>02/25/22  9:15:00 AM</EventEndTime>
    <EventFilter>PP22|Prize Speaker</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Piyush Sao</EventSpeakers>
    <EventSpeakerIDs>790070</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119106</EventHandoutURL>
    <EventDescription>We present a highly optimized implementation of the communication-avoiding sparse LU factorization algorithm, specifically targeting pre-exascale multi-GPU clusters such as Summit Supercomputer at Oak Ridge National Laboratory. 

Prior to this work, distributed memory sparse LU factorization used GPUs mostly as a co-processor because of the relatively smaller DRAM capacity available and limited hardware support for GPU-aware message passing on older GPUs. The current pre-exascale multi-GPU clusters have relatively higher DRAM capacity and hardware support for GPU-aware MPI that allows performing the entire sparse LU factorization on GPU. The challenge is, sparse LU factorization consists of many operations on small and irregular size operands, which makes it difficult to effectively use GPU during all phases on sparse LU factorization. 

To overcome such challenges, we (a) redesigned the data structure to reduce the cost of index-algebra on GPUs; b) combined streams with the so-called tree parallelism to schedule multiple operations, and; c) exploit high bandwidth GPU-2-GPU and GPU-aware MPI with the \emph{look-ahead} factorization techniques to effectively overlap communication with computation. 

Our proposed optimizations improve the performance of communication-avoiding sparse LU factorization by up to $3\times$ over offload based GPU acceleration of the same algorithm on single and multiple node configurations on the Summit supercomputer.
</EventDescription>
    <EventParentName>73820-SESS</EventParentName>
    <EventUniqueID>73820-119106</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T08:30:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Smart Munition Lethality and Effectiveness M\&amp;S</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP1</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:20:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Antonio Aguirre</EventSpeakers>
    <EventSpeakerIDs>802600</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116794</EventHandoutURL>
    <EventDescription>Smart munitions are increasingly important and integral to our nation's offensive capabilities. Such munitions are of a regime of weapons possessing enabling technologies, which enhance system performance, with respect to most currently fielded weapons, by limiting system performance sensitivities to deficiencies in guidance, navigation, target location and dynamics intelligence, and counter-measures. The DOD—Army, at Picatinny Arsenal, supports the identification of smart munition strengths and weaknesses, to guide government and contractor efforts when designing, improving, and fielding smart munitions and/or assessing systems against the various alternatives. Smart munition M\&amp;S typically involves large trade spaces that lead to the need for millions of simulations, using a mix of high and low-fidelity simulations, which often create a plethora of data to be wrangled, cleaned, analyzed, reduced, and visualized. A hypothetical smart munition M\&amp;S process will be shown for a guided-seeker-munition having a fragmenting warhead. High-fidelity simulations are used to characterize lethal performance against a target(s) of interest at various terminal conditions and angles of approach. The lethal performance simulations are then summarized for injection into lower level simulation tools for assessing mission effectiveness across an array of expected real-world scenarios, e.g. a volley of six smart munitions fired at a complex heterogeneous target array tens of kilometers away.

</EventDescription>
    <EventParentName>73758-SESS</EventParentName>
    <EventUniqueID>73758-116794</EventUniqueID>
    <STATUS>inactive</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Parallel Domain Decomposition Techniques Applied to Multivariate Functional Approximation of Discrete Data</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP2</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Vijay Mahadevan</EventSpeakers>
    <EventSpeakerIDs>758470</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119084</EventHandoutURL>
    <EventDescription>Compactly expressing large-scale datasets through Multivariate Functional Approximations (MFA) can be critically important for analysis and visualization to drive scientific discovery. We present scalable domain partitioning approaches to compute MFA representations, by reducing the total work per task through domain-decomposition strategies utilizing Schwarz-type iterative schemes for converging the interface data. For the underlying MFA, we utilize a tensor expansion of non-uniform B-spline (NURBS) basis in multiple dimensions to reduce the functional approximation error in the input data. Previous work on adaptive NURBS-based MFA rely primarily on post-processing techniques to blend discontinuities across subdomains boundaries after computing the MFA. In contrast, we explore a robust constrained parallel solution infrastructure to impose higher-order continuity directly on the MFA representation computed. We demonstrate effectiveness of the approach with an overlapping Restricted-type Additive Schwarz method (RASM), with a l-BFGS subdomain solver to minimize the local error residuals, and more specifically to recover continuity across non-matching boundaries. The analysis of the presented scheme for analytical and scientific datasets in 1-D and 2-D are also presented. Additionally, scalability studies are also shown for several analytical and realistic, scientific 1d and 2d datasets to evaluate the parallel speedup of the algorithm on large computing clusters.</EventDescription>
    <EventParentName>73759-SESS</EventParentName>
    <EventUniqueID>73759-119084</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Parrsb: Spectral Element Mesh Partitioning at Exascale</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP5</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 12:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22 12:30:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Thilina Ratnayaka</EventSpeakers>
    <EventSpeakerIDs>804316</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119086</EventHandoutURL>
    <EventDescription>Most of scientific simulations start with reading a mesh which is a representation
of the computational domain using smaller units called elements. Next step is
distributing these elements among the processors. This element distribution, is 
known as partitioning and critical for overall efficiency of the simulation. Large
simulations over 100 million of elements are already running on pre-exascale
systems like Summit at OLCF which is a sign that we will see meshes with about
billions of elements on exascale computers that are planned for future.

In this talk, we introduce “parRSB”, a partitioner designed to run on exascale
computers. parRSB is a parallel mesh partitioner which uses Recursive Spectral
Bisection (RSB) method to partition the mesh. RSB makes the partitioning decision
based on a specific Eigenvector of the Laplacian matrix associated with dual graph
of the mesh and is known to produce better partitions than methods like Recursive
Coordinate Bisection. 

We talk about how we efficiently evaluate the action of Laplacian in a parallel setting
and the preconditioners and other pre-partition optimizations we did to speed up the 
partitioning process. We also talk about some of the challenges we faced with
disconnected components and parallel sorting at scale. We present performance and
partition quality results we got with parRSB running at Scale on Summit supercomputer
with meshes as large as 100 million elements.
</EventDescription>
    <EventParentName>73826-SESS</EventParentName>
    <EventUniqueID>73826-119086</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Asynchronous Global-Local Non-Invasive Coupling</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP6</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  3:40:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ahmed El Kerim</EventSpeakers>
    <EventSpeakerIDs>803465</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118653</EventHandoutURL>
    <EventDescription>Our aim is to derive an asynchronous version of the non intrusive Global-Local coupling. This method consists in using a coarse description of a structure and enriching the modeling of some zones of interest with enhanced geometry, refined mesh, complex material laws. Mathematically, the method is a stationary iteration which proved to be particularly robust to solve nonlinear problems. An asynchronous version would bring several advantages, the method would be more resilient and less sensitive to load balance, it would make better use of the computational resources whether high performance cluster or network of workstations. Paracontraction technics have been used to prove the convergence of the asynchronous model. From an implementation point of view based, our approach makes use of MPI one-sided communication, each CPU can directly access the memory of the other CPUs to get or to put pieces of information. Using a python code driving the getfem finite element software and the mpi4py library. Comparison with a synchronous version of the code will be provided. The resulats are carried out with Ruche supercomputer commonly used by École CentraleSupelec and ENS Paris Saclay. A weak scalability study will be performed on an academic configuration, and a turbine blade from the high-pressure compressor of a turbomachine will be studied where patches are used to introduce micro-perforations in the geometry of the leading and trailing edges. 
	</EventDescription>
    <EventParentName>73761-SESS</EventParentName>
    <EventUniqueID>73761-118653</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Automating Code Generation for Point Evaluations with Finite Elements</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP8</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>David Ham</EventSpeakers>
    <EventSpeakerIDs>732280</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119083</EventHandoutURL>
    <EventDescription>Point data turn up all the time in scientific computing problems - perhaps you want to know the value of a field at a particular point or set of points, couple the inputs and outputs of models via a grid of points, or you have point data measurements that you want to assimilate into a model of some phenomenon. Firedrake is a code generation system which enables straightforward and scalable model development for systems of partial differential equations using the finite element method (FEM). In Firedrake, mathematical expressions of the problem to solve are written in Domain Specific Language (DSL) abstractions from which optimised C code is generated. Hitherto it has been difficult to generate code from expressions involving point data: either one would extrapolate the point data to match some field defined over a larger domain or be forced to write bespoke code.

We present additions to Firedrake which allow code generation from expressions which include point data. Our framework is compatible with the automated adjoint system dolfin-adjoint/pyadjoint and we demonstrate automated code generation for inverse problems which include point data via a variational data assimilation example. This work is part of a broader project aiming to use DSLs and code generation to automate diagnostics of geoscientific models with very large output data sets.</EventDescription>
    <EventParentName>73762-SESS</EventParentName>
    <EventUniqueID>73762-119083</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Improving the Performance of Reverse Time Migration Based on the First-Order Formulation of the Wave Equation using MWD-TB</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP10</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Long Qu</EventSpeakers>
    <EventSpeakerIDs>747448</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118490</EventHandoutURL>
    <EventDescription>Widely used in the oil and gas exploration industry, Reverse Time Migration (RTM) is a powerful technique for imaging complex geologic structures. It is conventionally based on directly solving the wave equation in the time domain using seismic datasets recorded at various locations. The computation of RTM concentrates on the evaluation of finite-difference stencils. Spatial blocking is a standard method for accelerating stencil computation that reduces memory bandwidth pressure by enabling data reuse in spatial directions. On the other hand, Multicore Wavefront Diamond (MWD) is a new temporal blocking technique that enables further data reuse among multiple successive time steps. Previous work shows that MWD tiling can be integrated into RTM based on the second-order acoustic wave equation. This integration has a snowball effect on the absorbing boundary conditions, the I/O operations, and the imaging condition in order to maintain the performance superiority of temporal blocking and the correctness of the migrated image. In this talk, we present its integration into the RTM using the first-order formulation and discuss its performance on various contemporary architectures.

	
</EventDescription>
    <EventParentName>73763-SESS</EventParentName>
    <EventUniqueID>73763-118490</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Parallel High-Resolution Compact Partial FFT-Type Direct Algorithms for Subsurface Scattering Problems</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP10</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ron Gonzales</EventSpeakers>
    <EventSpeakerIDs>790102</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118553</EventHandoutURL>
    <EventDescription>This research presents a direct parallel partial FFT-type algorithm for the numerical solutions of the two and three-dimensional Helmholtz equations. The governing equations are discretized by high-order compact finite-difference or finite-element methods. The resulting discretized system is indefinite, making the convergence of the most iterative methods deteriorate with the increase of the frequency. In this situation, the parallel direct approaches become a better alternative, especially for the systems with discontinuous and singular right-hand sides.
The focus is the efficient parallel implementation of the proposed algorithm in the shared (OpenMP) and distributed (MPI) memory environment. The complexity and scalability of the direct parallel method are investigated on scattering problems with realistic ranges of parameters in soil and mine-like targets. The results are also compared with the Krylov-type preconditioned methods developed in our previous publications.</EventDescription>
    <EventParentName>73763-SESS</EventParentName>
    <EventUniqueID>73763-118553</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Parallel FFT-Krylov Subspace Method for Helmholtz Equation</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP10</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yun Teck Lee</EventSpeakers>
    <EventSpeakerIDs>781837</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118677</EventHandoutURL>
    <EventDescription>In this talk, we will present an efficient parallel implementation of the iterative compact high-order approximation numerical solver for 3D Helmholtz equation on multicore computers. The developed algorithms will have many applications such as imaging mine-like targets using ground penetrating radar. Our high-order parallel iterative algorithm is built upon a combination of Krylov subspace-type method with a direct parallel Fast Fourier transform (FFT) type preconditioner from our previous work. The resulting numerical method enables a natural and fast implementation on multicore computers. The numerical results and scalability of our implementation in the MPI programming environment will be presented.
</EventDescription>
    <EventParentName>73763-SESS</EventParentName>
    <EventUniqueID>73763-118677</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Signal Processing for a Reverse-Gps Wildlife Tracking System: CPU and GPU Implementation Experiences</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP11</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:25:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:45:00 AM</EventEndTime>
    <EventFilter>PP22|Contributed</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sivan Toledo</EventSpeakers>
    <EventSpeakerIDs>704793</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118987</EventHandoutURL>
    <EventDescription>We present high-performance implementations of signal-processing tasks performed by a wildlife tracking system called ATLAS. The system tracks radio transmitters attached to wild animals by estimating the time of arrival of radio packets to multiple receivers (base stations). Time-of-arrival estimation of wideband radio signals is computationally expensive, especially in acquisition mode. We developed a sequential high-performance CPU implementation of the computations a few years back, and more recently a parallel GPU implementation. Both strive to balance performance with simplicity, maintainability, and development effort, as most real-world codes do. The talk will describe the two implementations and carefully evaluate their performance. The GPU implementation dramatically improves performance and power-performance relative to the sequential CPU implementation running on a desktop CPU typical of the computers in current base stations. Performance improves by more than 50X on a high-end GPU and more than 4X with a GPU platform that consumes almost 5 times less power than the CPU platform. Performance-per-Watt ratios also improve (by more than 16X), and so do the price-performance ratios.

The results have already been published in a journal (Concurrency and Computation Practice and Experience (2021) but the presentation is highly appropriate for SIAM PP 2022.</EventDescription>
    <EventParentName>73764-SESS</EventParentName>
    <EventUniqueID>73764-118987</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Life after Fugaku---What Have we Learned, and How do we Proceed with End of Moore's Law Approaching?</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP2</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  8:30:00 AM</EventStartTime>
    <EventEndTime>02/24/22  9:10:00 AM</EventEndTime>
    <EventFilter>PP22|Invited Speaker</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Satoshi Matsuoka</EventSpeakers>
    <EventSpeakerIDs>735590</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119079</EventHandoutURL>
    <EventDescription>Circa 2021, Fugaku is the first of the many of the so-called ‘exascale’ machines that had been under planning across the globe since the end of the 2000s, and have not only exhibited supreme performance and scalability in traditional benchmarks e.g., Linpack, driving nearly 8 million high-performance Arm cores, but also, have demonstrated such in other benchmarks as new breed of applications such as those involving big data and AI. However, the next step after exascale still remains unclear, despite that there will be several by the mid 2020s, and R&amp;D for the next generation should already have started. This is due to the end of Moore’s law approaching, and traditional method of increasing the number of cores would cease to be effective, due to power and cost limitations. At R-CCS, in collaboration with other partners we are investigating innovative means of increasing performance beyond Fugaku, based on the data obtained from Fugaku as well as other exascale machines, and undertaking a series of projects to push the supercomputing technology forward towards the 2030s.
	
</EventDescription>
    <EventParentName>73441-SESS</EventParentName>
    <EventUniqueID>73441-119079</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T08:30:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Graph Neural Network Research at AWS AI</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP7</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  9:25:00 AM</EventStartTime>
    <EventEndTime>02/26/22 10:05:00 AM</EventEndTime>
    <EventFilter>PP22|Invited Speaker</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>George Karypis</EventSpeakers>
    <EventSpeakerIDs>91119</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119078</EventHandoutURL>
    <EventDescription>In the course of just a few years, Graph Neural Networks (GNNs) have emerged as the prominent supervised learning approach that brings the power of deep representation learning to graph and relational data. An ever-growing body of research has shown that GNNs achieve state-of-the-art performance for problems such as link prediction, fraud detection, target-ligand binding activity prediction, knowledge-graph completion, and product recommendations. As a result, GNNs are quickly moving from the realm of academic research involving small graphs to powering commercial applications and very large graphs. This talk will provide an overview of some of the research that AWS AI has been doing to facilitate this transition, which includes developing the Deep Graph Library (DGL)—an open source framework for writing and training GNN-based models, improving the computational efficiency and scaling of GNN model training for extremely large graphs, developing novel GNN-based solutions for different applications, and  making it easy for developers to train and use GNN models by integrating graph-based ML techniques in graph databases.

</EventDescription>
    <EventParentName>73438-SESS</EventParentName>
    <EventUniqueID>73438-119078</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T09:25:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Independent Parallel Particle Layer (IPPL) Implementation and Observations</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS1</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Andreas Adelmann</EventSpeakers>
    <EventSpeakerIDs>712759</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118949</EventHandoutURL>
    <EventDescription>IPPL is a generic library written in C/C++ and MPI for high performance Eulerian, Lagrangian and mixed Eulerian-Lagrangian simulations. Since its development around the turn of the millennium, the variety of hardware architectures has grown and software developers are challenged in the design and maintenance of their codes. 
In this talk, I discuss our efforts to rewrite IPPL in the latest C++ standard and to enhance the library with the performance portable capabilities of Kokkos. I will also show first benchmarks performed on CPUs and GPUs.

</EventDescription>
    <EventParentName>73409-SESS</EventParentName>
    <EventUniqueID>73409-118949</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Trust Tools: Experiences in Reproducibility in Undergraduate Research</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS5</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Patrick Hesse</EventSpeakers>
    <EventSpeakerIDs>803128</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118328</EventHandoutURL>
    <EventDescription>Computational results must be trustworthy to hold merit.  As hardware and software evolve, results that were considered reproducible can no longer be obtained.  With some extra effort, this problem can be avoided. After integrating reproducibility tools into a variety of computational workflows, our experience shows that these tools move the burden of executing common ad-hoc tasks from the human to the machine, therefore minimizing human error that could degrade the trustworthiness of computational results.

	</EventDescription>
    <EventParentName>73126-SESS</EventParentName>
    <EventUniqueID>73126-118328</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Reliable Performance Testing for Multi-Tenant Clouds</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS7</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Wei Wang</EventSpeakers>
    <EventSpeakerIDs>802901</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117224</EventHandoutURL>
    <EventDescription>To effectively utilize cloud computing, cloud resource management requires accurate knowledge of the performance of cloud applications. However, due to the random performance fluctuations, obtaining accurate performance results in the cloud is extremely difficult. To handle this random fluctuation, prior research on cloud performance testing usually relied on advanced statistical tools.

However, the effectiveness of these advanced statistical tools depends heavily on the statistical characteristics of cloud performance testing data. In this talk, we present two performance testing methodologies. The first methodology is designed to obtain the performance distribution of a cloud application. It relies on statistical stability and multinomial likelihood theory to handle the non-typical performance distributions of cloud applications. The second methodology is designed to obtain single-point estimates of performance and relies on block bootstrapping to handle the internal dependency of cloud performance data. Evaluation results show that both methodologies can provide testing results with more than 90% accuracy on average.
</EventDescription>
    <EventParentName>72962-SESS</EventParentName>
    <EventUniqueID>72962-117224</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Experience-Driven Performance Tuning of GPU Kernels for Dense Linear Algebra</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS8</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ahmad Abdelfattah</EventSpeakers>
    <EventSpeakerIDs>773150</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118607</EventHandoutURL>
    <EventDescription>High performance GPU kernels are often highly parameterized in order to achieve performance portability across different architectures. However, this comes at the cost of a parameter space that is often too large to search entirely for every possible use-case of the considered kernel. We present our work on the MAGMA library, where instead of pruning the search space, we perform exhaustive searches on specific use-cases that are known to be very important in the originating applications. As an example, rank-k updates in matrix multiplication (GEMM) are crucial for the performance of dense matrix factorizations, and so are considered during the tuning process of the GEMM kernel. Examples for other kernels will be presented as well.
</EventDescription>
    <EventParentName>73328-SESS</EventParentName>
    <EventUniqueID>73328-118607</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Adaptive Nonlinear Domain Decomposition Methods</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS9</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Martin Lanser</EventSpeakers>
    <EventSpeakerIDs>774576</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118656</EventHandoutURL>
    <EventDescription>In this talk, different nonlinear domain decomposition methods are applied to nonlinear problems with highly-heterogeneous coefficient functions with jumps. In order to obtain a robust nonlinear domain decomposition method with respect to nonlinear as well as linear convergence, adaptive coarse spaces are employed. First, as an example for a nonlinearly left-preconditioned domain decomposition method, the two-level restricted nonlinear Schwarz method H1-RASPEN (Hybrid Restricted Additive Schwarz Preconditioned Exact Newton) is combined with an adaptive generalized Dryja-Smith-Widlund (GDSW) coarse space.
Second, as an example for a nonlinearly right-preconditioned domain decomposition method, a nonlinear FETI-DP (Finite Element Tearing and Interconnecting - Dual Primal) method is equipped with an edge-based adaptive coarse space. Both approaches are compared with the nonlinear domain decomposition methods with classical coarse spaces as well as with the Newton-Krylov methods with adaptive coarse spaces. For two-dimensional problems with different spatial coefficient distributions,
it can be observed that the best linear and nonlinear convergence can only be obtained when combining the nonlinear domain decomposition methods with adaptive coarse spaces.

</EventDescription>
    <EventParentName>73346-SESS</EventParentName>
    <EventUniqueID>73346-118656</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Landau Collision Operator in the Cuda Programming Model Applied to Thermal Quench Plasmas</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS11</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  1:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mark Adams</EventSpeakers>
    <EventSpeakerIDs>735429</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118707</EventHandoutURL>
    <EventDescription>The Landau form of the Fokker-Planck equation is the gold standard for plasmas dominated by small angle collisions, but its O(N^2) work complexity has limited its use.
This talk extends previous work on a conservative finite element discretization of the Landau operator with new implementations in the CUDA programming model, in both CUDA and Kokkos.
Experiments on NVIDIA and AMD GPUs, and preliminary results on a Fujitsu vector processor, are presented.
This talk presents new optimizations of the Landau kernel and implementations of the sparse matrix assembly and algebraic solver on the GPU, and shows that a well resolved Landau collision time advance is practical for kinetic plasma applications.
We apply this solver to a reduced Vlasov-Maxwell-Landau model of a plasma thermal quench for runaway electron studies.
Both the fully implicit Landau time integrator and the Vlasov-Maxwell-Landau model are available in the PETSc (Portable, Extensible, Toolkit for Scientific computing) numerical library.</EventDescription>
    <EventParentName>73362-SESS</EventParentName>
    <EventUniqueID>73362-118707</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>openPMD - Open and F.A.I.R. I/O for Particle-Mesh Data at the Exascale</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS12</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Franz Poeschel</EventSpeakers>
    <EventSpeakerIDs>790274</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118953</EventHandoutURL>
    <EventDescription>This talk presents openPMD, and open and F.A.I.R. standard for particle-mesh data, and its impact in Exascale scientific workflows. The openPMD standard is made accessible to scientific software via the openPMD-api, a library for the description of scientific data. It approaches recent challenges posed by hardware heterogeneity by decoupling the data description in domain sciences, such as plasma physics simulations, from concrete implementations in hardware and IO. This concept helps us build a transition path from file-based IO to streaming-based workflows of scientific applications in an HPC environment. The streaming backend is provided by the ADIOS2 framework, developed at Oak Ridge National Laboratory.
This talk discusses two openPMD-based loosely coupled setups to demonstrate flexible applicability and to evaluate performance. In loose coupling, as opposed to tight coupling, two (or more) applications are executed separately, e.g. in individual MPI contexts, yet cooperate by exchanging data. This way, a streaming-based workflow allows for standalone codes instead of tightly-coupled plugins, using a unified streaming-aware API and leveraging high-speed communication infrastructure available in modern compute clusters for massive data exchange.
The presented setups show the potential for a more flexible use of compute resources brought by streaming IO as well as the ability to increase throughput by avoiding filesystem bottlenecks.
</EventDescription>
    <EventParentName>73410-SESS</EventParentName>
    <EventUniqueID>73410-118953</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Exascale Surrogate Modeling of Plasma Accelerators</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS12</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>N. Hoffmann</EventSpeakers>
    <EventSpeakerIDs>804166</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118954</EventHandoutURL>
    <EventDescription xsi:nil="true" />
    <EventParentName>73410-SESS</EventParentName>
    <EventUniqueID>73410-118954</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Sparse Neighborhood Collectives on Heterogeneous Architectures</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS13</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Amanda Bienz</EventSpeakers>
    <EventSpeakerIDs>803820</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118711</EventHandoutURL>
    <EventDescription>Sparse solvers and simulations are often bottlenecked by inter-process boundary exchanges.  Typically, these boundary exchanges are implemented with point-to-point communication, leaving optimizations up to the application programmer.  Sparse neighborhood collectives, such as the MPI_Neighbor_alltoallv, can be utilized to add necessary optimizations within MPI.  Optimization challenges arise on heterogeneous architectures, with large numbers of paths between sets of GPUs.  During this talk, I discuss optimizations for sparse neighborhood collectives on heterogeneous architectures, and how these optimizations can affect applications.</EventDescription>
    <EventParentName>73363-SESS</EventParentName>
    <EventUniqueID>73363-118711</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Composable Solvers in the Ginkgo Library</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS13</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tobias Ribizel</EventSpeakers>
    <EventSpeakerIDs>792288</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118712</EventHandoutURL>
    <EventDescription>The Ginkgo Numerical Linear Algebra Library provides a modern C++ interface and kernels for high performance sparse matrix-vector products, iterative solvers and preconditioners for modern GPUs.

Inside Ginkgo, all of these operations share the same abstract representation as the application of a linear operator (LinOp)

   $$y = Ax$$

with A being a system matrix, a matrix-free operator, its inverse or an approximation thereof.
Just as a simple Krylov solver is composed of a system matrix, a factorization-based preconditioner is composed of the solvers for the individual factors, and a multigrid solver is composed of smoothers, restriction/prolongation operators and a coarse-grid solver.

We will study the use of operator composition in the design of novel solvers and preconditioners for massively parallel architecture that trade in synchronization overheads, which can be very costly on these architectures, for higher overall computational effort in iterative solvers.

Finally, we show how these approaches can be extended to distributed and batched solvers as well as mixed-precision algorithms.</EventDescription>
    <EventParentName>73363-SESS</EventParentName>
    <EventUniqueID>73363-118712</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Ft-Gcr: a Fault-Tolerant Generalized Conjugate Residual Elliptic Solver</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS14</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Tommaso Benacchio</EventSpeakers>
    <EventSpeakerIDs>740836</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118743</EventHandoutURL>
    <EventDescription>With the steady advance of high performance computing systems featuring smaller and smaller hardware components, the systems and algorithms used for numerical simulations increasingly contend with disruptions caused by hardware failures and bit-levels misrepresentations of computing data. In numerical frameworks exploiting massive processing power, the solution of linear systems often represents the most computationally intensive component. Given the large amount of repeated operations involved, iterative solvers are particularly vulnerable to bit-flips. A new method named FT-GCR is proposed here that supplies the preconditioned Generalized Conjugate Residual Krylov solver with detection of, and recovery from, soft faults. The algorithm tests on the monotonic decrease of the residual norm and, upon failure, restarts the iteration within the local Krylov space. Numerical experiments on the solution of an elliptic problem arising from a stationary flow over an isolated hill on the sphere show the skill of the method in addressing bit-flips on a range of grid sizes and data loss scenarios, with best returns and detection rates obtained for larger corruption events. The simplicity of the method makes it easily extendable to other solvers and an ideal candidate for algorithmic fault tolerance within integrated model resilience strategies.</EventDescription>
    <EventParentName>73369-SESS</EventParentName>
    <EventUniqueID>73369-118743</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Application of Domain Decomposition Algorithms to Physics-Informed Neural Networks</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS16</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hyea Hyun Kim</EventSpeakers>
    <EventSpeakerIDs>741246</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118661</EventHandoutURL>
    <EventDescription>Physics-informed neural networks are new approaches for solutions of partial differential equations and present promising features for many application problems. On the other hand, there are many issues to address for the new approaches, such as accuracy of the approximate solutions and efficient training of parameters. To deal with such issues, the solution can be formed by using local neural networks and the local neural networks are found as solutions of suitable local problems. The idea is similar to that of domain decomposition algorithms. In this talk, we present some experimental study on application of domain decomposition algorithms to find solutions of physics-informed neural networks in pursuit of enhancement of accuracy and efficiency of existing methods.</EventDescription>
    <EventParentName>73347-SESS</EventParentName>
    <EventUniqueID>73347-118661</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>UFL to GPU: Generating Near-Roofline Finite Element Action Kernels</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS17</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Andreas Kloeckner</EventSpeakers>
    <EventSpeakerIDs>740185</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118614</EventHandoutURL>
    <EventDescription>Recent GPUs are capable of providing peak performance up to 6 TFlOps/s
(DP), making them an attractive computational target for Finite Element
Methods. However, effectively mapping an FEM solver to a GPU remains
challenging due to the scattered memory access, large amounts of on-chip
state space (e.g. registers) required for efficient execution, and the
inherently large algorithmic variety encountered in local assembly
kernels for variational forms.

For a general class of finite element operators expressed in UFL (the
“Unified Form Language”), in 1/2/3D and at various approximation orders,
we focus on the generation of efficient GPU code for the application of
the operator action (i.e. the matrix-vector product). We describe a
parametrized family of transformation strategies targeting these
kernels, a heuristic cost model, and an auto-tuning strategy that
enables us to achieve near-roofline performance for a wide variety of
variational forms across domains such as fluid dynamics, solid
mechanics, and wave motion.

We have integrated our algorithm into the Firedrake Project, a UFL-based
FEM solver framework for broad accessibility of our work and make a case
for software frameworks based on code-generation that progressively
lower high-level mathematical abstractions into high-performance codes.</EventDescription>
    <EventParentName>73329-SESS</EventParentName>
    <EventUniqueID>73329-118614</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Temporal Alignment: Your Performance Depends On It!</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS18</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jim Brandt</EventSpeakers>
    <EventSpeakerIDs>802658</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117229</EventHandoutURL>
    <EventDescription>To gain understanding of how HPC resources are being utilized, identify bottlenecks, stay within power bounds, and improve system design, significant effort has gone into monitoring, analysis, and visualization of system resource state. Application developers have also been instrumenting applications to understand sub-component performance, identify bottlenecks, and develop more efficient algorithms and programming methods. While the latter approach can drive innovations in algorithms and improved parallelism, it won’t address system resource availability and health issues which have been shown to cause large variation in HPC application run times. Likewise, while the former approach can be used to identify degraded resource performance and bottlenecks, it cannot provide operations staff with insight into what in the associated workflow mix is causing problems. Only by combining time aligned application performance information and system resource state information can correlations between the two be discovered, root causes identified, and problems mitigated. I present work in low-overhead capture of timestamped system resource and application performance data along with tools for run time analysis and time-aligned visualization of both to enable visual correlation of features. This work is foundational for improving application performance and system throughput through automated correlation, root cause analysis, and mitigating feedback to system resources and applications.</EventDescription>
    <EventParentName>72963-SESS</EventParentName>
    <EventUniqueID>72963-117229</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Temporal Data Mining in Dynamic Resource Management</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS18</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Abdullah Mueen</EventSpeakers>
    <EventSpeakerIDs>761858</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117230</EventHandoutURL>
    <EventDescription>Dynamic resource management in an HPC platform needs a responsive resource manager that tracks the changes in applications over the run-time, as well as over life-time (i.e. versions) of the applications. Application owners (i.e. researchers and developers) freely change their applications, hence such tracking inherently needs unsupervised methods to discover signature patterns of an application for identification, distinguishing patterns to characterize changes, and response synthesis based on the discovered patterns. We consider discovering signature patterns from resource utilization traces and develop algorithms to exploit temporal motifs to accurately categorize/cluster jobs into applications. The algorithm uniquely warps the time scale in order to allow for various delays unrelated to the applications. The algorithm exploits multiple resource types (i.e. memory, processor, network) at a higher sampling rate (i.e. 1Hz) to capture a wide variety of signature patterns. On a test dataset collected at a Sandia National Laboratory, the algorithm achieved 90% accuracy in grouping the jobs in applications with common user and working directory. We demonstrate use cases of additional temporal data mining algorithms in other domains and envision that these algorithms can be transformative in dynamic resource management.</EventDescription>
    <EventParentName>72963-SESS</EventParentName>
    <EventUniqueID>72963-117230</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Autotuning of Execution Parameters and Autonomic Response: the Performance Analysis Tool Perspective</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS18</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kevin Huck</EventSpeakers>
    <EventSpeakerIDs>716127</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117232</EventHandoutURL>
    <EventDescription>Software users and application developers would like to have faith that all parametric choices in software, both “hard wired” and “user configurable”, have optimal defaults.  In a perfect world, the defaults would be analytically tested values that are guaranteed to provide optimal performance in response to all reasonable inputs on all supported platforms.  Sadly, the world is not perfect.  While the default settings are usually reasonable, and may even be set after some initial testing and heuristic guidance, these “magic number” settings rarely provide optimal performance for all inputs on all systems - especially on architectures that didn’t exist when the parameter choices were made.  Software autotuning has a long, rich history involving several active areas of research.  In this talk, we will discuss some research from our group into the area of both offline and online autotuning of software parameters, code transformations, and system parameters to improve performance and/or efficiency.  We will also discuss some of the challenges in the area, and requirements for reconfigurable software that can enable more aggressive optimizations for potential future input and system challenges.</EventDescription>
    <EventParentName>72963-SESS</EventParentName>
    <EventUniqueID>72963-117232</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Sparse Matrices and High-Performance Computing Meet Biology</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS19</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Giulia Guidi</EventSpeakers>
    <EventSpeakerIDs>796286</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117836</EventHandoutURL>
    <EventDescription>Recently, the benefit of high-performance computing (HPC) for science has grown rapidly, beyond traditional simulations to data analysis, for example, in genomics. Given the vast amount of data and computation involved in such applications, they can require the full computational power and memory of institutional or agency-wide HPC systems.
One of the most data- and compute-intensive challenges in genomics is de novo genome assembly, i.e., reconstructing an unknown genome from redundant, erroneous genomic sequences. Here we introduce the first distributed memory assembler for long-read sequencing data, called ELBA. ELBA introduces sparse matrices as the main abstraction in this context and makes extensive use of sparse linear algebra computation and probabilistic modeling. ELBA is up to 2$\times$ faster on CPU than an algorithm based on distributed hash tables, which are harder to parallelize. ELBA integrates GPU support in the most compute-intensive stages of the pipeline to take advantage of today's HPC heterogeneous hardware.</EventDescription>
    <EventParentName>73138-SESS</EventParentName>
    <EventUniqueID>73138-117836</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Scaling AMReX Applications using a Bittree Framework</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS20</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Akash V. Dhruv</EventSpeakers>
    <EventSpeakerIDs>803351</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117918</EventHandoutURL>
    <EventDescription>Flash-X is a multiphysics code with octree based adaptive mesh refinement (AMR), and AMReX is a block structured AMR library with no constraint on how refined regions are organized. Integration of AMReX into FLash-X has provided an opportunity to examine its performance in comparison to FLash-X’s native Octree mode with Paramesh. AMReX’s native refinement algorithm provides better refinement efficiency, while octree has better scaling behavior. We have integrated grid generation using Bittree, a compact octree bitmap representation, into AMReX as an alternative for applications where flexibility of AMReX’s native algorithm is not needed. In this talk we present the details of this integration and its impact on AMReX’s performance in Flash-X.</EventDescription>
    <EventParentName>73162-SESS</EventParentName>
    <EventUniqueID>73162-117918</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Mixed Precision Strategies for GMRES</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS26</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:45:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jennifer Loe</EventSpeakers>
    <EventSpeakerIDs>775808</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117634</EventHandoutURL>
    <EventDescription>Sparse iterative linear solvers are crucial to many applications.  In order to effectively make use of new high performance computers, these solvers must be able to exploit hardware for low precision (32 and 16 bit) computations.  We provide experimental studies for low precision GMRES wrapped in an iterative refinement loop (GMRES-IR). We discuss the convergence obtained with GMRES-IR and show how it can give up to 1.35 times speedup over traditional fp64 GMRES.  Furthermore, we provide a model for speedup of the sparse matrix-vector product (SpMV) and discuss low-precision preconditioning strategies for GMRES. Finally, we will discuss options for mixed precision GMRES in the software library Trilinos.   </EventDescription>
    <EventParentName>73076-SESS</EventParentName>
    <EventUniqueID>73076-117634</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>High Performance Computing in the Context of General Software for Hierarchical Statistical Modeling</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS27</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:45:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christopher Paciorek</EventSpeakers>
    <EventSpeakerIDs>803156</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117762</EventHandoutURL>
    <EventDescription>I'll discuss the challenges of doing high performance computing for hierarchical statistical modeling, with a focus on MCMC. I'll start by giving an overview of how different packages for general hierarchical modeling (e.g., JAGS, Stan, PyMC3, NIMBLE) have been designed in terms of achieving both flexibility and speed. I'll specifically discuss the design decisions we've made in developing the NIMBLE package, of which I am a core developer. NIMBLE provides an interface for programming models and algorithms in R from which we programmatically generate C++ via the NIMBLE compiler. I'll close by talking about the challenges involved in making effective use of parallelization in the context of general purpose hierarchical modeling software.</EventDescription>
    <EventParentName>73114-SESS</EventParentName>
    <EventUniqueID>73114-117762</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Developing GPU Based Conforming Curved Mesh Adaptation Procedures in Mfem</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS28</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:20:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:40:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Morteza Siboni</EventSpeakers>
    <EventSpeakerIDs>796058</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118608</EventHandoutURL>
    <EventDescription>Ongoing efforts on the development of conforming mesh adaptation procedures in MFEM will be summarized. MFEM is a high-order finite element library that focuses on performant and scalable execution on the latest GPU accelerated super computing systems. Thus, mesh adaptation must support curved elements of the order required for the order of finite element used. In addition, it is desirable that those procedures execute on the compute node’s GPUs. Building on Omega_h’s GPU based linear simplex mesh adaptation procedures, efforts are ongoing to extend, and add to, its cavity based mesh modification operators to support curved element geometries. To support the interactions needed to properly curve mesh faces and edges to domain boundaries, the EGADS geometry interrogation APIs, a version of which executes on GPUs, will be used. The application of these methods on the latest DOE systems, which include NVIDIA, AMD and Intel GPUs, will be demonstrated for fusion plasma physics simulations.


</EventDescription>
    <EventParentName>73163-SESS</EventParentName>
    <EventUniqueID>73163-118608</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>The p4est Software for Parallel AMR:  Algorithms and Interfaces</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS28</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 12:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22 12:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mikhail Kirilin</EventSpeakers>
    <EventSpeakerIDs>805565</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118616</EventHandoutURL>
    <EventDescription>A forest of octrees is one data structure to represent the recursive adaptive refinement of an initial, conforming coarse mesh of hexahedra. By
construction, it generates a space filling curve ordering of the inner and leaf nodes of the refinement forest that can be exploited for fast partitioning and load balancing, adhering to a strictly disjoint parallel storage of leaves. In addition, the structure allows for communication-free and efficient local neighbor finding and general remote object searches through the entire partition.

The p4est software is a well-known software library that implements a collection of algorithms for parallel AMR.  In this presentation, we will focus on (a) performance improvements by exploiting the AVX instruction set and (b) latest additions to the set of available algorithms, in particular configurably ordered traversals and non-balanced halo gathering and mesh iteration.
</EventDescription>
    <EventParentName>73163-SESS</EventParentName>
    <EventUniqueID>73163-118616</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Extending the Parallel Scalability of FROSch Solvers Beyond 100K Cores</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS30</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 12:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22 12:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Oliver Rheinbach</EventSpeakers>
    <EventSpeakerIDs>763792</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118667</EventHandoutURL>
    <EventDescription>Techniques to extend the parallel performance of the three-level Fast and Robust Overlapping Schwarz (FROSch) preconditioners are described. The FROSch framework is part of the Trilinos software library and contains a parallel implementation of different preconditioners with energy minimizing coarse spaces of GDSW (Generalized Dryja–Smith–Widlund) type.

A three-level extension for FROSch is constructed by a recursive application of the FROSch preconditioner to the coarse problem. No explicit geometric information is needed in this recursive application. In particular, for elasticity, the rigid body modes, including the rotations, can be interpolated on the coarse level without additional geometric information. 

Parallel results for a three-dimensional linear elasticity problem obtained on the Theta supercomputer (ALCF, Argonne, USA) using up to 220000 cores are discussed and compared to results obtained on the SuperMUC-NG supercomputer (LRZ, Garching, Germany).


</EventDescription>
    <EventParentName>73348-SESS</EventParentName>
    <EventUniqueID>73348-118667</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>We Needed a Fast Performance-Portable Geometric Search. Here's What We Learned Along the Way</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS31</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 12:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22 12:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Andrey Prokopenko</EventSpeakers>
    <EventSpeakerIDs>755869</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118722</EventHandoutURL>
    <EventDescription>Fast geometric search is an often overlooked but a critical part in many scientific applications. For example, computational cosmologists learn more about the Universe by examining distribution of clumped areas of dark matter, called halos. We wanted to help, so we developed a geometric search library called ArborX. To avoid doing the hard work of learning to be experts in all modern architectures (from multithreaded CPUs to novel GPUs), we used Kokkos to hide all that complexity. Instead, we focused on developing a robust implementation based on bounding volume hierarchy (BVH). This talk explores many areas we delved in during our quest to performance. We explore different approaches to BVH traversal, low level mechanisms to Kokkos kernel launching, and adapting the library design. Not all approaches work in all cases, so we still have more to learn.
</EventDescription>
    <EventParentName>73358-SESS</EventParentName>
    <EventUniqueID>73358-118722</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Revising Pseudospectral Methods for Hybrid Kohn-Sham Density Functional Theory in the Age of Exascale Computing</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS32</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:15:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>David Williams-Young</EventSpeakers>
    <EventSpeakerIDs>785504</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118685</EventHandoutURL>
    <EventDescription>In large scale hybrid Kohn-Sham density functional theory (KS-DFT) simulations, the primary computational bottleneck in the solution of the self-consistent field (SCF) eigenvalue problem is the assembly of the exact-exchange contribution to the Fock matrix. In atom-centered bases, traditional methods to perform this task involve the evaluation of an enormous number of electron repulsion integrals which introduce a number of challenges when considering modern massively parallel accelerator based architectures. These include difficulties in load balancing in distributed memory contexts and lack of SIMD concurrency for GPU architectures. In this work, we revisit a class of pseudospectral methods which are capable of overcoming the aforementioned hurdles by exploiting spatial locality of the Coulomb interaction and exhibit linear scaling with system size.</EventDescription>
    <EventParentName>73360-SESS</EventParentName>
    <EventUniqueID>73360-118685</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Improvements in the Elsi Infrastructure for Kohn-Sham Eigenvalue and Density Matrix Solutions</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS32</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 11:20:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:40:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Volker Blum</EventSpeakers>
    <EventSpeakerIDs>762885</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118686</EventHandoutURL>
    <EventDescription xsi:nil="true" />
    <EventParentName>73360-SESS</EventParentName>
    <EventUniqueID>73360-118686</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Chapel Overview and Future Opportunities and Challenges for Chapel</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS34</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:15:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>2900</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Michelle Strout</EventSpeakers>
    <EventSpeakerIDs>803828</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118718</EventHandoutURL>
    <EventDescription>
A variety of applications have taken advantage of Chapel's ease of programming and high performance, some of which will be featured in subsequent talks in this session/minisymposium.  This talk will provide an overview of some key concepts in Chapel that users have reported finding attractive such as its global view of computation and constructs that enable the specification of distributed- and shared-memory parallelism.  These concepts will be illustrated with some example programs running on laptops to supercomputers.
 
This talk will also feature future opportunities and challenges for Chapel such as a compiler rework to provide faster and interactive compilation, expanding Chapel's support for GPUs, and finalizing the language and standard library stabilization effort.



</EventDescription>
    <EventParentName>73366-SESS</EventParentName>
    <EventUniqueID>73366-118718</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Case Study on the Impact of Chapel within an Academic Computational Aerodynamic Laboratory</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS34</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 12:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22 12:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>2900</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Éric Laurendeau</EventSpeakers>
    <EventSpeakerIDs>803832</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118721</EventHandoutURL>
    <EventDescription>Aerodynamic simulations involve solving Computational Fluid Dynamics (CFD) models on supercomputers. The resulting non-linear partial differential equations, comprising up to10 billion unknowns, are solved using proprietary C/C++ or Fortran source codes. Scalability via domain decomposition is mandatory for industrial run times, typically using MPI libraries. When multi-physical phenomena are accounted, such as aero-icing, code development becomes more challenging since different packages constructed with different paradigms must interact.
Orthogonal to these challenges are organizational ones. As R&amp;D involves cascading university-based low Technology Readiness Level (TRL) advances into industrial high TRL workflows, the technical gap for graduate students with knowledge of precompiled-based tools handling few unknowns on single CPU machines is tremendous.

At Polytechnique Montreal, we used the Chapel programming language so that students can spend time addressing scientific problems minimizing technical challenges. In this talk, a case study highlights how Chapel enabled a small group of graduate students write a world-class CFD software participating in international workshops alongside similar software, albeit with MPI libraries, developed by R&amp;D professionals. The obtained results showcase the efficiency of the Chapel language not only in terms of HPC metrics, but also in terms of capability enhancement for university-based contributions to the aeronautical field.

</EventDescription>
    <EventParentName>73366-SESS</EventParentName>
    <EventUniqueID>73366-118721</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Adaptation of Explainable AI for Auto-Tuning on Accurate Matrix Multiplication Library</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS35</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 11:15:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Takahiro Katagiri</EventSpeakers>
    <EventSpeakerIDs>747426</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118619</EventHandoutURL>
    <EventDescription>Explainable AI (XAI) is one of very important topics for not only AI field but also numerical computations. The authors have been developing for Auto-tuning (AT) facility for numerical libraries in this two decades, especially to autotune parameters on numerical algorithms, selections of high-performance implementations, etc. 

In this presentation, the authors focus on XAI for AT to numerical libraries. Target numerical library is chosen for high accurate matrix multiplication library, which is developed by author's group. For more details, AI is adapted to select the best implementation between 11 implementations on the library, including CPU and GPU. LIME and SHAP are utilized for tools of XAI. To prepare training data, a GPU based-supercomputer, named the Supercomputer “Flow” Type II Subsystem in Information Technology Center, Nagoya University, is used. 

According to preliminary result, the tools explain well for behavior of learned models by random-forest regressor.


</EventDescription>
    <EventParentName>73330-SESS</EventParentName>
    <EventUniqueID>73330-118619</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Massively Parallel Eigensolvers based on Unconstrained Minimization</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS35</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 12:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22 12:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Doru Thom Popovici</EventSpeakers>
    <EventSpeakerIDs>798224</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118622</EventHandoutURL>
    <EventDescription>This presentation will show recent developments in unconstrained minimization schemes for the solution of eigenvalue problems in electronic structure calculations. These schemes employ a preconditioned conjugate gradient approach that avoids an explicit reorthogonalization of the trial eigenvectors, in contrast to typical iterative eigensolvers, therefore reducing communications and becoming an attractive approach for the solution of very large problems on massively parallel computers. We show results for a set of benchmark systems with an implementation of the unconstrained minimization in first-principles materials and chemistry codes that perform electronic structure calculations based on a density functional theory (DFT) approximation to the solution of the many-body Schr{\"o}dinger equation. The results show that the unconstrained formulation, together with an appropriate preconditioner, offers good convergence properties and scales well on a large number of cores. We also show results for an implementation in the commonly used plane wave formulation of DFT, on mixed GPU/CPU computers such as the IBM Summit computer at the leadership computing facility at Oak Ridge National Laboratory.  </EventDescription>
    <EventParentName>73330-SESS</EventParentName>
    <EventUniqueID>73330-118622</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>An Operator-Splitting Approach to Solving Cell-Based Mathematical Models of Cardiac Tissue using Modern CPU Architectures</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS36</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kristian Gregorius Hustad</EventSpeakers>
    <EventSpeakerIDs>790143</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118773</EventHandoutURL>
    <EventDescription>A number of pathologies related to the electrical activity in the heart can be studied using computer simulations of reaction-diffusion models.
With the recent extracellular-membrane-intracellular (EMI) model, the geometry of each cell is resolved in the mesh, allowing for a more accurate representation of cardiac tissue on the cell scale.
However, the EMI model requires a very fine mesh, and the linear systems arising from the diffusion process in the extracellular and the intracellular domains are ill-conditioned.

In this talk, we present an improved operator-splitting method that decouples the intracellular and extracellular domains, such that each sub-problem becomes a classical elliptic partial differential equation.
Using this operator-splitting method, the computing time scales linearly with the problem size.
This operator-splitting method enables us to solve the linear systems efficiently on shared-memory parallel computers, and we demonstrate that we are able to solve a system with $512 \times 256$ cardiac cells, solving linear systems with approximately $2.5 \cdot 10^8$ degrees of freedom.</EventDescription>
    <EventParentName>73365-SESS</EventParentName>
    <EventUniqueID>73365-118773</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Improving the Computation and the Application of Eigenspaces as Deflation for Variance Reduction in Lattice Qcd</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS37</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  3:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Andreas Stathopoulos</EventSpeakers>
    <EventSpeakerIDs>31242</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118689</EventHandoutURL>
    <EventDescription xsi:nil="true" />
    <EventParentName>73361-SESS</EventParentName>
    <EventUniqueID>73361-118689</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Solving Many-Body Localization Eigenvalue Problems on GPUs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS37</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:35:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Roel Van Beeumen</EventSpeakers>
    <EventSpeakerIDs>760440</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118692</EventHandoutURL>
    <EventDescription xsi:nil="true" />
    <EventParentName>73361-SESS</EventParentName>
    <EventUniqueID>73361-118692</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Designing Smart and Resilient Extreme-Scale Systems</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS38</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:45:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christian Engelmann</EventSpeakers>
    <EventSpeakerIDs>712668</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118749</EventHandoutURL>
    <EventDescription>Resilience is one of the critical challenges of extreme-scale high-performance computing (HPC) systems, as component counts increase, individual component reliability decreases, and software complexity increases. Building a reliable supercomputer that achieves the expected performance within a given cost budget and providing efficiency and correctness during operation in the presence of faults, errors, and failures requires a full understanding of the resilience problem. This talk provides an overview of recent achievements in developing a taxonomy, catalog and models that capture the observed and inferred fault, error, and failure conditions in current supercomputers and in extrapolating this knowledge to future-generation systems. It also describes the path forward in machine-in-the-loop operational intelligence for smart computing systems, leveraging operational data analytics in a loop control that maximizes productivity and minimizes costs through adaptive autonomous operation for resilience.</EventDescription>
    <EventParentName>73371-SESS</EventParentName>
    <EventUniqueID>73371-118749</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Cooperative Compiler and Runtime Checkpoint/Restart Approach for Kokkos</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS38</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:35:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Akihiro Hayashi</EventSpeakers>
    <EventSpeakerIDs>803860</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118751</EventHandoutURL>
    <EventDescription>Resilience is an imminent issue for next-generation platforms due to projected increases in node failures. Typically, Checkpoint/restart is used to save the temporary state of a program and to resume from where failures happened. However, it is not always trivial for users to add the Checkpoint/restart capability to their applications because there is no performance portable and out-of-box Checkpoint/restart framework for different HPC platforms.

Kokkos is designed to provide a productive yet performance portable parallel programming model. In this talk, we explore the possibility of constructing a performance portable Checkpoint/restart framework built on top of the Kokkos programming system. Essentially, our key contributions are twofold: First, we propose a clang-based source-to-source translator that analyzes the use of Kokkos's View in Kokkos's parallel constructs and automatically generates a sequence of checkpoint API. Second, we propose a cooperative runtime and its API, which enables optimizing the frequency of checkpointing, thereby avoiding too-fine-grain checkpointing. Such a synergetic approach enables Kokkos programmers to focus on writing standard Kokkos programs. We will discuss the effectiveness and applicability of our approach using different Kokkos applications such as a 3D heat distribution applications and mini-applications from the Mantevo project.</EventDescription>
    <EventParentName>73371-SESS</EventParentName>
    <EventUniqueID>73371-118751</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Adaptive Mesh Refinement Simulations of Compressible Reacting Flows in Complex Geometry</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS39</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Marc T. Henry de Frahan</EventSpeakers>
    <EventSpeakerIDs>796066</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118611</EventHandoutURL>
    <EventDescription>Compressible reacting flow systems, such as turbine combustors and internal combustion engine cylinders, exhibit complex multiscale behavior and their detailed simulation at the scale of realistic devices requires massive computing resources. Even with the availability of exascale computing hardware, such simulations at practically relevant time and length scales require advanced algorithms that can intelligently reduce the computing resources required while also retaining required accuracy in the solution. PeleC is a code for compressible reacting flows in complex geometries that features adaptive mesh refinement (AMR). AMR is used to capture and track critical time-dependent fine-scale flow features that arise from chemical reactions and their interaction with complex boundary structures. In this talk, we discuss the embedded boundary formulation used in conjunction with AMR in PeleC to represent practically relevant complex flow geometries, and the coupling strategy used to incorporate the combustion chemistry.  We will demonstrate that PeleC, which is leverages the AMReX library for block-structured AMR, is well-suited for modern, extreme-scale, heterogenous compute platforms.</EventDescription>
    <EventParentName>73164-SESS</EventParentName>
    <EventUniqueID>73164-118611</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>High Performance Bayesian Computing with Applications to Spatial/Spatiotemporal Data Analysis and Genome Wide Association Studies</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS40</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  3:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Marco Ferreira</EventSpeakers>
    <EventSpeakerIDs>803157</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117764</EventHandoutURL>
    <EventDescription>We present Bayesian algorithms for multi-core architectures for the analysis of spatial/spatiotemporal data and genome wide association studies (GWAS) data. To design Bayesian algorithms, we use the following guidelines: (1) Minimize communication amongst cores; (2) Break large datasets into many small pieces; (3) Transform the data to make computations faster. We employ these guidelines in three data analysis applications. The first application presents a multiscale decouple/recouple algorithm for the analysis of multivariate spatiotemporal data. This algorithm decouples the original dataset into many small datasets at multiple scales of resolution, analyzes these small datasets in parallel, and finally recouples these analyses into a coherent analysis at the original scale of resolution. The second application presents a Bayesian algorithm for the analysis of areal/regional spatial data where the selection of a best subset of predictors is of interest. This algorithm uses the spectral decomposition of the covariance matrix to transform the dependent variable and the predictors to the spectral domain, fits each possible model in parallel, and finally combines the results from the various models to choose the best subset of predictors. Related to the second application, the third application presents an algorithm for the identification of important predictors in GWAS where the number of possible predictors is on the order of hundreds of thousands to millions. 
</EventDescription>
    <EventParentName>73115-SESS</EventParentName>
    <EventUniqueID>73115-117764</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>BLIS: From BLAS to Tensor Contraction, Mixed-Precision, and Beyond</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS41</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:45:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Devin Matthews</EventSpeakers>
    <EventSpeakerIDs>795106</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117637</EventHandoutURL>
    <EventDescription>The BLIS project provides a portable framework for implementing high-performance BLAS operations such as matrix-matrix multiplication. Portability (in particular, performance portability) is supported by formulating  level 3 BLAS operations as a set of 5 loops, written in C99, around an assembly-language microkernel, in addition to matrix packing operations to improve data locality. Because the only "real" work happens in these computational and packing microkernels, we can use this framework to go beyond the standard BLAS API. In particular, we discuss how BLIS enables low-complexity implementations of mixed-precision/mixed-domain linear algebra and extended operations such as tensor contraction and machine learning primitives.</EventDescription>
    <EventParentName>73077-SESS</EventParentName>
    <EventUniqueID>73077-117637</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>HPAC: Evaluating Approximate Computing Techniques on HPC Openmp Applications</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS41</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Konstantinos Parasyris</EventSpeakers>
    <EventSpeakerIDs>796369</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117638</EventHandoutURL>
    <EventDescription>As we approach the limits of Moore’s law, researchers are exploring new paradigms for future high-performance computing (HPC) systems. Approximate computing has gained traction by promising to deliver substantial computing power. However, due to the stringent accuracy requirements of HPC scientific applications,  the broad adoption of approximate computing methods in HPC requires an in-depth understanding of the application’s amenability to approximations. 

We develop HPAC, a framework with compiler and runtime support for code annotation and transformation, and accuracy vs. performance trade-off analysis of OpenMP HPC applications. We use HPAC to perform an in-depth analysis of the effectiveness of approximate computing techniques when applied to HPC applications. The results reveal possible performance gains of approximation and its interplay with parallel execution. For instance, in the LULESH proxy application approximation provides substantial performance gains due to the reduction of memory accesses. However, in the leukocyte benchmark approximation induces load imbalance in the parallel execution and thus limiting the performance gains.
</EventDescription>
    <EventParentName>73077-SESS</EventParentName>
    <EventUniqueID>73077-117638</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Mixed-Precision Algorithm for Approximating Selected Eigenvalues and Eigenvectors of Symmetric and Hermitian Matrices</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS41</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:35:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Piotr Luszczek</EventSpeakers>
    <EventSpeakerIDs>739657</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117639</EventHandoutURL>
    <EventDescription>As the new hardware is being equipped with powerful low-precision capabilities
driven primarily by the needs of the burgeoning field of Artificial
Intelligence (AI), mixed-precision algorithms are now showing far greater
potential and renewed interest in scientific computing community.  The
multi-precision methods commonly follow approximate-iterate scheme by first
obtaining the approximate solution from a low-precision factorization and
solve.  Then, they iteratively refine the solution to the desired accuracy
that is often as high as what is possible with traditional approaches.
While targeting symmetric and Hermitian eigenvalue problems of
the form Ax=\lambda x, we
revisit the SICE algorithm proposed by Dongarra et al. By applying the
Sherman-Morrison formula on the diagonally-shifted tridiagonal systems, we
propose an updated SICE-SM algorithm.  By incorporating the latest two-stage
algorithms from the PLASMA and MAGMA software libraries for numerical linear
algebra, we achieved up to 3.6\times speedup using the mixed-precision
eigensolver with the blocked SICE-SM algorithm for iterative refinement when
compared with full double complex precision solvers for the cases with a portion
of eigenvalues and eigenvectors requested.
</EventDescription>
    <EventParentName>73077-SESS</EventParentName>
    <EventUniqueID>73077-117639</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Scalable Decomposition for Solving ACOPF with GPU Batch Solver</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS42</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:45:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1200</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Kibaek Kim</EventSpeakers>
    <EventSpeakerIDs>779499</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117444</EventHandoutURL>
    <EventDescription>We present the application of distributed control of alternating current optimal power flow, where a large problem is decomposed into many smaller nonlinear programs using a Lagrangian approach
The small nonlinear programs are solved by ExaTron, the implementation of a trust-region Newton algorithm for bound-constrained nonlinear programming problems, fully running on multiple GPUs. Without data transfers between CPU and GPU, our implementation achieves the elimination of a major performance bottleneck under a memory-bound situation, particularly when solving many small problems in batch. We discuss the design principles and implementation details for our kernel function and core operations. Different design choices are justified by numerical experiments. We present the numerical results that demonstrate computational performance of ExaTron on the Summit supercomputer at Oak Ridge National Laboratory, showing the linear scaling with respect to the batch size and the number of GPUs and more than 35 times speedup on 6 GPUs than on 40 CPUs available on a single node.
</EventDescription>
    <EventParentName>73019-SESS</EventParentName>
    <EventUniqueID>73019-117444</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Graph Analytics in the Exascale Era</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS43</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  3:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>500</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Mahantesh Halappanavar</EventSpeakers>
    <EventSpeakerIDs>739547</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116755</EventHandoutURL>
    <EventDescription>Combinatorial algorithms in general and graph algorithms in particular play a critical enabling role in numerous scientific applications. The irregular memory access nature of these algorithms makes them one of the hardest algorithmic kernels to implement on parallel systems. To address the challenges, the co-design center on combinatorial algorithms, ExaGraph, was established to design and develop methods and techniques for efficient implementation of key combinatorial (graph) algorithms chosen from a set of exascale applications, targeting the pre-exascale and exascale systems. Algebraic and combinatorial methods have a complementary role in the advancement of computational science and engineering, including playing an enabling role on each other. In this talk, we present a brief overview of the latest work on multi-graphics processing unit (GPU) systems for two prototypical graph problems: graph clustering and influence maximization, along with demonstrations on substantial gains in performance for both PNNL systems and the current No. 2 supercomputer, Summit. We believe that several applications will benefit from the algorithmic and software tools developed by the ExaGraph team.
</EventDescription>
    <EventParentName>72775-SESS</EventParentName>
    <EventUniqueID>72775-116755</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Parallel Graph Algorithms by Blocks for Heterogeneous Systems</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS43</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:10:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>500</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Abdurrahman Yasar</EventSpeakers>
    <EventSpeakerIDs>780566</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116757</EventHandoutURL>
    <EventDescription>Efficient graph analysis is crucial for many data analytics applications.
However, developing high-performance parallel graph algorithms is a
challenging task: Several well-known hardware and software-related challenges
arise. Recent architectural advances and variety makes designing
flexible graph kernels that can run well on various platforms more
challenging. In this work, we propose a novel graph processing framework for
modern shared-memory heterogeneous platforms. Our framework implements a
block-based programming model. That allows a user to express a graph algorithm
using host and device functors that operate on an ordered list of blocks
(subgraphs). Our framework deploys these computations to all available
resources in a heterogeneous architecture. One can implement a diverse set of
graph algorithms in our framework, and we support graph computations that fit
in host DRAM but not in GPU device memory. Our experimental results show that
these achieve competitive performance compared to hand-optimized
implementations. Based on our experiments in the median, our framework
achieves 1.6 times to 5.7 times better performance than state-of-the-art graph
processing systems, respectively.</EventDescription>
    <EventParentName>72775-SESS</EventParentName>
    <EventUniqueID>72775-116757</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>GPU Graph Processing using Modern C++</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS43</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  4:35:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>500</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Andrew Lumsdaine</EventSpeakers>
    <EventSpeakerIDs>785016</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116758</EventHandoutURL>
    <EventDescription xsi:nil="true" />
    <EventParentName>72775-SESS</EventParentName>
    <EventUniqueID>72775-116758</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>ComPort: Rigorous Testing Methods to Safeguard Numerics During Software Porting</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS44</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  3:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ganesh Gopalakrishnan</EventSpeakers>
    <EventSpeakerIDs>803609</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118370</EventHandoutURL>
    <EventDescription>The ComPort project develops techniques to ensure that numerical
results exhibited by codes continue to meet the intended science
objectives even after porting to new environments that involve
different compilers and/or the execution platforms. Our specific focus
will be the increased use of GPUs and other accelerators in future
platforms.  After an initial study to obtain an in-depth understanding
of how accelerators treat numerical expressions differently from
traditional CPUs, the ComPort project will aim for techniques to
root-cause code sections that introduce unacceptable degrees of
numerical variability. Incisive test-generation methods driven by
user-written higher-level specifications will help reveal such code
regions with higher assurance. Automated expression repair through
rewriting will help restore acceptable numerical behavior within
observed operating ranges of values. ComPort also intends to
incorporate these methods into performance portability layers such as
Kokkos and Raja.</EventDescription>
    <EventParentName>72832-SESS</EventParentName>
    <EventUniqueID>72832-118370</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Robust Identification of Differential Equations from Noisy Data</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS82</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:35:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:55:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Sung Ha Kang</EventSpeakers>
    <EventSpeakerIDs>734368</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118266</EventHandoutURL>
    <EventDescription>Identifying unknown differential equations from given discrete time dependent data is a challenging problem. Noisy data make such identification particularly challenging.  In this talk,  we present robust methods against a high level of noise which approximate the underlying noise-free dynamics well.  This approach is fundamentally based on numerical PDE techniques, and we introduce successively denoised differentiation and utilize subspace pursuit time evolution error  for PDE identification.  
</EventDescription>
    <EventParentName>73240-SESS</EventParentName>
    <EventUniqueID>73240-118266</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Toward Automated Test Generation for Performance Portable Programs using Clang/llvm and Formal Methods</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS44</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:45:00 PM</EventStartTime>
    <EventEndTime>02/24/22  4:05:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Shyamali Mukherjee</EventSpeakers>
    <EventSpeakerIDs>803614</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118378</EventHandoutURL>
    <EventDescription>Automated test generation using formal methods has been widely applied to verifying the correctness of system software and hardware, but comprehensive test coverage for scientific applications relevant to domain scientists is difficult to achieve with current frameworks. Furthermore, increasing complexity and limited accessibility of leadership-class high performance computing (HPC) systems make the testing of emerging applications even more complicated.  To address these challenges, we are going to develop and integrate high performance computing (HPC) runtime abstractions and emulation into automated test generation frameworks to make test generation faster, more comprehensive and more understandable to the domain scientists.  In this talk, we will demonstrate our initial attempt to automate detection of error-prone segments of application program source written with Kokkos (performance portable parallel programming model) for successful automatic test case generation.   We will discuss (1) formal specification of the Kokkos programming model and (2) adaptation of Clang’s abstract syntactic tree (AST) to extract Kokkos abstractions and API calls in a program source and (3) the integration of these two techniques.</EventDescription>
    <EventParentName>72832-SESS</EventParentName>
    <EventUniqueID>72832-118378</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>BLIS: Mixing, Matching, and Extending Precision</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS45</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Devangi Parikh</EventSpeakers>
    <EventSpeakerIDs>785470</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117042</EventHandoutURL>
    <EventDescription>We report on efforts to support alternative floating point precisions, mixed precision, and/or mixed domain computation within the BLAS-like Library Instantiation Software (BLIS) framework.  We describe a novel approach to the high-performance implementation of double-double precision matrix-matrix multiplication (gemm) in terms of a given double-precision matrix-matrix multiplication (dgemm).  This technique can be generalized to bootstrap a higher-precision gemm in terms of a given lower-precision gemms. The alternative object-based interface  available for BLIS simplifies the calling sequences for accessing such BLAS-like functionality.</EventDescription>
    <EventParentName>72752-SESS</EventParentName>
    <EventUniqueID>72752-117042</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Solving Mixed-Integer Nonlinear Programs in the ARPA-e Grid Optimization Competition</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS47</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:35:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:55:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1200</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hassan Hijazi</EventSpeakers>
    <EventSpeakerIDs>802986</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117449</EventHandoutURL>
    <EventDescription>The talk will cover recent work on solving large-scale Mixed-Integer NonLinear Programs (MINLPs). We will discuss ideas such as automatic projection of auxiliary variables, building convex outer relaxations as well as convex inner restrictions, and showcasing how they both play an important role in providing feasible solutions and optimality guarantees for challenging MINLPs. We will also discuss open-source software used to implement these ideas while exploiting parallelization in computing clusters. Numerical results on the ARPA-e Grid Optimization Competition Challenge 2 (https://gocompetition.energy.gov) will be presented.
</EventDescription>
    <EventParentName>73020-SESS</EventParentName>
    <EventUniqueID>73020-117449</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Portable Performance for AMR on GPUs: The Proto Approach</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS48</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Brian Van Straalen</EventSpeakers>
    <EventSpeakerIDs>88922</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118450</EventHandoutURL>
    <EventDescription xsi:nil="true" />
    <EventParentName>72873-SESS</EventParentName>
    <EventUniqueID>72873-118450</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Computational Waves in Parallel Programs and Their Impact on Performance Modeling</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS51</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:30:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>804</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Georg Hager</EventSpeakers>
    <EventSpeakerIDs>735540</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118316</EventHandoutURL>
    <EventDescription>Analytic, first-principles performance modeling of nontrivial parallel programs is hampered by the observation that simply adding runtime predictions for different program phases is often inaccurate. One of the reasons for this is that even perfectly load-balanced parallel code can get out of lockstep unless frequent synchronization is enforced. In this talk we show under which circumstances this "desynchronization" can occur and how it can lead to surprising effects, first and foremost an automatic overlap of communication overhead with useful work. Execution bottlenecks such as memory bandwidth and the behavior of a parallel code under external or internal noise are both crucial for the understanding of the dynamics of desynchronization: While bottleneck-free programs tend to synchronize automatically, bandwidth-bound programs appear to be unstable against noise and move into a state that can be described as a "computational wave," where different processes arrive at the same time step at different wall times. The amplitude of such waves is influenced by the communication topology of the code and connected to the velocity of "idle waves," propagating phases of idleness caused by one-off noise injections. </EventDescription>
    <EventParentName>73130-SESS</EventParentName>
    <EventUniqueID>73130-118316</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>The Price Performance of Performance Models</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS51</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:35:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:55:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>804</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Marcus Ritter</EventSpeakers>
    <EventSpeakerIDs>803893</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118335</EventHandoutURL>
    <EventDescription>To understand the scaling behavior of HPC applications, developers often
use performance models. A performance model is a formula that expresses
a key performance metric, such as runtime, as a function of one or more
execution parameters, such as core count and input size. Performance
models offer quick insights on a very high level of abstraction,
including predictions of future behavior. In view of the complexity of
today’s applications, which often combine several sophisticated
algorithms, creating performance models manually is extremely laborious.
Empirical performance modeling, the process of learning such models from
performance data, offers a convenient alternative, but comes with its
own set of challenges.  The two most prominent ones are noise and the
cost of the experiments needed to generate the underlying data. In this
talk, we will review the state of the art in empirical performance
modeling and investigate how we can employ machine learning and other
strategies to improve the quality and lower the cost of the resulting
models.</EventDescription>
    <EventParentName>73130-SESS</EventParentName>
    <EventUniqueID>73130-118335</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>perf-taint: Extracting Clean Performance Models from Tainted Programs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS51</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>804</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Marcin Copik</EventSpeakers>
    <EventSpeakerIDs>803585</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118336</EventHandoutURL>
    <EventDescription>Performance models are well-known instruments to understand
the scaling behavior of parallel applications. They express how
performance changes as key execution parameters, such as the number of
processes or the size of the input problem, vary. Besides reasoning
about program behavior, such models can also be automatically derived
from performance data. This is called empirical performance modeling.
While this sounds simple at the first glance, this approach faces
several serious interrelated challenges, including expensive
performance measurements, inaccuracies inflicted by noisy benchmark
data, and overall complex experiment design, starting with the
selection of the right parameters. The more parameters one considers,
the more experiments are needed and the stronger the impact of noise.
In this paper, we show how taint analysis, a technique borrowed from
the domain of computer security, can substantially improve the
modeling process, lowering its cost, improving model quality, and help
validate performance models and experimental setups.</EventDescription>
    <EventParentName>73130-SESS</EventParentName>
    <EventUniqueID>73130-118336</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Overview of Excalibur Project Neptune</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS52</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:30:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>3000</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Wayne Arter</EventSpeakers>
    <EventSpeakerIDs>790026</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118086</EventHandoutURL>
    <EventDescription>The talk will describe ExCALIBUR project NEPTUNE, list contributors, and describe the aims of the project as set in the abstract for the minisymposium. Background to the other presentations in the session(s) will be given.
</EventDescription>
    <EventParentName>73205-SESS</EventParentName>
    <EventUniqueID>73205-118086</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Improving the Speed and Quality of Parallel Graph Coloring</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS53</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:35:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:55:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>500</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ghadeer Alabandi</EventSpeakers>
    <EventSpeakerIDs>802670</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=116760</EventHandoutURL>
    <EventDescription>Graph coloring assigns a color to each vertex of a graph such that no two adjacent vertices get the same color. It is a key building block in many applications. In practice, solutions that require fewer distinct colors and that can be computed faster are often preferable. Various coloring heuristics exist that provide different quality versus speed tradeoffs. The highest-quality heuristics tend to be quite slow. To improve performance, several parallel implementations have been proposed. In this talk, I will describe two improvements to the widely used LDF heuristic. First, I will present “shortcutting” approaches to increase the parallelism by non-speculatively breaking data dependencies. Second, I will present “color reduction” techniques to boost the solution quality. On 18 graphs from diverse domains, the shortcutting techniques yield 2.5 times more parallelism on average, and the color-reduction techniques result in up to 20\% fewer colors needed. My deterministic CUDA implementation running on a Titan V is 2.9 times faster and uses as few or fewer colors as the best GPU codes from the literature. </EventDescription>
    <EventParentName>72776-SESS</EventParentName>
    <EventUniqueID>72776-116760</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Distributed Memory Implementation of CP-POPT-GDGN: An All-at-Once Decomposition Algorithm for Count Tensors</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS57</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5100</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Teresa Ranadive</EventSpeakers>
    <EventSpeakerIDs>797574</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118723</EventHandoutURL>
    <EventDescription>Data sets from many applications can be represented as count tensors. As such tensors grow larger and increasingly elaborate, more sophisticated algorithms are required to enable accurate CP decompositions of these tensors.  Recent work has demonstrated that CP-POPT-GDGN, an all–at–once algorithm for count tensor decomposition that utilizes a generalized damped Gauss-Newton method, typically achieves more accurate decompositions than earlier count tensor decomposition algorithms, including CP-APR.  In this work, we consider a distributed memory implementation of CP-POPT-GDGN, and discuss its scalability.  This implementation allows us to compute CP decompositions in an amount of time comparable to that required by a distributed memory implementation of CP--APR, when using the same computing resources.  

</EventDescription>
    <EventParentName>73352-SESS</EventParentName>
    <EventUniqueID>73352-118723</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A New Alternating Optimization Algorithm for CP Decomposition</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS57</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:00:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5100</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Navjot Singh</EventSpeakers>
    <EventSpeakerIDs>790176</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118724</EventHandoutURL>
    <EventDescription>CP decomposition (CPD) is prevalent in chemometrics, signal processing, data mining and many more fields. While a plethora of algorithms have been proposed to compute the CPD, alternating least squares (ALS) still remains one of the most widely used algorithm for computing the decomposition. Recent works have shown that ALS suffers from slow convergence when the CP factors become collinear (also known as the swamp phenomenon). Several attempts have been made to overcome this issue through incorporating techniques like line search, pivoting in the ALS algorithm. We propose a new algorithm which minimizes a different objective function via alternating optimization leading to a sequence of better conditioned iterates of the CP factors, alleviating the swamp phenomenon. Alternating optimization of this new objective leads to simple updates to the factor matrices with the same asymptotic computational cost as ALS. We show that a subsweep of this algorithm can achieve a superlinear convergence rate for CPD with known rank and verify it experimentally. We further introduce a formulation which generalizes our approach to interpolate between updates corresponding to the ALS and the new algorithm to manage the tradeoff between stability and fitness of the decomposition for approximation problems.  Our experimental results show that for approximate CPD, this algorithm and its variants converge to a better conditioned decomposition with a small change in fitness as compared to the ALS.
</EventDescription>
    <EventParentName>73352-SESS</EventParentName>
    <EventUniqueID>73352-118724</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>IP3 </EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP3</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  2:05:00 PM</EventStartTime>
    <EventEndTime>02/24/22  2:50:00 PM</EventEndTime>
    <EventFilter>PP22|Invited Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>27158</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>762908</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73436</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73436-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>1</DirectLinkEnabled>
    <DirectLinkRecordingEnabled>1</DirectLinkRecordingEnabled>
    <start>2022-02-24T14:05:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS16 Domain Decomposition Methods – Parallel Algorithms, Software, and Machine Learning - Part II of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS16</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs>720131,803849,774576,763792,790005</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803784,741246,729291,720131</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73347</EventHandoutURL>
    <EventDescription>Domain decomposition methods are parallel scalable, preconditioned iterative methods for the efficient solution of problems arising from the discretization of partial differential equations. In this minisymposium, we focus on several important aspects of domain decomposition methods: Coarse spaces, nonlinear preconditioning, high-performance computing, software development, and scientific machine learning.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73347-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Performance Modeling of Graph Processing Workloads</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS58</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>804</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ana Lucia Varbanescu</EventSpeakers>
    <EventSpeakerIDs>803587</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118338</EventHandoutURL>
    <EventDescription>Graph processing workloads are increasing in popularity and complexity: many problems ranging from bioinformatics to text analysis, and from social network analysis to logistics and optimizations use graphs as primary data models. With the increase in analysis complexity and data size, graph processing performance becomes problematic: execution time increases signficantly, yet unpredictably. 

For the past 15+ years, using massive parallelism - often in the form of GPU-like accelerators - has been a successful approach for many applications. However, parallel graph processing workloads do not always provide the expected performance benefits; moreover, the performance is heavily dependent on the graph structure, system architecture, and the algorithm itself. 

In this talk, we will discuss several methods for performance modeling for parallel graph processing workloads on GPUs. We will contrast and compare analytical and machine-learning models, and discuss the pro's and con's of both approaches. We will show results on multiple GPU systems, different applications, and a large number of datasets.  </EventDescription>
    <EventParentName>73131-SESS</EventParentName>
    <EventUniqueID>73131-118338</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Automatic Application Performance Data Collection with Caliper and Adiak</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS58</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  4:25:00 PM</EventStartTime>
    <EventEndTime>02/25/22  4:45:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>804</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>David Boehme</EventSpeakers>
    <EventSpeakerIDs>803591</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118340</EventHandoutURL>
    <EventDescription>Most performance analysis, optimization, and modeling tasks rely on accurate performance measurements, but collecting this data is often a time-consuming process that requires specialized tools and expert users. This talk presents two software libraries, Caliper and Adiak, that simplify and automate performance data collection. Caliper embeds performance profiling capabilities directly into HPC codes, so that performance measurements can be enabled at runtime for any program run. Equally important, Adiak collects program metadata - what was the machine, software environment, and program configuration for a given run? Together, these tools enable new, automated workflows for large performance comparison studies or long-term performance regression testing. We demonstrate use cases and analysis techniques for automated performance data recording involving large collections of runs.
</EventDescription>
    <EventParentName>73131-SESS</EventParentName>
    <EventUniqueID>73131-118340</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>An MPI-Based Algorithm for Mapping Complex Networks onto Hierarchical Architectures</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS60</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  3:55:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Henning Meyerhenke</EventSpeakers>
    <EventSpeakerIDs>781788</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117704</EventHandoutURL>
    <EventDescription>Processing massive application graphs on distributed memory systems requires to map the graphs onto the system's processing elements (PEs). This task becomes all the more important when PEs have non-uniform communication costs or the input is highly irregular. Typically, mapping is addressed using partitioning, in a two-step approach or an integrated one. Parallel partitioning tools do exist; yet, corresponding mapping algorithms or their public implementations all have major sequential parts or other severe scaling limitations. We propose a parallel algorithm that maps graphs onto the PEs of a hierarchical system. Our solution integrates partitioning and mapping; it models the system hierarchy as an implicit labeled tree. The vertices of the application graph are labeled as well, and these vertex labels induce the mapping. The mapping optimization follows the basic idea of parallel label propagation, but we tailor the gain computations of label changes to quickly account for the induced communication costs. Our MPI-based code extends the partitioning library ParHIP. To evaluate our algorithm's implementation, we perform comparative experiments with complex networks in the million- and billion-scale range. In general our mapping tool shows good scalability on up to a few thousand PEs. Compared to other MPI-based competitors, our algorithm achieves the best speed to quality trade-off and our quality results are even better than non-parallel mapping tools.</EventDescription>
    <EventParentName>73094-SESS</EventParentName>
    <EventUniqueID>73094-117704</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Dynamic and Intelligent Workflows with Eflows4hpc</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS65</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:00:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Rosa Badia</EventSpeakers>
    <EventSpeakerIDs>719917</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118803</EventHandoutURL>
    <EventDescription>Distributed computing infrastructures are evolving from traditional models to environments that involve sensors, edge devices, instruments, etc, and,  as well, high-end  computing systems such as clouds and HPC clusters. A key aspect is how to describe and develop the applications to be executed in such platforms. What is more, the use of data analytics and artificial intelligence in general is on high demand in current HPC applications. However, the methodologies to develop such workflows that integrate HPC simulations and data analytics is not well integrated. eFlows4HPC project has recently started with the goal of providing workflow software stack and an additional set of services to enable the integration of HPC simulations and modelling with big data analytics and machine learning in scientific and industrial applications. The project will demonstrate its advances through three application Pillars with high industrial and social relevance: manufacturing, climate and urgent computing for natural hazards; these applications will help to prove how the realization of forthcoming efficient HPC and data-centric applications can be developed with new workflow technologies. The talk will present the motivation, challenges and project workplan.
</EventDescription>
    <EventParentName>73378-SESS</EventParentName>
    <EventUniqueID>73378-118803</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Multi-GPU Implementation of Sparse Triangular Solve and Its Performance Analysis</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS68</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:05:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:25:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yang Liu</EventSpeakers>
    <EventSpeakerIDs>780194</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118444</EventHandoutURL>
    <EventDescription>Solving large-scale sparse triangular linear systems is a critical computation component for many ECP and SciDAC application codes. However due to low-arithmetic intensity and sequential nature of sparse triangular solve (SpTRSV), optimizing its performance on GPU-based HPC machines is a tremendously difficult problem. This talk presents recent efforts for improving multi-GPU sparse solves in the SuperLU_DIST software. First, we improve the data structure, parallelization strategy and memory transfer for single-GPU SpTRSV on both NVIDIA and AMD GPUs. Second, we developed a NVSHMEM/ROCSHMEM-based multi-GPU SpTRSV code to minimize communication costs. Third, we developed a critical-path-based performance model to predict and analyze the multi-GPU SpTRSV performance.</EventDescription>
    <EventParentName>72875-SESS</EventParentName>
    <EventUniqueID>72875-118444</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Prototyping Parallel-in-Time Integration Methods with pySDC</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS69</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:00:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Robert Speck</EventSpeakers>
    <EventSpeakerIDs>747124</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117794</EventHandoutURL>
    <EventDescription>The efficient use of modern supercomputers has become one of the key challenges in computational science. For the numerical solution of time-dependent processes, time-parallel methods have opened new ways to overcome both strong and weak scaling limits. With the "parallel full approximation scheme in space and time" (PFASST), multiple time-steps can be integrated simultaneously. Based on spectral deferred corrections (SDC) methods, PFASST uses a space-time hierarchy with various coarsening strategies to maximize parallel efficiency. 
However, since the integration of SDC or PFASST into an existing application code is by far not straightforward and the potential gain is typically uncertain, the Python prototyping framework pySDC enables users to rapidly test new ideas in this field and to implement first toy problems more easily and more quickly. Yet, the main focus of this framework is time-integration of PDEs and in order to keep this focus, spatial discretization, solvers and (ideally) parallelization are left to established software frameworks like PETSc or FEniCS. In this talk we report on the pros and cons of this separation, the results achieved so far, current roadblocks and our ever-growing todo list. We will also shed some light on potential collaboration opportunities with other research software developers or potential users.

</EventDescription>
    <EventParentName>73122-SESS</EventParentName>
    <EventUniqueID>73122-117794</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Hippylib-Muq: Scalable Markov Chain Monte Carlo Sampling Methods for Large-Scale Bayesian Inverse Problems Governed by PDEs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS69</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:30:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:50:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Ki-Tae Kim</EventSpeakers>
    <EventSpeakerIDs>797025</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117797</EventHandoutURL>
    <EventDescription>Bayesian inference provides a systematic framework to quantify and reduce
uncertainties in inverse problems governed by partial differential equations (PDEs),
but its solution for large-scale inverse problems is computationally
intractable.
We present in this paper an extensible and scalable software framework to
overcome the prohibitive nature of Bayesian inverse problems, providing
unprecedented access to state-of-the-art algorithms for large-scale
Bayesian inverse problems governed by PDEs forward models.
The software integrates two complementary open-source software packages,
hIPPYlib and MUQ.
hIPPYlib implements powerful large-scale gradient/Hessian-based inverse solvers
in an environment that can automatically generate needed derivatives, but it
lacks full Bayesian capabilities.
MUQ provides a spectrum of powerful Bayesian inversion models and algorithms,
but expects forward models to come equipped with gradients/Hessians to permit
large-scale solution.
By combining these two complementary libraries, we create a robust, scalable,
and efficient software framework that realizes the benefits of each to tackle
complex large-scale Bayesian inverse problems across a broad spectrum of
scientific and engineering disciplines.



</EventDescription>
    <EventParentName>73122-SESS</EventParentName>
    <EventUniqueID>73122-117797</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Programming Heterogeneous Hpc Platforms with the Starpu Task-Based Runtime System</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS70</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:30:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:50:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Oliver Aumage</EventSpeakers>
    <EventSpeakerIDs>803353</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117939</EventHandoutURL>
    <EventDescription>The StarPU task-based runtime system for heterogeneous platforms is developed by INRIA Team STORM in Bordeaux, France. It offers heterogeneous task scheduling capabilities on computing nodes equipped with accelerators, while presenting application developers with its programmer-friendly sequential task flow programming model. This talk will give an overview of StarPU's model and specific characteristics, together with an update on latest developments.
</EventDescription>
    <EventParentName>73178-SESS</EventParentName>
    <EventUniqueID>73178-117939</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Multi-Start Poisson Tensor Factorization</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS71</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:00:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5100</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jeremy Myers</EventSpeakers>
    <EventSpeakerIDs>790183</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118727</EventHandoutURL>
    <EventDescription>The Poisson Tensor Factorization (PTF) is a special case of the CP decomposition applied to low-rank, multilinear structure via Poisson maximum likelihood estimation. Owing to the nonlinearity and non-convexity of the underlying problem, it is important to use a multi-start strategy to improve the chance of recovering the maximum likelihood estimator rather than suboptimal local solutions. We compare scalability-accuracy trade-offs among state-of-the-art methods for solving the PTF problem on real problems in the multi-start context. </EventDescription>
    <EventParentName>73353-SESS</EventParentName>
    <EventUniqueID>73353-118727</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Parallel Tensor Train Rounding using Gram SVD</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS71</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:55:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5100</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Grey Ballard</EventSpeakers>
    <EventSpeakerIDs>735519</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118730</EventHandoutURL>
    <EventDescription>Tensor Train (TT) is a low-rank tensor representation consisting of a series of three-way cores whose dimensions specify the TT ranks.  Formal tensor train arithmetic often causes an artificial increase in the TT ranks.  Thus, a key operation for applications that use the TT format is rounding, which truncates the TT ranks subject to an approximation error guarantee.  Truncation is performed via SVD of a highly structured matrix, and current rounding methods require careful orthogonalization to compute an accurate SVD.  We propose a new algorithm for TT rounding based on the Gram SVD algorithm that avoids the expensive orthogonalization phase.  Our algorithm performs less computation and can be parallelized more easily than existing approaches, at the expense of a slight loss of accuracy.  We demonstrate that our implementation of the rounding algorithm is efficient, scales well, and consistently outperforms the existing state-of-the-art parallel implementation in our experiments.</EventDescription>
    <EventParentName>73353-SESS</EventParentName>
    <EventUniqueID>73353-118730</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Design, Usage and Features of the Tlapack Library</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS72</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:55:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Weslley Pereira</EventSpeakers>
    <EventSpeakerIDs>803811</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118778</EventHandoutURL>
    <EventDescription>The sustainability of legacy software libraries, in particular legacy numerical libraries, has been seriously deteriorated by several recent radical changes in microprocessor and system designs. These have rendered most legacy software infrastructure obsolete. The Linear Algebra PACKage (LAPACK) is a community standard for dense linear algebra and has been adopted and supported by a large community of users, computing centers, and high-performance computing (HPC) vendors. The &lt;T&gt;LAPACK library is one proposal to close the gap between LAPACK and the new and emerging computing platforms. It uses C++ templates to provide precision-neutral algorithms, i.e., which work in single, double, half, and multiprecision types, and, in some cases, allow for mixed-precision. In the long term, we hope that some of the framework of &lt;T&gt;LAPACK could be used to develop new implementations of state-of-the-art numerical linear algebra libraries. In this talk, I will present &lt;T&gt;LAPACK, its design, features, and some examples of usage. I will talk about the test suite that has been developed jointly.</EventDescription>
    <EventParentName>73372-SESS</EventParentName>
    <EventUniqueID>73372-118778</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Evolutionary Re-Engineering of Multi-Physics Industrial HPC Applications with OP-DSLs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS73</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:00:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Gihan Mudalige</EventSpeakers>
    <EventSpeakerIDs>803867</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118763</EventHandoutURL>
    <EventDescription>The rapid evolution of High Performance Computing architectures has resulted in highly complex systems with massively parallel heterogeneous processors, deep and multiple memory hierarchies and interconnects. As a result, maintaining performance as platforms change has becomes increasingly difficult. On the one hand, open standards have been slow to catch up with supporting new hardware, and for many real applications have not provided the best performance achievable from these systems. On the other hand, proprietary solutions have only targeted narrow vendor-specific devices resulting in a proliferation of parallel programming models and technologies. The only practically viable approach to addressing the above issue, is through the development of appropriate application-oriented, high-level programming abstractions such as Domain Specific Languages (DSLs). In this talk I will present how two of the earliest DSLs developed in the UK, OP2 and OPS, have been able to build on multi-layered abstractions techniques to evolve production-grade HPC applications. I will detail how simple source-to-source and automatic code-generation techniques using maintainable software technologies have enabled us to develop a framework that has remained agile in delivering performance portability in the face of nearly a decade of hardware and parallel programming innovations. I will also discuss lessons learnt and my outlook for our DSLs in the run-up to deploying exa-scale systems. </EventDescription>
    <EventParentName>73373-SESS</EventParentName>
    <EventUniqueID>73373-118763</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Vectorized Implicit Time Discretion</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS75</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:55:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Christian Engwer</EventSpeakers>
    <EventSpeakerIDs>732534</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118636</EventHandoutURL>
    <EventDescription>Numerical software to approximatly solve partial differential
equations (PDE) is of central importance in many fluid dynamics
application. Constantly increasing model details require almost
unlimited computer power, but the way increased hardware performance
is achieved has changed fundamentally in recent years.

To use modern hardware efficiently new dedicated numerical methods are
necessary. Particular challenges are posed by the increasing parallelism,
especially the instruction level parallelism, as well as the ever
increasing gap between memory and computing speed.

We will discuss a novel approach to increase the arithmetic intensity
and to make full use of instruction level parallelizem, like AVX512.
The idea is to reformulate the space-time problem such yields a matrix
equation, similar to the approaches taken in parallel in time (PiT)
methods. The fundamental difference to PiT methods is that we do not
aim at using more nodes, but to increase the local efficiency on the
individual nodes. The arising matrix equation is then solved
efficiently using block-Krylov methods.

We will discuss how to efficiently implement and vectorize
block-Krylov methods and investigate how the increase performance of a
single Krylov iteration and the decreased convergence rate (due to our
reformulation) can be balanced to achieve the best over-all
performance.</EventDescription>
    <EventParentName>73338-SESS</EventParentName>
    <EventUniqueID>73338-118636</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Versatile and Modular Parallel Multi-Resolution Framework for Compressible Multiphase Flows</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS75</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:30:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:50:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Nikolaus Adams</EventSpeakers>
    <EventSpeakerIDs>781066</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118640</EventHandoutURL>
    <EventDescription xsi:nil="true" />
    <EventParentName>73338-SESS</EventParentName>
    <EventUniqueID>73338-118640</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Sparseloop: An Analytical Design Space Exploration Tool for Sparse Tensor Accelerators</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS76</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:05:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:25:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>801</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Po-An Tsai</EventSpeakers>
    <EventSpeakerIDs>803883</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118788</EventHandoutURL>
    <EventDescription>Sparse tensor algorithms are critical to many emerging workloads (DNNs, data analytics, recommender systems, graph algorithms, etc.). As a result, recently, many sparse tensor accelerators and systems have been proposed to improve efficiency and performance for sparse tensor algorithms. These designs often involve complex tradeoffs between compression formats, hardware support that exploits sparsity, and on-chip dataflows. Each design therefore represents a specific design point in the large space. As more and more distinct solutions are proposed, we as a community need a better tool to model and search the large design space for various sparse tensor algorithms.

In this talk, I will introduce Sparseloop, a tool that analytically models the impact of different dataflows, data representations, and associated hardware support for sparsity in sparse tensor accelerators. Sparseloop builds on prior dense tensor accelerator modeling tools and serves as an efficient tool for architects to explore the large design space for sparse tensor accelerators. Sparseloop is fast, accurate, and flexible for design space exploration. Each modeling takes less than 0.1 seconds to generate results, which is orders of magnitude faster than cycle-accurate simulators, and Sparseloop produces less than 1% error compared to custom hardware analytical models. I will also present case studies of using Sparseloop to study various sparsity optimizations, such as hardware support for structured sparsity.</EventDescription>
    <EventParentName>73326-SESS</EventParentName>
    <EventUniqueID>73326-118788</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Automatic High-Performance Kernel Generation for Accelerators : Specialization to Generalization</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS76</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 11:30:00 AM</EventStartTime>
    <EventEndTime>02/26/22 11:50:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>801</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Prasanth Chatarasi</EventSpeakers>
    <EventSpeakerIDs>803884</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118790</EventHandoutURL>
    <EventDescription>The goal of achieving lower latency, higher throughput, and higher energy efficiency for compute-intensive domains (e.g., Deep Learning) has led to a meteoric increase in domain-specific hardware accelerators. However, it requires significant efforts to develop both hardware and software stacks for each domain. An even bigger problem is the lack of flexibility and scalability of the hardware and software stack across multiple domains. As a result, there is a great interest in building programmable and flexible accelerators. In this talk, I present two works starting with closely looking at the "Vyasa" compiler generating high-performance code for the specific accelerator, i.e., for Xilinx AI Engine, and the "PolyEDDO" framework targetting code generation for various accelerators using hardware abstractions. 

</EventDescription>
    <EventParentName>73326-SESS</EventParentName>
    <EventUniqueID>73326-118790</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>ParaRKC: A Parallel Runge-Kutta-Chebyshev Method</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS78</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Peter Förster</EventSpeakers>
    <EventSpeakerIDs>803760</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118646</EventHandoutURL>
    <EventDescription>The Parareal parallel-in-time algorithm [Lions et al., "A parareal in time discretization of PDEs, 2001] has been successfully applied to numerous engineering problems over the last two decades. It allows for the concurrent solution of initial value problems, independent of the choice of time integrator. Often, it is employed in combination with implicit time integration schemes to solve semi-discretized parabolic partial differential equations. An alternative to implicit integrators is given by Runge-Kutta-Chebyshev (RKC) methods [Sommeijer et al., "RKC: An explicit solver for parabolic PDEs", 1998]. Due to their explicit structure, they avoid matrix inversions and may therefore save computational effort compared to implicit schemes.

Recently it was noted that a Parareal-like update may be used to parallelize any recursion relation admitting solutions based on different accuracies [Gander et al., "ParaStieltjes: Parallel computation of Gauss quadrature rules using a Parareal-like approach for the Stieltjes procedure", 2021]. Using this idea we propose a new approach (ParaRKC) combining Parareal and RKC type methods. The new method parallelizes over the internal stages of the RKC method, making use of a Parareal-like update and a coarse spatial discretization for the coarse propagator. We present results on the performance of ParaRKC and a comparison with Parareal using only a coarsening in space.



</EventDescription>
    <EventParentName>73333-SESS</EventParentName>
    <EventUniqueID>73333-118646</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Space-Time Block Preconditioning</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS78</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  3:05:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:25:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Federico Danieli</EventSpeakers>
    <EventSpeakerIDs>784887</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118680</EventHandoutURL>
    <EventDescription>Parallel-in-time (PinT) methods have become increasingly popular in the simulation of time-dependent numerical PDEs, owing to their potential for providing additional speedup when more classical spatial parallelism saturates. In particular, all-at-once schemes attempt to achieve parallelisation in time by simultaneously solving the whole system which arises from the monolithic space-time discretisation of the target PDEs. 
In this talk, we consider a novel take on the all-at-once solution of systems of PDEs, and introduce the concept of space-time block preconditioning. The proposed method takes motivation from well-developed spatial block preconditioning techniques used in sequential time-stepping or steady-state problems, and extends such principles to the whole space-time setting. The resulting preconditioning procedure has the advantage of making the system more amenable to be solved via PinT methods, by reducing the target system to (parallel-)integrate in time to single-variable PDEs.
To showcase its efficiency and flexibility, the method is tested on problems in incompressible fluid dynamics and magnetohydrodynamics. Results indicate good scalability under temporal and spatial mesh refinement in terms of number of iterations to convergence for both the linear and nonlinear solvers. Moreover, the total number of preconditioner applications compares well with sequential time-stepping, showing only minimal overhead in this sense.
</EventDescription>
    <EventParentName>73333-SESS</EventParentName>
    <EventUniqueID>73333-118680</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Overview of Some Popular Machine Learning Frameworks for Data Parallelism</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS80</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Denis Boyda</EventSpeakers>
    <EventSpeakerIDs>803842</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118736</EventHandoutURL>
    <EventDescription>Data parallelism is used in machine learning to improve the performance of model optimization and evaluation. In this scheme, the data is split into multiple workers/nodes, and every worker optimizes a replica of a model with its piece of data. In the process of optimization, every worker computes the loss function and gradients, but before updating the model parameters, gradients are averaged over all workers through the collective operation. Although the idea of data parallelism is quite simple, the practical implementation of it is complicated and, without proper handling of communications, may result in poor performance. We do an overview of several popular data parallelism frameworks and study their scaling with a number of nodes.</EventDescription>
    <EventParentName>73367-SESS</EventParentName>
    <EventUniqueID>73367-118736</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Robust Approximation of Tensor Networks, and Its Applications in Quantum Chemistry</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS81</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5100</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Edward Valeev</EventSpeakers>
    <EventSpeakerIDs>763099</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118733</EventHandoutURL>
    <EventDescription>Approximation of a tensor network by approximating (e.g., factorizing) one or more of its constituent tensors can be improved by canceling the leading-order error due to the constituents' approximation. The utility of such robust approximation is demonstrated for robust canonical polyadic (CP) approximation of a (density-fitting) factorized 2-particle Coulomb interaction tensor. The resulting algebraic (grid-free) approximation for the Coulomb tensor, closely related to the factorization appearing in pseudospectral and tensor hypercontraction approaches, is efficient and accurate, with significantly reduced rank compared to the naive (non-robust) approximation. Application of the robust approximation to the particle-particle ladder term in the coupled-cluster singles and doubles reduces the size complexity from $\mathcal{O}(N^6)$ to $\mathcal{O}(N^5)$ with robustness ensuring negligible errors in chemically-relevant energy differences using CP ranks approximately equal to the size of the density-fitting basis. Preliminary applications to other electronic structure methods will also be discussed.</EventDescription>
    <EventParentName>73354-SESS</EventParentName>
    <EventUniqueID>73354-118733</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Towards Data-Driven Inference of Discrete Differential Operators</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS82</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:30:00 AM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Yannis Schumann</EventSpeakers>
    <EventSpeakerIDs>803527</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118262</EventHandoutURL>
    <EventDescription>Partial differential equations (PDEs) are extensively used across scientific disciplines for modeling and describing various processes under consideration. Finite element, finite difference or similar methods allow to numerically solve them by discretizing the variables under consideration, e.g. space and time. In this discretized domain, differential operators can be approximated by matrices with nonzero coefficients for positions in the neighborhood of the considered point – the so called stencil.


We consider the inference of stencil weights for differential operators from linear, one-dimensional and two-dimensional PDEs using comprehensive regression techniques. Starting with the 1D case, we show that linear regression using an ordinary-least-squares (OLS) approach is able to recover mathematically meaningful stencils given a full-rank matrix of predictor variables. We discuss, how regularization techniques can allow the inference of the correct stencils even for rank-deficient matrices. The performance of the OLS-approach is compared for different orders of expansion of the differential operator. We discuss the impact of noise on the data in both predictor and predicted variables for various noise levels and compare different errors-in-variables approaches to mitigate the inherent consequences of noisy predictors. The presented techniques will be extended to the two dimensional case.</EventDescription>
    <EventParentName>73240-SESS</EventParentName>
    <EventUniqueID>73240-118262</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Updates on Sequential and Parallel FFTX</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS83</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Franz Franchetti</EventSpeakers>
    <EventSpeakerIDs>795282</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117334</EventHandoutURL>
    <EventDescription>We present an update on the design of both the sequential and parallel API of FFTX. The software package is developed as part of the DOE ExaScale effort by LBL, Carnegie Mellon University, and SpiralGen, Inc. We aim at translating the LAPACK/BLAS approach from the numerical linear algebra world to the spectral algorithm domain. FFTX is extending and updating FFTW for the exascale era and beyond while providing backwards compatibility. Patterns of FFTX call sequences capture higher level spectral algorithms and their variants, including convolutions, Poisson solvers, correlations, and numerical differentiation approaches that translate to FFT calls. The SPIRAL code generation and autotuning system--now available as open source under a BSD/Apache license--underpins the effort to provide performance portability.
 </EventDescription>
    <EventParentName>72986-SESS</EventParentName>
    <EventUniqueID>72986-117334</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>EPEEC: The European Joint Effort Towards Highly Productive Programming for Heterogeneous HPC</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS85</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Antonio Pena</EventSpeakers>
    <EventSpeakerIDs>803354</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117941</EventHandoutURL>
    <EventDescription>The European joint Effort toward a Highly Productive Programming Environment for Heterogeneous Exascale Computing (EPEEC) is a Horizon 2020 project from the European Commission involving 10 partners, which include research centres, universities, and SMEs (https://epeec-project.eu). Started 2018 and already on its last mile, the partners have been advancing European technology in programming environments specifically addressed to turn upcoming overwhelmingly complex supercomputers into manageable platforms for domain application developers. EPEEC’s components include tools, programming models, and applications. After having released intermediate software prototypes, EPEEC will soon release final software components.</EventDescription>
    <EventParentName>73179-SESS</EventParentName>
    <EventUniqueID>73179-117941</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Architecture’s View Point of Hardware-Software Co-Design for Heterogeneous Computing</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS85</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Hyesoon Kim</EventSpeakers>
    <EventSpeakerIDs>747195</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117942</EventHandoutURL>
    <EventDescription>ISA has been an excellent abstract to decouple microarchitecture and applications. However, rapid progress in new circuit technologies and expanding applications make the boundaries become a blur. Furthermore, new technologies such as processing near memory demand a new set of ISAs to reflect the computation changes. In this talk, I’ll present a few examples that show the trade-off between software implementation vs. hardware support.
First, I’ll focus on processing near memory.  When is good to offload computation on PNM can be studied with both analytical models and also cycle-level simulation. Second, I’ll discuss the design trade-off on heterogeneous computing platforms especially CPU+GPU architectures.  I’ll also discuss how to abstract the applications and architecture to estimate the benefits of new ISA support.

</EventDescription>
    <EventParentName>73179-SESS</EventParentName>
    <EventUniqueID>73179-117942</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Task-Based Parallel Programming for Scalable Algorithms : Application to Matrix Multiplication</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS86</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:10:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Antoine Jego</EventSpeakers>
    <EventSpeakerIDs>803042</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=117562</EventHandoutURL>
    <EventDescription>  Task-based programming models have succeeded in gaining attraction in
  the numerical linear algebra community thanks to how they have
  relieved part of the burden of building portable distributed-memory
  parallel algorithms.  In increasingly larger, more heterogeneous
  clusters of computers, these models appear as a way to maintain and
  enhance more complex algorithms.  However, task-based programming
  models lack flexible, elegant expression for scalable algorithms
  such as Communication Avoiding algorithms.  We show that the
  Sequential Task Flow paradigm can be extended to write a compact yet
  efficient scalable General Matrix Multiplication.  This extension
  required fewer modifications to the StarPU runtime system.  The
  final implementation is competitive with state-of-the-art libraries
  as it outperforms them in application-specific scenarios or on
  irregular logical computing grids, using homogeneous multicore
  clusters.
</EventDescription>
    <EventParentName>73057-SESS</EventParentName>
    <EventUniqueID>73057-117562</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Ultracold Neutral Plasmas: An Ideal Testbed for Numerical Plasma Models</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS87</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  2:15:00 PM</EventStartTime>
    <EventEndTime>02/26/22  2:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Grant Gorman</EventSpeakers>
    <EventSpeakerIDs>803650</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118465</EventHandoutURL>
    <EventDescription>Ultracold neutral plasmas (UCNPs), created by photoionization of a cold gas, are an excellent platform for studying neutral plasmas in far more complex environments such as plasma in the Sun’s atmosphere, white dwarf stars, and inertial-confinement fusion devices. UCNPs have clean and controllable initial conditions, with ion temperatures below 1 K, tunable electron temperatures from $1-1000\,$K, and densities ranging from $10^6-10^{12}\,$cm$^{-3}$, and precise diagnostics and have been used to benchmark theories and numerical techniques for molecular-dynamic, kinetic, and hydrodynamic descriptions of the plasma, including those that capture strongly coupled physics. Magnetized UCNPs are of current interest because of the interplay of magnetization and strong coupling, connection to plasma confinement, and modification of recombination dynamics in strong fields. We recently demonstrated the magnetic confinement of UCNPs at the null of a biconic cusp, or quadrupole magnetic field. In this talk, I will discuss what makes UCNPs amenable to a wide variety of computational modeling techniques and will overview our recent progress developing a two-fluid magnetohydrodynamic code to model UCNP evolution in the presence of quadrupole magnetic fields.


Research supported by NSF/DOE Partnership in Basic Plasma Science and Engineering through the NSF Award Number 2107709 and the NSF Graduate Research Fellowship Program under Grant No. 1842494.




</EventDescription>
    <EventParentName>73284-SESS</EventParentName>
    <EventUniqueID>73284-118465</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>An Introduction to SambaNova</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS88</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  3:05:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:25:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>801</EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Jennifer Glore</EventSpeakers>
    <EventSpeakerIDs>803878</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118784</EventHandoutURL>
    <EventDescription xsi:nil="true" />
    <EventParentName>73327-SESS</EventParentName>
    <EventUniqueID>73327-118784</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>CP5 Graph Algorithms and Applications</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP5</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:35:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>744636</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>804298,804316,744636,771753</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73826</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73826-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>CP6 Asynchronous Algorithms, Load Balancing</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP6</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  5:00:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>790204</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>790204,796717,803465,804335</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73761</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73761-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>CP7 Sparse Linear Algebra and Applications</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP7</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  5:00:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>805413</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>798545,786124,803588,788597</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73822</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73822-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>CP8 Numerical Methods for PDEs</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP8</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 12:50:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>796147</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>790097,796671,796147,732280</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73762</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73762-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Parallel Variational Mesh Quality Improvement Method for Tetrahedral Meshes Based on the MMPDE Method</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP0</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  6:00:00 PM</EventStartTime>
    <EventEndTime>02/24/22  8:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Suzanne Shontz</EventSpeakers>
    <EventSpeakerIDs>774784</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118761</EventHandoutURL>
    <EventDescription>There are numerous large-scale applications requiring mesh adaptivity, e.g., cardiac electrophysiology, computational fluid dynamics, fracture propagation, and weather prediction. Parallel processing is needed for simulations involving largescale adaptive meshes. In this paper, we propose a parallel variational mesh quality improvement algorithm for use with distributed memory machines. Our parallel method is based on the sequential method by Huang, Ren, and Russell and the recent implementation by Huang and Kamenski. Their approach is based on the use of the Moving Mesh PDE method to adapt the mesh based on the minimization of an energy functional for mesh equidistribution and alignment. This leads to a system of ordinary differential equations (ODEs) to be solved which determine where to move the interior mesh nodes. The MMPDE method successfully removes/reduces the number of extreme dihedral angles, particularly those less than $20^{o}$ or greater than $150^{o}$. An efficient solution is obtained by solving the ODEs on subregions of the mesh with overlapped communication and computation. Strong and weak scaling experiments on up to 128 cores for tetrahedral meshes with up to 160M elements demonstrate excellent results.</EventDescription>
    <EventParentName>73765-SESS</EventParentName>
    <EventUniqueID>73765-118761</EventUniqueID>
    <STATUS>inactive</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>A Multiresolution Discrete Element Method for Triangulated Objects with Implicit Timestepping</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP0</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  6:00:00 PM</EventStartTime>
    <EventEndTime>02/24/22  8:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Peter Noble</EventSpeakers>
    <EventSpeakerIDs>791858</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=118812</EventHandoutURL>
    <EventDescription>Simulations of many rigid bodies colliding with each other sometimes yield particularly interesting results if the colliding objects differ significantly in size and are non-spherical.  The most expensive part within such a simulation code is the collision detection.  We propose a family of novel multiscale collision detection algorithms that can be applied to triangulated objects within explicit and implicit time stepping methods.  They are well-suited to handle objects that cannot be represented by analytical shapes or assemblies of analytical objects.  Inspired by multigrid methods and adaptive mesh refinement, we determine collision points iteratively over a resolution hierarchy, and combine a functional minimisation plus penalty parameters with the actual comparison-based geometric  distance  calculation.   Coarse  surrogate  geometry  representations  identify  “no  collision” scenarios early on and otherwise yield an educated guess which triangle subsets of the next finer level potentially yield collisions.  They prune the search tree, and furthermore feed conservative contact force estimates into the iterative solve behind an implicit time stepping.  Implicit time stepping and non-analytical  shapes  often  yield  prohibitive  high  compute  cost  for  rigid  body  simulations.   Our approach reduces these cost algorithmically by one to two orders of magnitude.  It also exhibits high vectorisation efficiency due to its iterative nature.
</EventDescription>
    <EventParentName>73765-SESS</EventParentName>
    <EventUniqueID>73765-118812</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Sundials: Suite of Nonlinear and Differential/Algebraic Equation Solvers</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP0</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  6:00:00 PM</EventStartTime>
    <EventEndTime>02/24/22  8:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs></EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers>Cody Balos</EventSpeakers>
    <EventSpeakerIDs>784882</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>Abstract</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_talk.cfm?p=119104</EventHandoutURL>
    <EventDescription>SUNDIALS is a suite of robust and scalable integrators and solvers for systems of ordinary differential equations, differential-algebraic equations, and nonlinear equations. The suite consists of six independent packages, CVODE(S), ARKODE, IDA(S), and KINSOL, and is designed to be easily incorporated into existing simulation codes with minimal information from the user. In this poster, we overview the capabilities of the SUNDIALS suite, discuss recent results from applications codes ranging from additive manufacturing to combustion and fusion energy, and highlight efforts to add performance instrumentation and measurement capabilities as well as automated performance regression testing.
 
This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. 
</EventDescription>
    <EventParentName>73765-SESS</EventParentName>
    <EventUniqueID>73765-119104</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Break</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  2:40:00 PM</EventStartTime>
    <EventEndTime>02/23/22  3:10:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs xsi:nil="true" />
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73766-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T14:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Break</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  4:50:00 PM</EventStartTime>
    <EventEndTime>02/23/22  5:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs xsi:nil="true" />
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73776-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T16:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Break</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  2:50:00 PM</EventStartTime>
    <EventEndTime>02/24/22  3:20:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs xsi:nil="true" />
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73768-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T14:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Break</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  5:00:00 PM</EventStartTime>
    <EventEndTime>02/24/22  5:15:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs xsi:nil="true" />
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73780-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T17:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Break</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  9:15:00 AM</EventStartTime>
    <EventEndTime>02/24/22  9:25:00 AM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs xsi:nil="true" />
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73779-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T09:15:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Break</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:25:00 AM</EventStartTime>
    <EventEndTime>02/24/22 10:55:00 AM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs xsi:nil="true" />
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73767-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:25:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Break</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 12:35:00 PM</EventStartTime>
    <EventEndTime>02/24/22  2:05:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs xsi:nil="true" />
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73773-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T12:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Break</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:05:00 PM</EventStartTime>
    <EventEndTime>02/25/22  3:35:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs xsi:nil="true" />
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73770-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:05:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Break</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  9:15:00 AM</EventStartTime>
    <EventEndTime>02/25/22  9:25:00 AM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs xsi:nil="true" />
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73781-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T09:15:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Break</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/25/22 11:10:00 AM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs xsi:nil="true" />
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73769-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Break</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 12:50:00 PM</EventStartTime>
    <EventEndTime>02/25/22  2:20:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs xsi:nil="true" />
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73774-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T12:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Break</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  9:15:00 AM</EventStartTime>
    <EventEndTime>02/26/22  9:25:00 AM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs xsi:nil="true" />
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73782-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T09:15:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Break</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:10:00 AM</EventStartTime>
    <EventEndTime>02/26/22 10:40:00 AM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs xsi:nil="true" />
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73771-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Break</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 12:20:00 PM</EventStartTime>
    <EventEndTime>02/26/22  1:50:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs xsi:nil="true" />
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73775-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T12:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Closing Remarks</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  3:30:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:45:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>798097,89317</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>74293-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>0</AutoRecord>
    <EnabledForIAV>0</EnabledForIAV>
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T15:30:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>CP1 Applications - Part I of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP1</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:40:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>771959</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>786005,802600,796148,771959,795435</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73758</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73758-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>CP10 Geoscience Applications</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP10</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:15:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>790102</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>747448,790102,780578,781837</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73763</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73763-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>CP11 Applications - Part III of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP11</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>805413</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803504,796853,704793</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73764</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73764-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>CP2 Applications - Part II of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP2</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:50:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>803814</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>796761,803863,803814,758470</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73759</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73759-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>CP3 Applications of Machine Learning and Algorithm Performance (This session includes proceedings papers)</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP3</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:35:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>795697</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>795697,805314,805315,795361</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73828</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73828-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>CP4 Low Rank Techniques</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CP4</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:35:00 PM</EventEndTime>
    <EventFilter>PP22|Contributed|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>795709</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>792440,769524,795709,804281</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73760</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73760-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>IP4 A Quantum Future of Computation</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP4</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  9:55:00 AM</EventStartTime>
    <EventEndTime>02/25/22 10:40:00 AM</EventEndTime>
    <EventFilter>PP22|Invited Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>726311</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>802637</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73442</EventHandoutURL>
    <EventDescription>Still in early development, quantum computing is already overturning our contemporary notions of computational methods and devices. Using new concepts of computing based in quantum physics,  quantum computers will be able to solve certain problems that are completely intractable on any imaginable classical computer, such as accurate simulations of molecules and materials, or breaking public key encryption. While this potential is real, quantum computers are best viewed as special purpose accelerators for specific problem classes.
In an effort to bring clarity to the fast-growing field of quantum computing, I will describe the hardware and software architecture of quantum computers and discuss how they differ from conventional classical high performance computers. Based on this, I will also attempt to dispel myths and hype surrounding the field and present a realistic assessment of the potential of these devices and the specific application areas on which they are expected to have a large impact. I will end by showing that quantum computing already generates value today, through quantum inspired approaches. These are quantum approaches implemented on classical hardware that outperform the state of the art of classical methods known before, with applications  in health care, logistics, chemistry and other areas.
</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73442-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>1</DirectLinkEnabled>
    <DirectLinkRecordingEnabled>1</DirectLinkRecordingEnabled>
    <start>2022-02-25T09:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>IP5 </EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP5</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  2:20:00 PM</EventStartTime>
    <EventEndTime>02/25/22  3:05:00 PM</EventEndTime>
    <EventFilter>PP22|Invited Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>720359</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>726876</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73439</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73439-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>1</DirectLinkEnabled>
    <DirectLinkRecordingEnabled>1</DirectLinkRecordingEnabled>
    <start>2022-02-25T14:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>IP6 </EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP6</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  8:30:00 AM</EventStartTime>
    <EventEndTime>02/26/22  9:15:00 AM</EventEndTime>
    <EventFilter>PP22|Invited Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>805345</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>804309</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73434</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73434-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>1</DirectLinkEnabled>
    <DirectLinkRecordingEnabled>1</DirectLinkRecordingEnabled>
    <start>2022-02-26T08:30:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>IP7 Graph Neural Network Research at AWS AI</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>IP7</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  9:25:00 AM</EventStartTime>
    <EventEndTime>02/26/22 10:10:00 AM</EventEndTime>
    <EventFilter>PP22|Invited Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>735416</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>91119</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73438</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73438-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>1</DirectLinkEnabled>
    <DirectLinkRecordingEnabled>1</DirectLinkRecordingEnabled>
    <start>2022-02-26T09:25:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS1 Architecture and Performance of Hardware-Independent Frameworks for Particle-In-Cell Methods - Part I of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS1</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs>712759</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>804163,712759,772169,731156</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73409</EventHandoutURL>
    <EventDescription>Heterogeneous computing architectures are unavoidable moving towards the era of exascale computing. Computing nodes are being built with the ever-increasing depth of the hierarchy. Hardware as well as performance portability are key capabilities to make efficient use of them. Particle-in-cell (PIC) methods are the method of choice in computational simulations of many physical applications including but not limited to particle accelerators, nuclear fusion, and astrophysics. Hence, portability in the context of PIC schemes is the need of the hour to carry out these extreme-scale simulations in current and next-generation architectures. This minisymposium will serve as a platform to discuss the architecture and performance of hardware-independent and, thus, portable frameworks regarding particle and grid computations.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73409-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS10 Approximate Linear Algebra in Numerical Optimization Methods</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS10</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs>767223</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>797065,751676,790068,790109</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73351</EventHandoutURL>
    <EventDescription>Approximate numerical linear algebra is central to the design of efficient numerical optimization algorithms. Approximation techniques such as sketching, iterative solvers, and preconditioning can be devised for general numerical optimization schemes such as the interior point method as well as adapted to specific applications. This symposium presents recent advances in using such techniques, with applications including modelling of dynamical systems by numerical PDEs and quantum states by tensor networks. A commonality among the new methods is that they leverage matrix structure that is specific to the numerical optimization method problem (e.g., block structure of KKT systems or tensor product structure in tensor networks) to design suitable sketches or preconditioners. These new methods enable scalability to larger problem sizes with less cost and pose new challenges for parallelization.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73351-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS11 Versatile Solvers on GPU-Based High-Performance Computing Architectures - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS11</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs>797436,796171,732925,719838,752016</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>735429,803819,796365,795393</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73362</EventHandoutURL>
    <EventDescription>As a power-efficient compute device, graphics processing units (GPUs) have been the coprocessors of choice in the high-performance computing community; they have been selected to power the first generation of exascale supercomputers in the United States including Aurora, El Capitan, and Frontier. While GPUs have displaced CPUs as the primary provider of arithmetic and memory bandwidth, GPUs are not as versatile as CPUs. This presents a number of challenges for scientific algorithms and software designed for CPU-based granularity and persistent memory locality. The throughput-optimized architectures have higher intrinsic latencies and an inverted cache hierarchy with different coherence semantics. This requires decomposing algorithms differently and still increases baseline latency to solve problems under strong scaling. Also it is a challenge to choose from different vectorization strategies and programming models. This minisymposium will bring together researchers working on adapting and optimizing existing solvers for GPUs as well as developing novel approaches that not only deliver great performance but also the versatility to accelerate a wide range of applications beyond AI.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73362-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS12 Architecture and Performance of Hardware-Independent Frameworks for Particle-In-Cell Methods - Part II of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS12</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs>712759</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>790276,804164,790274,804166</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73410</EventHandoutURL>
    <EventDescription>Heterogeneous computing architectures are unavoidable moving towards the era of exascale computing. Computing nodes are being built with the ever-increasing depth of the hierarchy. Hardware as well as performance portability are key capabilities to make efficient use of them. Particle-in-cell (PIC) methods are the method of choice in computational simulations of many physical applications including but not limited to particle accelerators, nuclear fusion, and astrophysics. Hence, portability in the context of PIC schemes is the need of the hour to carry out these extreme-scale simulations in current and next-generation architectures. This minisymposium will serve as a platform to discuss the architecture and performance of hardware-independent and, thus, portable frameworks regarding particle and grid computations.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73410-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS13 Versatile Solvers on GPU-Based High-Performance Computing Architectures - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS13</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs>797436,796171,732925,719838,752016</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>796171,803820,792288,803821</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73363</EventHandoutURL>
    <EventDescription>As a power-efficient compute device, graphics processing units (GPUs) have been the coprocessors of choice in the high-performance computing community; they have been selected to power the first generation of exascale supercomputers in the United States including Aurora, El Capitan, and Frontier. While GPUs have displaced CPUs as the primary provider of arithmetic and memory bandwidth, GPUs are not as versatile as CPUs. This presents a number of challenges for scientific algorithms and software designed for CPU-based granularity and persistent memory locality. The throughput-optimized architectures have higher intrinsic latencies and an inverted cache hierarchy with different coherence semantics. This requires decomposing algorithms differently and still increases baseline latency to solve problems under strong scaling. Also it is a challenge to choose from different vectorization strategies and programming models. This minisymposium will bring together researchers working on adapting and optimizing existing solvers for GPUs as well as developing novel approaches that not only deliver great performance but also the versatility to accelerate a wide range of applications beyond AI.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73363-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS14 Toward Scalable Resilient and Fault Tolerant Applications for Extreme Scale Computing Systems - Part I of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS14</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs>795281,747244,712668,89317</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>730729,740836,753207,803858</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73369</EventHandoutURL>
    <EventDescription>Supercomputers in the exascale era are becoming more complex, with larger scale machines and heterogenous architectures. As these machines become more complex, the mean time between failure (MTBF) decreases. New methods are needed to adapt to the increased failure rate, including error analysis, innovations in system architectures and compilers, novel programming models, and algorithm-based fault tolerance. We present a series of talks representing the diversity of this topic in three sessions. Session 1 will cover algorithm-based fault tolerance (ABFT). Session 2 will cover software approaches to resilience and fault tolerance. Finally, session 3 will cover system and compiler advances for mitigating faults, along with analysis of resilience issues.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73369-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS15 Progress on Portable Math Libraries for Modern Computer Architectures - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS15</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs>780431</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>796491,760022,783179,780888</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73357</EventHandoutURL>
    <EventDescription>High performance computing is currently navigating changes in computer architectures with a variety of new accelerators and central processing units (CPU) becoming available at major computing facilities. New runtime models and math libraries are being actively developed to provide performance portable layers to applications and high level libraries thus insulating them to some of the changes necessary to obtain good performance on modern architectures. This minisymposium gathers math library developers and their low level users to discuss new techniques and algorithms enabling good performance on multiple architectures (these techniques might include hierarchical parallelism, memory management, communication, etc...) and return on experience and tips on using the currently available libraries on modern hardware.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73357-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS17 Performance Modeling, Code Generation, and Machine Learning for Autotuning - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS17</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs>739657</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>740185,791468,790117</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73329</EventHandoutURL>
    <EventDescription>Autotuning has been an established technique for performance engineering
of applications with the assistance of performance models. This allows
the implementers to overcome the issues of selecting hyperparameters to
match the modern and emerging hardware with complex application codes.
This session will review optimization space search and ML techniques
employed in the exploration of performance space and the results of
improvements in runtime. We gather together practitioners of the
autotuning that target a variety of methods, software stacks, and
hardware platforms.

Some of the topics of interest include:

- performance search space exploration and exhaustive search issues

- surrogate models and approximations of performance metrics

- hyperparameter space representation techniques

- code generation methods for efficient HPC code

- strategies for exhaustive search overhead mitigation</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73329-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS18 Performance-Driven Dynamic Resource Management: New Analytics and Architectural Designs - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS18</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs>735577,758179,802658</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>802658,761858,716127,803729</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72963</EventHandoutURL>
    <EventDescription>Urgency in HPC for Dynamic Resource Management is being driven by increasing variability in resource demands as a result of complex workflows, live instrument analytics, and new application models incorporating AI. Heterogeneous platforms may better support variable demands, but the ability to continuously assess applications’ demands and system conditions and to use those to dynamically match applications to resources simply does not exist. Progress has been made in developing application resource utilization profiles and understanding impacts of system conditions, however, little attention has been paid to the mathematics necessary to determine appropriate responses nor to the architectures necessary to enable them. Acceptance of automated reconfiguration will require profiling, contention and root cause analyses to provide confidence measures. Determination of effective responses relies on the assessment of headrooms of components given their capabilities and determination of complimentary sharing of resources without contention. Analytics that capture evolution of, overcorrection in, and instabilities of response actions are needed to evaluate the effectiveness of response. Concurrent development of hardware, application, and system software options for response can drive analytics and provide data for evaluation. We bring together performance analytics and architecture practitioners to discuss codesign of capabilities to enable Dynamic Resource Management.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72963-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS19 Graph Algorithms in the Exascale Era - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS19</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs>795247,739547</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803305,796286,803306,785429</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73138</EventHandoutURL>
    <EventDescription>Graph Analytics in general and graph algorithms in particular play a
central role in scientific applications. The unique traits of their runtime
behavior with irregular memory access patters and fine grained
synchronization make them particularly challenging to effectively implement
and scale on parallel machines. Several parallel algorithms have been
designed in recent years to address these challenges for the upcoming
generation of exascale systems. In this two parts minisymposium, we will
have two set of talks. One will present recent progress on parallel
algorithms and implementations, while the other will describe novel
approaches and applications that offer new opportunities for
parallelization.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73138-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS2 Progress and Challenges in Extreme Scale Computing and Big Data</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS2</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs>789960,706361,715772</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>706361,715772,789960,768136</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72849</EventHandoutURL>
    <EventDescription>Extreme scale computing efforts have resulted in numerous advances for multicore and accelerator based scalable systems. In addition, large-scale applications must increasingly deal with data management and analysis as a first-class concern. Therefore, new applications often have to manage distributed and parallel computing, and have to manage workflows of different tasks (computing, data analytics, data migrations, machine learning, visualization,…).  
In this minisymposium we present some of the latest work in scalable algorithms, programming paradigms, and libraries for next generation computing platforms. Furthermore, we discuss efforts to better incorporate data science concerns as an important component of our scientific workflows.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72849-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS20 Challenges in Parallel Adaptive Mesh Refinement - Part I of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS20</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs>730729,723667,712891</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>793413,803351,803788,803785</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73162</EventHandoutURL>
    <EventDescription>Parallel adaptive mesh refinement (AMR) is a key technique when simulations are required to capture time-dependent and/or multiscale features. Frequent re-adaptation and repartitioning of the mesh during the simulation can impose significant overhead, particularly in largescale parallel environments. But also on static meshes, variable work load may force reactive load balancing. Further challenges arise on accelerated or special-purpose hardware, and the trend toward hierarchical and hybrid compute architectures. Our minisymposium addresses algorithms, scalability, and software issues of parallel AMR.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73162-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS21 Research Challenges and Opportunities within Software Productivity, Sustainability, and Reproducibility - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS21</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs>34694,27158,705457</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>779008,803119,728876,783663</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73127</EventHandoutURL>
    <EventDescription>From predicting the folding of proteins to climate models that integrate observations, new types of scientific workflows are combining high-performance computing (HPC) simulation, data analysis and machine learning.  The integration of these different modes of computing is creating rich opportunities for the development of innovative scientific methods and offering new research challenges in software productivity, sustainability and reproducibility, targeting diverse hardware such as exascale machines and cloud computing environments.  
 
Productivity can be advanced by enabling rapid development of new methods and applications that achieve high performance while remaining portable and easy to change and support, through high-level languages, well designed libraries and frameworks, and advanced development environments.  Reproducibility provides the confidence of obtaining the same or similar results when running workflows in different or changing hardware and software environments, or by effectively managing the stochastic elements of complex workflows.  Overall, we want both productivity and reproducibility while achieving high performance on scalable systems with accelerators and multi-level memories.  
 
This minisymposium will highlight progress toward these goals with a diverse group of speakers, who are working on relevant aspects of the overall problem of combining productivity, reproducibility and high performance for various scientific computing problems.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73127-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS22 Statistical Methods for Uncertainty Quantification and Parallel Computing - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS22</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5000</EventKeywords>
    <EventChairs>799823,790440</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803307,799823,796515,803797</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73104</EventHandoutURL>
    <EventDescription>In many engineering and science disciplines such as climate modelling, physics, energy, life sciences, finance, geosciences and healthcare, computer models help to understand the properties and behaviors of large and complex processes. However, computer models are imperfect representations of the reality and they almost always include uncertain inputs and rely on approximations. Uncertainty quantification aims to systematically account for relevant uncertainties associated with models, inputs, and experiments for more accurate and precise predictions with well-quantified interpretable uncertainties. The computational effort associated with uncertainty quantification of complex systems has been one of the major challenges over the years. Parallel computing is a key technology to make computationally demanding uncertainty quantification problems tractable. In this minisymposium, efficient algorithms in the areas such as parameter selection, surrogate model construction, and sensitivity analysis and computational frameworks that exploit parallel computing architectures will be presented.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73104-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS23 Approximate Computing for Scientific Applications: The Why and The How - Part I of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS23</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs>780186,790290</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803063,790132,780186,803336</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73075</EventHandoutURL>
    <EventDescription>Approximate Computing brings together various aspects of high performance computing, be it the leading paradigm of minimizing complexities in scientific computing, the evermore growing demand for handling larger datasets with the widening gap of the costs for a single floating point operation and transferring the result, and the hardware design with adaptive precision for each single computation. Only with the combination of all these aspects will we be able to overcome the challenges in achieving exascale performance (and beyond), while limiting power consumption.

We therefore bring together experts from the different fields of approximate computing to present the current state and to inspire future projects.

Topics are:
- mixed/adaptive precisions
- low-rank approximations
- hardware support for approximate computing
- programming models
- floating-point number representations (not IEEE 754 compliant)
- HPC applications</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73075-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS24 Scalable Solvers for Large-Scale Multiphysics Problems - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS24</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  3:10:00 PM</EventStartTime>
    <EventEndTime>02/23/22  4:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs>795503,706380,795507</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>712687,785798,720359,735672</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72942</EventHandoutURL>
    <EventDescription>Many applications, such as those arising from geomechanics and aerodynamics, are modeled by increasingly sophisticated and computationally intense numerical models. These numerical models often require the solution to coupled systems of partial differential equations. The development and implementation of efficient and scalable solutions to large-scale multiphysics systems is becoming more and more important as applications become increasingly large and complex. The purpose of this session is to bring together researchers working on scalable solvers for large-scale multiphysics problems. The focus is on implementations of efficient algorithms and software frameworks for modern and next-gen architectures.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72942-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T15:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS25 Architecture and Performance of Hardware-Independent Frameworks for Particle-In-Cell Methods - Part III of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS25</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs>712759</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>804167,750670,804168,785706</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73411</EventHandoutURL>
    <EventDescription>Heterogeneous computing architectures are unavoidable moving towards the era of exascale computing. Computing nodes are being built with the ever-increasing depth of the hierarchy. Hardware as well as performance portability are key capabilities to make efficient use of them. Particle-in-cell (PIC) methods are the method of choice in computational simulations of many physical applications including but not limited to particle accelerators, nuclear fusion, and astrophysics. Hence, portability in the context of PIC schemes is the need of the hour to carry out these extreme-scale simulations in current and next-generation architectures. This minisymposium will serve as a platform to discuss the architecture and performance of hardware-independent and, thus, portable frameworks regarding particle and grid computations.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73411-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS26 Approximate Computing for Scientific Applications: The Why and The How - Part II of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS26</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs>780186,790290</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>746995,794956,775808,734193</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73076</EventHandoutURL>
    <EventDescription>Approximate Computing brings together various aspects of high performance computing, be it the leading paradigm of minimizing complexities in scientific computing, the evermore growing demand for handling larger datasets with the widening gap of the costs for a single floating point operation and transferring the result, and the hardware design with adaptive precision for each single computation. Only with the combination of all these aspects will we be able to overcome the challenges in achieving exascale performance (and beyond), while limiting power consumption.

We therefore bring together experts from the different fields of approximate computing to present the current state and to inspire future projects.

Topics are:
- mixed/adaptive precisions
- low-rank approximations
- hardware support for approximate computing
- programming models
- floating-point number representations (not IEEE 754 compliant)
- HPC applications</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73076-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS27 High Performance Statistical Computing (HPSC): Challenges and Best Practices - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS27</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1000</EventKeywords>
    <EventChairs>794833,53411</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>794833,803155,803156,769018</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73114</EventHandoutURL>
    <EventDescription>Big data in terms of volume, intensity, and complexity is one of the main challenges to statisticians. The availability of different data sources has essential implications in collecting colossal data volumes that require other ways to be managed and analyzed. Existing analytical tools cannot be applied easily to big data volumes due to memory and computation constraints. Previously, statistical applications and traditional high performance oriented computing have followed independent paths. However, important opportunities now arise that can be addressed by merging the two. As a prominent big data application,  statistics is increasingly performance-bound in different fields. HPC is becoming increasingly significant in scaling existing statistical methods to larger and more complex applications and developing novel methods that are amenable to scaling within the constraints that exist in modern HPC architectures. This minisymposium aims to show the existing efforts to harness the HPC capabilities in different statistics branches to serve large-scale statistics. It also aims to cover the current challenges and opportunities towards exploiting current HPC technologies in accelerating applications related to applied statistics.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73114-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS28 Challenges in Parallel Adaptive Mesh Refinement - Part II of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS28</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs>730729,723667,712891</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>805565,803350,796058,790206</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73163</EventHandoutURL>
    <EventDescription>Parallel adaptive mesh refinement (AMR) is a key technique when simulations are required to capture time-dependent and/or multiscale features. Frequent re-adaptation and repartitioning of the mesh during the simulation can impose significant overhead, particularly in largescale parallel environments. But also on static meshes, variable work load may force reactive load balancing. Further challenges arise on accelerated or special-purpose hardware, and the trend toward hierarchical and hybrid compute architectures. Our minisymposium addresses algorithms, scalability, and software issues of parallel AMR.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73163-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS29 Understanding and Exploiting Mixed-Precision Accelerators for High-Performance Computing - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS29</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs>789947,765346</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>4846,790350,789947,758295</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72751</EventHandoutURL>
    <EventDescription>The growth of domain-specific hardware devices, such as low- and mixed-precision Matrix-Multiply Accumulate (MMA) accelerators (for example Tensor Processing Units and Tensor Cores), motivates several strands of research in scientific computing. First, algorithm designers aim to benefit from the speedup these hardware devices make possible by adapting algorithms, or parts of them, to run in low or mixed precisions. Second, we need to understand the low level details of how the devices implement floating-point arithmetic and to what extent they satisfy floating-point arithmetic standards. Third, new rounding error analysis is being developed to further support the task of finding the best ways to use the accelerators in order to maximize the accuracy of the results.  This minisymposium gathers researchers in scientific computing, numerical analysis, and the standardization and testing of floating-point arithmetics to report the latest research on applying and understanding the MMA hardware.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72751-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS3 Scalable Solvers for Large-Scale Multiphysics Problems - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS3</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs>795503,706380,795507</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>790151,803129,795507,723261</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72941</EventHandoutURL>
    <EventDescription>Many applications, such as those arising from geomechanics and aerodynamics, are modeled by increasingly sophisticated and computationally intense numerical models. These numerical models often require the solution to coupled systems of partial differential equations. The development and implementation of efficient and scalable solutions to large-scale multiphysics systems is becoming more and more important as applications become increasingly large and complex. The purpose of this session is to bring together researchers working on scalable solvers for large-scale multiphysics problems. The focus is on implementations of efficient algorithms and software frameworks for modern and next-gen architectures.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72941-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS30 Domain Decomposition Methods – Parallel Algorithms, Software, and Machine Learning  - Part III of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS30</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs>720131,803849,774576,763792,790005</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>768190,803849,790005,763792</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73348</EventHandoutURL>
    <EventDescription>Domain decomposition methods are parallel scalable, preconditioned iterative methods for the efficient solution of problems arising from the discretization of partial differential equations. In this minisymposium, we focus on several important aspects of domain decomposition methods: Coarse spaces, nonlinear preconditioning, high-performance computing, software development, and scientific machine learning.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73348-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS31 Progress on Portable Math Libraries for Modern Computer Architectures - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS31</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs>780431</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>763842,769510,770972,755869</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73358</EventHandoutURL>
    <EventDescription>High performance computing is currently navigating changes in computer architectures with a variety of new accelerators and central processing units (CPU) becoming available at major computing facilities. New runtime models and math libraries are being actively developed to provide performance portable layers to applications and high level libraries thus insulating them to some of the changes necessary to obtain good performance on modern architectures. This minisymposium gathers math library developers and their low level users to discuss new techniques and algorithms enabling good performance on multiple architectures (these techniques might include hierarchical parallelism, memory management, communication, etc...) and return on experience and tips on using the currently available libraries on modern hardware.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73358-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS32 Towards Exascale Eigenvalue Algorithms for Physical Simulation - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS32</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs>760440,785504,701749</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>785504,762885,751672,752091</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73360</EventHandoutURL>
    <EventDescription>Large scale eigenvalue computations are ubiquitous throughout scientific computation. However, the steep $O(N^3)$ scaling of traditional eigenvalue algorithms often lead to the solution of a particular eigenvalue problem becoming the computational bottleneck in many simulations of physical systems. Over the years, many important algorithmic developments have been made to allow leverage of the latest advances in massively parallel computing architectures such as GPU accelerators to enable the simulation of large physical systems on the world's largest supercomputers. In this minisymposium, we examine several recent advances in parallel eigenvalue algorithms for eigenvalue problems which arise in physical simulations.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73360-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS33 Toward Scalable Resilient and Fault Tolerant Applications for Extreme Scale Computing Systems - Part II of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS33</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs>795281,747244,712668,89317</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>795518,803078,790016,780470</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73370</EventHandoutURL>
    <EventDescription>Supercomputers in the exascale era are becoming more complex, with larger scale machines and heterogenous architectures. As these machines become more complex, the mean time between failure (MTBF) decreases. New methods are needed to adapt to the increased failure rate, including error analysis, innovations in system architectures and compilers, novel programming models, and algorithm-based fault tolerance. We present a series of talks representing the diversity of this topic in three sessions. Session 1 will cover algorithm-based fault tolerance (ABFT). Session 2 will cover software approaches to resilience and fault tolerance. Finally, session 3 will cover system and compiler advances for mitigating faults, along with analysis of resilience issues.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73370-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS34 Achieving Productivity at Scale with Chapel in User Applications</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS34</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>2900</EventKeywords>
    <EventChairs>803828</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803828,803830,803831,803832</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73366</EventHandoutURL>
    <EventDescription>Chapel is a parallel programming language designed to simplify the programmability, portability, and scalability of HPC applications, from traditional scientific simulations to modern data science, relative to conventional approaches.  Chapel provides first-class language features for parallelism and locality that simplify writing code that scales from laptops to supercomputers. This minisymposium focuses on real-world applications written in Chapel, to discuss their benefits in terms of scalability, performance, and time-to-science. We'll start with an introductory talk to introduce Chapel to those unfamiliar with it, and then discuss current status and future directions. The three applications that will be highlighted are (1) ChplUltra for simulating ultralight dark matter, (2) Arkouda for performing NumPY-like operations on supercomputers, and (3) CHAMPS for computational fluid dynamics simulations.  Our goal with this minisymposium is to illustrate to other HPC scientific computing groups significant applications written in Chapel and discuss the programmability and performance of such applications.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73366-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS35 Approaches and Challenges for Attaining Performance in the Exascale Era</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS35</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22 10:55:00 AM</EventStartTime>
    <EventEndTime>02/24/22 12:35:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs>747426,75645,725824</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>747426,751636,802778,798224</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73330</EventHandoutURL>
    <EventDescription>The architectures of the existing top performing systems are undeniable complex, building upon multi-core units and proprietary interconnects, with very high levels of parallelism. These features pose many challenges to numerical library and application developers, and this scenario is not expected to become any more favorable as architectures evolve.

Automatic tuning (AT) has been demonstrated to be an effective way for producing high performance implementations of a number of computations and for the selection of optimal configuration parameters. In addition, algorithms that did not perform well on past architectures may find a resurgence if they provide, for example, significant communications savings. Pursuing increased performance, however, cannot be dissociated from concerns about the accuracy of numerical computations, which can be an issue for conventional (e.g., BLAS) or complex algorithms (e.g., eigensolvers) and thus to large-scale applications that depend on those computations. 

The presentations in this minisymposium will discuss recent work on AT (through ML techniques), novel approaches for accuracy verification, and iterative eigensolvers that do not enforcing orthogonality on the iterates thus reducing communication.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73330-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T10:55:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS83 Next Generation FFT Algorithms in Theory and Practice: Parallel Implementations and Applications</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS83</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs>83250,795282,747261</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>83250,795282,747261,798421</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72986</EventHandoutURL>
    <EventDescription>The fast Fourier Transform (FFT) is an algorithm used in a wide variety of applications, yet does not make optimal use of many current hardware platforms. Hardware utilization performance on its own does not however imply optimal problem solving. The purpose of this minisymposium is to enable exchange of information between people working on alternative FFT algorithms, to those working on FFT implementations, in particular for parallel hardware.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72986-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS36 The Future of Cardiac Electrophysiology Simulation</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS36</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  5:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs>47064</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>47064,790232,790143,730056</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73365</EventHandoutURL>
    <EventDescription>Cardiovascular disease remains the leading cause of death
worldwide. Many cardiovascular diseases correlate with abnormal
electrical activity in the heart. Computational modelling and
simulation of cardiac electrophysiology are a non-invasive means to
better diagnose and treat cardiac pathologies. The COVID-19 pandemic
has further underscored the importance of simulation in telemedicine
to enhance remote interactions between patients and doctors.

Arguably the most reliable model for cardiac electrophysiology
simulation is the bidomain model, which is based on a homogenization
of myocardial tissue. The human heart consists of approximately two
billion muscle cells. It is still generally infeasible to model
individual cells. Unfortunately, the bidomain model obscurs the cell
membrane dynamics and consequently the possibility of studying
membrane channelopathies that contribute to cardiac pathologies such
as long QT syndrome.

The recently proposed extracellular-membrane-intracellular (EMI) model
resolves the detailed characteristics of ion channels on individual
cell membranes, thus making it highly suitable for studying
channelopathies.

The EMI model resolves tissue-scale dynamics at about four orders of
magnitude higher than the bidomain model. Accordingly, we are still
quite far away from real-time cardiac simulations.

This minisymposium provides a glimpse at the current challenges and
future directions in cardiac electrophysiology simulation.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73365-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS37 Towards Exascale Eigenvalue Algorithms for Physical Simulation - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS37</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  5:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs>760440,785504,701749</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>31242,796647,727519,760440</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73361</EventHandoutURL>
    <EventDescription>Large scale eigenvalue computations are ubiquitous throughout scientific computation. However, the steep $O(N^3)$ scaling of traditional eigenvalue algorithms often lead to the solution of a particular eigenvalue problem becoming the computational bottleneck in many simulations of physical systems. Over the years, many important algorithmic developments have been made to allow leverage of the latest advances in massively parallel computing architectures such as GPU accelerators to enable the simulation of large physical systems on the world's largest supercomputers. In this minisymposium, we examine several recent advances in parallel eigenvalue algorithms for eigenvalue problems which arise in physical simulations.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73361-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS38 Toward Scalable Resilient and Fault Tolerant Applications for Extreme Scale Computing Systems - Part III of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS38</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  5:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs>795281,747244,712668,89317</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803859,712668,786170,803860</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73371</EventHandoutURL>
    <EventDescription>Supercomputers in the exascale era are becoming more complex, with larger scale machines and heterogenous architectures. As these machines become more complex, the mean time between failure (MTBF) decreases. New methods are needed to adapt to the increased failure rate, including error analysis, innovations in system architectures and compilers, novel programming models, and algorithm-based fault tolerance. We present a series of talks representing the diversity of this topic in three sessions. Session 1 will cover algorithm-based fault tolerance (ABFT). Session 2 will cover software approaches to resilience and fault tolerance. Finally, session 3 will cover system and compiler advances for mitigating faults, along with analysis of resilience issues.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73371-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS39 Challenges in Parallel Adaptive Mesh Refinement - Part III of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS39</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  5:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs>730729,723667,712891</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>804302,752526,712891,796066</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73164</EventHandoutURL>
    <EventDescription>Parallel adaptive mesh refinement (AMR) is a key technique when simulations are required to capture time-dependent and/or multiscale features. Frequent re-adaptation and repartitioning of the mesh during the simulation can impose significant overhead, particularly in large-scale parallel environments. On any given mesh, variable work load may force reactive load balancing. Further challenges arise on accelerated or special-purpose hardware and due to the
trend towards hierarchical and hybrid compute architectures. Our minisymposium addresses algorithms, scalability, and software issues of parallel AMR.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73164-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS4 Statistical Methods for Uncertainty Quantification and Parallel Computing - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS4</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5000</EventKeywords>
    <EventChairs>799823,790440</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803130,748559,765680,791219</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73103</EventHandoutURL>
    <EventDescription>In many engineering and science disciplines such as climate modelling, physics, energy, life sciences, finance, geosciences and healthcare, computer models help to understand the properties and behaviors of large and complex processes. However, computer models are imperfect representations of the reality and they almost always include uncertain inputs and rely on approximations. Uncertainty quantification aims to systematically account for relevant uncertainties associated with models, inputs, and experiments for more accurate and precise predictions with well-quantified interpretable uncertainties. The computational effort associated with uncertainty quantification of complex systems has been one of the major challenges over the years. Parallel computing is a key technology to make computationally demanding uncertainty quantification problems tractable. In this minisymposium, efficient algorithms in the areas such as parameter selection, surrogate model construction, and sensitivity analysis and computational frameworks that exploit parallel computing architectures will be presented.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73103-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS40 High Performance Statistical Computing (HPSC): Challenges and Best Practices - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS40</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  5:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1000</EventKeywords>
    <EventChairs>794833,53411</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>728972,803157,796107,795195</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73115</EventHandoutURL>
    <EventDescription>Big data in terms of volume, intensity, and complexity is one of the main challenges to statisticians. The availability of different data sources has essential implications in collecting colossal data volumes that require other ways to be managed and analyzed. Existing analytical tools cannot be applied easily to big data volumes due to memory and computation constraints. Previously, statistical applications and traditional high performance oriented computing have followed independent paths. However, important opportunities now arise that can be addressed by merging the two. As a prominent big data application,  statistics is increasingly performance-bound in different fields. HPC is becoming increasingly significant in scaling existing statistical methods to larger and more complex applications and developing novel methods that are amenable to scaling within the constraints that exist in modern HPC architectures. This minisymposium aims to show the existing efforts to harness the HPC capabilities in different statistics branches to serve large-scale statistics. It also aims to cover the current challenges and opportunities towards exploiting current HPC technologies in accelerating applications related to applied statistics.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73115-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS41 Approximate Computing for Scientific Applications: The Why and The How - Part III of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS41</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  5:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs>780186,790290</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>763986,795106,796369,739657</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73077</EventHandoutURL>
    <EventDescription>Approximate Computing brings together various aspects of high performance computing, be it the leading paradigm of minimizing complexities in scientific computing, the evermore growing demand for handling larger datasets with the widening gap of the costs for a single floating point operation and transferring the result, and the hardware design with adaptive precision for each single computation. Only with the combination of all these aspects will we be able to overcome the challenges in achieving exascale performance (and beyond), while limiting power consumption.

We therefore bring together experts from the different fields of approximate computing to present the current state and to inspire future projects.

Topics are:
- mixed/adaptive precisions
- low-rank approximations
- hardware support for approximate computing
- programming models
- floating-point number representations (not IEEE 754 compliant)
- HPC applications</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73077-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS42 High-Performance Computing in Optimization - Part I of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS42</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  5:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1200</EventKeywords>
    <EventChairs>770730,728235,775268</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>795067,704448,779499,795574</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73019</EventHandoutURL>
    <EventDescription>Large-scale optimization models are becoming increasingly important in operational research and applications such as energy and engineering systems, industrial engineering, advanced manufacturing, and others. One of the prime application domains is optimal power flow analysis for electric power grids. The computational challenges in this analysis stem from solving multiperiod optimization problems, further exacerbated by considering stochastic scenarios and security analysis. At the national grid level, this leads to massive computational problems. Novel computational methods that can efficiently utilize high performance computing (HPC) are required in order to address the computational complexity of these models. Recent developments in decomposition algorithms, parallel processing, and implementation of software frameworks leveraging linear algebra kernels suitable for deployment at heterogeneous HPC infrastructures make the solution of these complex systems possible. This minisymposium on HPC optimization will host a series of presentations on parallel solution methods and software implementations aiming to foster interdisciplinary collaborations between the optimization, parallel processing, and application domain communities.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73019-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS43 Scalable Data Analytics on the GPU - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS43</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  5:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>500</EventKeywords>
    <EventChairs>780304,712441</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>739547,780304,780566,785016</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72775</EventHandoutURL>
    <EventDescription>Data analytics are an integral part of data science. The introduction of new scalable and high-performance data analytics frameworks, including the use of accelerators such as GPUs,  has enabled data scientists to inspect their data and extract insights previously unattainable. In the last decade, the GPU has trailblazed this new exciting field of high-performance data analytics. This session will cover recent developments that have helped scale data analytics on the GPU to new heights. We will focus on analytics in social network analysis, bioinformatics, and cybersecurity, to name a few. We will also focus on other foundational components of scalable data analytics such as communication, data structures, algorithms, and ETL.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72775-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS44 Testing Numerical Code for Heterogeneity: Lessons Learned and Emerging Projects</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS44</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  5:00:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs>758209,803609,803717,803718,803719,790221</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803609,803614,803617,803616</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72832</EventHandoutURL>
    <EventDescription>As applications in the domains of scientific computing and machine learning continue to transition to more complex heterogeneous environments, testing such numerical codes continues to be a challenge. While testing numerical code on CPU systems has been a standard practice for years, traditional methods and tools do not automatically apply to heterogeneous systems—compiler optimizations, high degree of asynchrony in GPUs, emerging performance portability models, and reduced floating-point precision are just some of the challenges that testing software faces. In this minisymposium, we bring together experts from different fields to present some emerging projects on testing numerical code as well as lessons learned in existing scientific software.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72832-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS45 Understanding and Exploiting Mixed-Precision Accelerators for High-Performance Computing - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS45</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 12:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs>789947,765346</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>794954,741562,795125,785470</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72752</EventHandoutURL>
    <EventDescription>The growth of domain-specific hardware devices, such as low- and mixed-precision Matrix-Multiply Accumulate (MMA) accelerators (for example Tensor Processing Units and Tensor Cores), motivates several strands of research in scientific computing. First, algorithm designers aim to benefit from the speedup these hardware devices make possible by adapting algorithms, or parts of them, to run in low or mixed precisions. Second, we need to understand the low level details of how the devices implement floating-point arithmetic and to what extent they satisfy floating-point arithmetic standards. Third, new rounding error analysis is being developed to further support the task of finding the best ways to use the accelerators in order to maximize the accuracy of the results.  This minisymposium gathers researchers in scientific computing, numerical analysis, and the standardization and testing of floating-point arithmetics to report the latest research on applying and understanding the MMA hardware.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72752-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS46 Parallel Applications Using AMReX Particles</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS46</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 12:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>3000</EventKeywords>
    <EventChairs>802649,713700,803399</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>785662,802650,796070,785746</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72757</EventHandoutURL>
    <EventDescription>Particles and particle containers in the AMReX software framework are flexible constructs for representing any of a number of physical quantities and processes, including dark matter, biological cells, charged particles, and many more. This minisymposium will begin with an overview of the particle functionality in AMReX, including how the particle-related classes are implemented to enable performance portability. The remaining talks will highlight different ways that AMReX particles have been used to build scientific application software in a variety of subject areas.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72757-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS47 High-Performance Computing in Optimization - Part II of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS47</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 12:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1200</EventKeywords>
    <EventChairs>770730,728235,775268</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>802986,802987,783325</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73020</EventHandoutURL>
    <EventDescription>Large-scale optimization models are becoming increasingly important in operational research and applications such as energy and engineering systems, industrial engineering, advanced manufacturing, and others. One of the prime application domains is optimal power flow analysis for electric power grids. The computational challenges in this analysis stem from solving multiperiod optimization problems, further exacerbated by considering stochastic scenarios and security analysis. At the national grid level, this leads to massive computational problems. Novel computational methods that can efficiently utilize high performance computing (HPC) are required in order to address the computational complexity of these models. Recent developments in decomposition algorithms, parallel processing, and implementation of software frameworks leveraging linear algebra kernels suitable for deployment at heterogeneous HPC infrastructures make the solution of these complex systems possible. This minisymposium on HPC optimization will host a series of presentations on parallel solution methods and software implementations aiming to foster interdisciplinary collaborations between the optimization, parallel processing, and application domain communities.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73020-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS48 Experiences in Developing GPU Support for DOE Math Libraries - Part I of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS48</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 12:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs>735643,763182</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>763182,786064,88922,702311</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72873</EventHandoutURL>
    <EventDescription>The FASTMath (Frameworks, Algorithms and Scalable Technologies for Mathematics) Institute is a R&amp;D project funded by the SciDAC Program at the U.S. Department of Energy (DOE). The goal of FASTMath is to develop and deploy scalable mathematical algorithms and software tools for reliable simulation of complex physical phenomena and collaborating with DOE domain scientists to ensure the usefulness and applicability of the work in the project. The focus of FASTMath is strongly driven by the requirements of DOE application scientists who require fast, accurate, and robust forward simulation along with the ability to efficiently perform ensembles of simulations in optimization or uncertainty quantification studies. This minisymposium will present work by FASTMath participants focused on developing GPU support for their numerical mathematics libraries. Presentations will include discussion of different strategies as well as performance results.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72873-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS49 Partitioning and Process Mapping for Emerging Architectures - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS49</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 12:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs>781788,797540,803115</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>797540,797676,803101,795541</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73093</EventHandoutURL>
    <EventDescription>In the context of parallel computing, the main use of graph partitioning has been as a key pre-processing step for load balancing. Traditional massively parallel tools for graph partitioning were designed for scientific computing applications on meshes. In the past few years, new tools have emerged to address important changes in the computing landscape as well as in computer architecture for distributed computations: input data often has a more complex structure than meshes and current as well as forthcoming architectures are hierarchical in nature. Heterogeneous systems with (multiple) CPUs and (multiple) GPUs have become more and more popular as well. As a consequence, thread concurrency on large systems of this sort can reach into the millions and the architecture hierarchy emphasizes the importance of mapping, an extended partitioning problem. Regarding the input, many recent applications also add another layer of complexity, since highly irregular and/or dynamic/streaming data sets require more complicated (re)partitioning/mapping methods than previously.

In summary, the aforementioned challenges require new algorithms and implementations for solving load balancing problems efficiently. They need to compute partitions/mappings on hierarchical and/or heterogeneous hardware for graphs from complex applications executed on that hardware. This MS contains presentations that discuss the design of modern partitioning/mapping algorithms and software for these purposes.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73093-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS5 Research Challenges and Opportunities within Software Productivity, Sustainability, and Reproducibility - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS5</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs>34694,27158,705457</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>720350,803298,803128,803127</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73126</EventHandoutURL>
    <EventDescription>From predicting the folding of proteins to climate models that integrate observations, new types of scientific workflows are combining high-performance computing (HPC) simulation, data analysis and machine learning.  The integration of these different modes of computing is creating rich opportunities for the development of innovative scientific methods and offering new research challenges in software productivity, sustainability and reproducibility, targeting diverse hardware such as exascale machines and cloud computing environments.  
 
Productivity can be advanced by enabling rapid development of new methods and applications that achieve high performance while remaining portable and easy to change and support, through high-level languages, well designed libraries and frameworks, and advanced development environments.  Reproducibility provides the confidence of obtaining the same or similar results when running workflows in different or changing hardware and software environments, or by effectively managing the stochastic elements of complex workflows.  Overall, we want both productivity and reproducibility while achieving high performance on scalable systems with accelerators and multi-level memories.  
 
This minisymposium will highlight progress toward these goals with a diverse group of speakers, who are working on relevant aspects of the overall problem of combining productivity, reproducibility and high performance for various scientific computing problems.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73126-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS50 Flexible, Performance Portable Software for Partial Differential Equations - Part I of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS50</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 12:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs>796711,740185</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>785392,782760,803296,803403</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73120</EventHandoutURL>
    <EventDescription>Nowadays, advances in scientific computing strive to `free two birds with one key'. Software development is motivated to fulfill two reciprocal aspirations: flexibility and high performance. By increasing the flexibility of software, advances in performance are ported more easily between applications. But where flexibility is attained through separation of concerns, optimisation opportunities across separated layers may be hampered.

Flexibility shows up in many facets in computational science: The efficacy of numerical algorithms is improved for a wider range of physical scenarios. Highly optimised software components are layered in abstractions to enable reuse by other scientists. Multiple computer architectures are supported from within one software. All the while, simulations are sped up and and their accuracy is enhanced.

In this minisymposium we discuss how flexibility and high performance are achieved in various software packages for PDEs and how applications profit from it. We bring together practitioners to explore and disseminate recent progress.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73120-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS51 Advances in Performance Modeling of Parallel Code - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS51</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 12:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>804</EventKeywords>
    <EventChairs>735540</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>735540,803585,803586,803893</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73130</EventHandoutURL>
    <EventDescription>Performance modeling is an indispensable tool for the assessment, analysis, prediction, and optimization of parallel code in scientific computing and computational science. Modeling approaches can take a variety of forms, from purely analytic, first-principle models to curve fitting, machine learning, and AI-based solutions. The goals of modeling are just as diverse: Identification of bottlenecks or scaling problems, extrapolation, architectural exploration, and even the prediction of power dissipation and energy consumption can all be supported be modeling procedures. This minisymposium tries to provide an overview of the current state of the art in performance, or more generally, resource modeling of parallel code. The hardware focus will be very broad, from the node to the massively parallel level, including standard multicore systems, GPUs, and reconfigurable hardware. Contributions will cover fundamental research as well as tools development and case studies. After the minisymposium, the organizers plan to issue an open call for a journal special issue.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73130-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS52 Status of ExCALIBUR Project NEPTUNE</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS52</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 12:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>3000</EventKeywords>
    <EventChairs>790026</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>790026,803454,741955,803590</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73205</EventHandoutURL>
    <EventDescription>ExCALIBUR is an acronym for Exascale Computing ALgorithms &amp; Infrastructures Benefiting UK Research, which describes the overall programme. The specific aim of project NEPTUNE (NEutrals &amp; Plasma TUrbulence Numerics for the Exascale) is to produce new software to  model the edge region of plasmas magnetically confined in tokamak devices, currently viewed as a most promising approach to sustainable power production from nuclear fusion. 
Existing uncertainties in turbulent plasma behaviour imply software which must not only be maintainable, but also capable of significant modification over the 20-30 year timescale expected for fusion reactor development. Additional to improving physics understanding, NEPTUNE code is intended for use in device design ('actionable') and so must not only help  predict quantities such as detailed power deposition on the first wall, but give an estimate of the errors in such predictions ('UQ').  ExCALIBUR project NEPTUNE has begun with an initial phase involving eight different UK research institutions, to identify plasma models, algorithms and software engineering techniques most suited to project goals. Results of this activity particularly in respect of solvers involving high order finite elements will be reported in the minisymposium.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73205-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS53 Scalable Data Analytics on the GPU - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS53</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 12:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>500</EventKeywords>
    <EventChairs>712441,780304</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>806186,747042,802670,802671,802672</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72776</EventHandoutURL>
    <EventDescription>Data analytics are an integral part of data science. The introduction of new scalable and high-performance data analytics frameworks, including the use of accelerators such as GPUs,  has enabled data scientists to inspect their data and extract insights previously unattainable. In the last decade, the GPU has trailblazed this new exciting field of high-performance data analytics. This session will cover recent developments that have helped scale data analytics on the GPU to new heights. We will focus on analytics in social network analysis, bioinformatics, and cybersecurity, to name a few. We will also focus on other foundational components of scalable data analytics such as communication, data structures, algorithms, and ETL.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72776-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS54 Co-Design of Data Flow Accelerators for Scientific Simulations and Machine Learning - Part I of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS54</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>801</EventKeywords>
    <EventChairs>790203</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>780569,803741,796150,790203</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73325</EventHandoutURL>
    <EventDescription>This minisymposium is focused on emerging data flow accelerators. There are several such accelerators that are being developed in academia and industry. Some examples of such accelerators from industry are Cerebras, Graphcore, NextSilicon, and SambaNova. Example accelerators from academia include Maeri, SIGMA and Eyeriss. The focus of these accelerators vary from a specific linear algebra kernel (GEMM), machine learning problems, or scientific computing problems. Irrespective of their differences, all the proposed accelerators are data flow / spatial accelerators.  This minisymposium will bring together researchers from academia, industry, and national laboratories to share the state-of-the-art in co-designing such accelerators.


</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73325-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS55 High-Performance Computational Fluid Dynamics - Part I of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS55</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs>789629,803764</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>723558,762655,774866,803764</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73337</EventHandoutURL>
    <EventDescription>This minisymposium concerns the design, implementation, and application of high-performance software in computational fluid dynamics.  
The development of scalable, extensible, and portable simulation code is crucial in many areas of computational fluid dynamics, e.g., aerodynamic design, combustion, climate modelling, or astrophysics.
These problems often include complex geometries, the interaction of multiple scales, or multi-physics modelling, and require careful software design and appropriate algorithms to efficiently exploit modern computing architectures.
We aim to gather experts in the field of numerical methods for flow simulations and scientific computing to discuss software design, parallel processing, and application-specific challenges.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73337-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS56 Parallel-in-Time Integration Techniques - Part I of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS56</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs>795239,780337</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>797393,803753,803755,803756</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73331</EventHandoutURL>
    <EventDescription>The need for new algorithmic concepts is particularly urgent in fields that require simulating the evolution of a system starting from a given initial condition. Since the parallelization potential of an algorithm is critical for its performance on modern HPC architectures, parallelization must be taken into account from the outset. Recent successes have established the potential of parallel-in-time integration as a powerful algorithmic paradigm to unlock the performance of Exascale systems. This minisymposium features talks on both theoretical and computational aspects of parallel-in-time integration. Research topics ranging from algorithm development and mathematical analysis to software implementation are discussed.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73331-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS57 Parallel Algorithms for Tensor Computations and their Applications - Part I of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS57</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5100</EventKeywords>
    <EventChairs>708009,735519,796902,790183</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>797574,790176,797608,804312</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73352</EventHandoutURL>
    <EventDescription>Tensors, or multidimensional arrays, are a natural way to represent high-dimensional data arising in a multitude of applications. Tensor decompositions, such as the CANDECOMP/PARAFAC, Tucker, and Tensor Train models, help to identify latent structure, achieve data compression, and enable other tools of scientific and data analysis.  This minisymposium explores recent advances in algorithms for computing tensor decompositions, parallel algorithms for computing key tensor decomposition kernels, and applications of these methods to scientific and data analysis use-cases.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73352-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS58 Advances in Performance Modeling of Parallel Code - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS58</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>804</EventKeywords>
    <EventChairs>735540,803615</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803589,803587,803591</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73131</EventHandoutURL>
    <EventDescription>Performance modeling is an indispensable tool for the assessment, analysis, prediction, and optimization of parallel code in scientific computing and computational science. Modeling approaches can take a variety of forms, from purely analytic, first-principle models to curve fitting, machine learning, and AI-based solutions. The goals of modeling are just as diverse: Identification of bottlenecks or scaling problems, extrapolation, architectural exploration, and even the prediction of power dissipation and energy consumption can all be supported be modeling procedures. This minisymposium tries to provide an overview of the current state of the art in performance, or more generally, resource modeling of parallel code. The hardware focus will be very broad, from the node to the massively parallel level, including standard multicore systems, GPUs, and reconfigurable hardware. Contributions will cover fundamental research as well as tools development and case studies. After the minisymposium, the organizers plan to issue an open call for a journal special issue.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73131-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS59 Flexible, Performance Portable Software for Partial Differential Equations - Part II of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS59</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs>796711,740185</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>724709,796711,785013,790036</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73121</EventHandoutURL>
    <EventDescription>Nowadays, advances in scientific computing strive to `free two birds with one key'. Software development is motivated to fulfill two reciprocal aspirations: flexibility and high performance. By increasing the flexibility of software, advances in performance are ported more easily between applications. But where flexibility is attained through separation of concerns, optimisation opportunities across separated layers may be hampered.

Flexibility shows up in many facets in computational science: The efficacy of numerical algorithms is improved for a wider range of physical scenarios. Highly optimised software components are layered in abstractions to enable reuse by other scientists. Multiple computer architectures are supported from within one software. All the while, simulations are sped up and and their accuracy is enhanced.

In this minisymposium we discuss how flexibility and high performance are achieved in various software packages for PDEs and how applications profit from it. We bring together practitioners to explore and disseminate recent progress.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73121-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS6 Graph Algorithms in the Exascale Era - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS6</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs>795247,739547</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>795247,782736,785023,796163</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73137</EventHandoutURL>
    <EventDescription>Graph Analytics in general and graph algorithms in particular play a
central role in scientific applications. The unique traits of their runtime behavior with irregular memory access patters and fine grained synchronization make them particularly challenging to effectively implement and scale on parallel machines. Several parallel algorithms have been designed in recent years to address these challenges for the upcoming generation of exascale systems. In this two parts minisymposium, we will have two set of talks. One will present recent progress on parallel algorithms and implementations, while the other will describe novel approaches and applications that offer new opportunities for parallelization.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73137-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS60 Partitioning and Process Mapping for Emerging Architectures - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS60</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs>781788,797540,803115</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>781788,763345,803104,767248</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73094</EventHandoutURL>
    <EventDescription>In the context of parallel computing, the main use of graph partitioning has been as a key pre-processing step for load balancing. Traditional massively parallel tools for graph partitioning were designed for scientific computing applications on meshes. In the past few years, new tools have emerged to address important changes in the computing landscape as well as in computer architecture for distributed computations: input data often has a more complex structure than meshes and current as well as forthcoming architectures are hierarchical in nature. Heterogeneous systems with (multiple) CPUs and (multiple) GPUs have become more and more popular as well. As a consequence, thread concurrency on large systems of this sort can reach into the millions and the architecture hierarchy emphasizes the importance of mapping, an extended partitioning problem. Regarding the input, many recent applications also add another layer of complexity, since highly irregular and/or dynamic/streaming data sets require more complicated (re)partitioning/mapping methods than previously.

In summary, the aforementioned challenges require new algorithms and implementations for solving load balancing problems efficiently. They need to compute partitions/mappings on hierarchical and/or heterogeneous hardware for graphs from complex applications executed on that hardware. This MS contains presentations that discuss the design of modern partitioning/mapping algorithms and software for these purposes.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73094-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS61 Experiences in Developing GPU Support for DOE Math Libraries - Part II of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS61</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs>735643,763182</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>758314,803645,701749,732925</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72874</EventHandoutURL>
    <EventDescription>The FASTMath (Frameworks, Algorithms and Scalable Technologies for Mathematics) Institute is a R&amp;D project funded by the SciDAC Program at the U.S. Department of Energy (DOE). The goal of FASTMath is to develop and deploy scalable mathematical algorithms and software tools for reliable simulation of complex physical phenomena and collaborating with DOE domain scientists to ensure the usefulness and applicability of the work in the project. The focus of FASTMath is strongly driven by the requirements of DOE application scientists who require fast, accurate, and robust forward simulation along with the ability to efficiently perform ensembles of simulations in optimization or uncertainty quantification studies. This minisymposium will present work by FASTMath participants focused on developing GPU support for their numerical mathematics libraries. Presentations will include discussion of different strategies as well as performance results.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72874-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS62 High-Performance Computing in Optimization - Part III of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS62</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>1200</EventKeywords>
    <EventChairs>770730,728235,775268</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>802988,802989,763015</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73021</EventHandoutURL>
    <EventDescription>Large-scale optimization models are becoming increasingly important in operational research and applications such as energy and engineering systems, industrial engineering, advanced manufacturing, and others. One of the prime application domains is optimal power flow analysis for electric power grids. The computational challenges in this analysis stem from solving multiperiod optimization problems, further exacerbated by considering stochastic scenarios and security analysis. At the national grid level, this leads to massive computational problems. Novel computational methods that can efficiently utilize high performance computing (HPC) are required in order to address the computational complexity of these models. Recent developments in decomposition algorithms, parallel processing, and implementation of software frameworks leveraging linear algebra kernels suitable for deployment at heterogeneous HPC infrastructures make the solution of these complex systems possible. This minisymposium on HPC optimization will host a series of presentations on parallel solution methods and software implementations aiming to foster interdisciplinary collaborations between the optimization, parallel processing, and application domain communities.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73021-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS63 A Roadmap to Robust Science for High-Throughput Applications: The Developers' Perspective</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS63</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs>784892,774696</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>784892,802730,802731,802732</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72771</EventHandoutURL>
    <EventDescription>Scientists using the high-throughput computing (HTC) paradigm for scientific discovery rely on complex software systems and heterogeneous architectures that must deliver robust science (i.e., ensuring performance scalability in space and time; trust in technology, people, and infrastructures; and reproducible or confirmable research). Developers must overcome a variety of obstacles to pursue workflow interoperability, identify tools and libraries for robust science, port codes across different architectures, and establish trust in non-deterministic results. This minisymposium presents recommendations to build a roadmap to overcome these challenges and enable robust science for HTC applications and workflows.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72771-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS64 Parallel Processing in Data Science Applications - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS64</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  3:35:00 PM</EventStartTime>
    <EventEndTime>02/25/22  5:15:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs>732634,712754</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>726292,790678,783678,36345</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73377</EventHandoutURL>
    <EventDescription>Some of the most challenging problems in data-driven science involve understanding the interactions between millions or even thousands of millions of variables. In the era of Big Data, we are faced with the ever-increasing size, variability, and uncertainty of the datasets which are challenging high-performance numerical methods and software for extreme-scale computing. Among these data-intensive applications are problems arising from finance, biomedical applications, social networks, image classification or climate data. In recent years, deep neural networks have become invaluable for machine learning based classification algorithms. Besides, parallel processing algorithms such as hypergraph partitioning methods or algorithms exploiting the structure of the associated graph Laplacian as well as the Bayesian approach are further computational key methods for big data analytics. The vast quantity and variety of big data applications leads to bottlenecks in storage and computational costs while at the same time green computing becomes indispensable to reduce the carbon footprint even on distributed platforms such as clusters, grids, and clouds.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73377-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T15:35:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS65 Parallel Processing in Data Science Applications - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS65</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs>732634,712754</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>719917,803913,804303,804304</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73378</EventHandoutURL>
    <EventDescription>Some of the most challenging problems in data-driven science involve understanding the interactions between millions or even thousands of millions of variables. In the era of Big Data, we are faced with the ever-increasing size, variability, and uncertainty of the datasets which are challenging high-performance numerical methods and software for extreme-scale computing. Among these data-intensive applications are problems arising from finance, biomedical applications, social networks, image classification or climate data. In recent years, deep neural networks have become invaluable for machine learning based classification algorithms. Besides, parallel processing algorithms such as hypergraph partitioning methods or algorithms exploiting the structure of the associated graph Laplacian as well as the Bayesian approach are further computational key methods for big data analytics. The vast quantity and variety of big data applications leads to bottlenecks in storage and computational costs while at the same time green computing becomes indispensable to reduce the carbon footprint even on distributed platforms such as clusters, grids, and clouds.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73378-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS66 Neuromorphic Architectures: Efficient and Parallel Post-Moore Scientific Computing Potential</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS66</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs>782527</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>782527,802770,802771,802772</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72861</EventHandoutURL>
    <EventDescription>Neural-inspired computer architectures have enjoyed a new renaissance as traditional von Neumann architectures face the end of Moore’s Law and Dennard scaling. Most of neuromorphic computing research has focused on emulating the brain’s architecture and focused on use within artificial intelligence. However, the efficiency in energy and parallelism gained from these biologically inspired architectures has potential to induce a paradigm shift in scientific computing on the heterogenous HPC of the future. 
An emerging trend in neuromorphic computing research has focused on leveraging the energy and scaling benefits for novel applications, both large-scale and problems where size, weight, and power constrained computation limits traditional von Neumann approaches. Despite these wins, wide-spread adoption of these systems has been limited due to, among other reasons, the prevailing mindset that neuromorphic is only for machine learning and artificial intelligence applications. 
This minisymposium will focus on the cutting-edge research utilizing neuromorphic hardware and architectures and neural-inspired dynamics for scientific computing applications. From particle physics, to graphs, to sampling, a variety of applications will be showcased with arguments for, and against, use in wider scientific computing in terms of efficiency and accuracy.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72861-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS67 Task-Based Programming for Distributed Memory Systems - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS67</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs>747120,747244,739678,727320</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803041,750626,747244,803133</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73056</EventHandoutURL>
    <EventDescription>As a consequence of processing units specialization, modern supercomputers are becoming more and more heterogeneous and difficult to program. For this reason, the high performance computing community is increasingly embracing task-based parallelism and runtimes as an alternative to traditional parallel programming paradigms; these provide the HPC practitioner with a unified programming model and interface which ensure high performance portability while relieving him from the burden of dealing with low level architectural details. In this minisymposium we will present the latest advances in the field of task-based parallel programming and runtimes with a special focus on large scale, distributed memory parallelism. The minisymposium is divided into two parts. The first will present recent research on programming models and some of most well known and widely used runtimes. The second will deal with the use of task based parallelism and runtimes for the development of scalable algorithms and applications.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73056-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS68 Experiences in Developing GPU Support for DOE Math Libraries - Part III of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS68</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs>735643,763182</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>780194,719805,793097,704203</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72875</EventHandoutURL>
    <EventDescription>The FASTMath (Frameworks, Algorithms and Scalable Technologies for Mathematics) Institute is a R&amp;D project funded by the SciDAC Program at the U.S. Department of Energy (DOE). The goal of FASTMath is to develop and deploy scalable mathematical algorithms and software tools for reliable simulation of complex physical phenomena and collaborating with DOE domain scientists to ensure the usefulness and applicability of the work in the project. The focus of FASTMath is strongly driven by the requirements of DOE application scientists who require fast, accurate, and robust forward simulation along with the ability to efficiently perform ensembles of simulations in optimization or uncertainty quantification studies. This minisymposium will present work by FASTMath participants focused on developing GPU support for their numerical mathematics libraries. Presentations will include discussion of different strategies as well as performance results.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72875-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS69 Flexible, Performance Portable Software for Partial Differential Equations - Part III of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS69</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs>796711,740185</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>747124,781263,797025,755456</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73122</EventHandoutURL>
    <EventDescription>Nowadays, advances in scientific computing strive to `free two birds with one key'. Software development is motivated to fulfill two reciprocal aspirations: flexibility and high performance. By increasing the flexibility of software, advances in performance are ported more easily between applications. But where flexibility is attained through separation of concerns, optimisation opportunities across separated layers may be hampered.

Flexibility shows up in many facets in computational science: The efficacy of numerical algorithms is improved for a wider range of physical scenarios. Highly optimised software components are layered in abstractions to enable reuse by other scientists. Multiple computer architectures are supported from within one software. All the while, simulations are sped up and and their accuracy is enhanced.

In this minisymposium we discuss how flexibility and high performance are achieved in various software packages for PDEs and how applications profit from it. We bring together practitioners to explore and disseminate recent progress.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73122-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS7 Performance-Driven Dynamic Resource Management: New Analytics and Architectural Designs - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS7</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs>735577,758179,802658</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>758179,802901,768277,802902</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=72962</EventHandoutURL>
    <EventDescription>Urgency in HPC for Dynamic Resource Management is being driven by increasing variability in resource demands as a result of complex workflows, live instrument analytics, and new application models incorporating AI. Heterogeneous platforms may better support variable demands, but the ability to continuously assess applications’ demands and system conditions and to use those to dynamically match applications to resources simply does not exist. Progress has been made in developing application resource utilization profiles and understanding impacts of system conditions, however, little attention has been paid to the mathematics necessary to determine appropriate responses nor to the architectures necessary to enable them. Acceptance of automated reconfiguration will require profiling, contention and root cause analyses to provide confidence measures. Determination of effective responses relies on the assessment of headrooms of components given their capabilities and determination of complimentary sharing of resources without contention. Analytics that capture evolution of, overcorrection in, and instabilities of response actions are needed to evaluate the effectiveness of response. Concurrent development of hardware, application, and system software options for response can drive analytics and provide data for evaluation. We bring together performance analytics and architecture practitioners to discuss codesign of capabilities to enable Dynamic Resource Management.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>72962-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS70 Targeting Future Exascale and Extreme Heterogeneity Era - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS70</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs>771959,803366</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803352,803353,780364,780223</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73178</EventHandoutURL>
    <EventDescription>While computing technologies have remained relatively stable for nearly two decades, new architectural features, such as heterogeneous cores, deep memory hierarchies, and near-memory processing, have emerged as possible solutions to address the concerns of energy-efficiency, manufacturability, and cost. However, we expect this ‘golden age’ of architectural change to lead to extreme heterogeneity and it will have a major impact on software systems and applications. In this upcoming exascale and extreme heterogeneity era, it will be critical to explore new software approaches that will enable us to effectively exploit this diverse hardware to advance science, the next-generation systems with heterogeneous elements will need to accommodate complex workflows. This is mainly due to the many forms of heterogeneous accelerators (no longer just GPU accelerators) in this heterogeneous era, and the need of mapping different parts of an application onto elements most appropriate for that application component. This minisymposium will address strategies to address these important challenges from the upcoming extreme heterogeneity era.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73178-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS71 Parallel Algorithms for Tensor Computations and their Applications - Part II of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS71</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5100</EventKeywords>
    <EventChairs>708009,735519,796902,790183</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>790183,784918,803834,735519</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73353</EventHandoutURL>
    <EventDescription>Tensors, or multidimensional arrays, are a natural way to represent high-dimensional data arising in a multitude of applications. Tensor decompositions, such as the CANDECOMP/PARAFAC, Tucker, and Tensor Train models, help to identify latent structure, achieve data compression, and enable other tools of scientific and data analysis.  This minisymposium explores recent advances in algorithms for computing tensor decompositions, parallel algorithms for computing key tensor decomposition kernels, and applications of these methods to scientific and data analysis use-cases.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73353-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS72 Recent Advances in Numerical Linear Algebra Programming and Libraries</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS72</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>802</EventKeywords>
    <EventChairs>752767,803811</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>760197,803811,755875,789995</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73372</EventHandoutURL>
    <EventDescription>In recent years, we have seen an explosion in the complexity of maintaining software libraries so as provide reliable results and superior performance on a wide range of ever evolving platforms. This complexity comes from recent hardward trends where heterogeneous architectures with accelerators offload, complex memory management, massive parallelism are becoming the norm. This complexity also comes from users' interest in using new types beyond the traditional 32 and 64-bit floating-point data types. Thankfully, programming languages and compilers are able to help us handling the situation in noticeable ways through some levels of abstraction and templating. We can observe that, in recent years, some Numerical Linear Algebra libraries have proposed a design that heavily relies on abstraction and templating. The goal of this minisymposium is to create a forum for developers of numerical libraries, in particular Numerical Linear Algebra libraries, where state of the art, best engineering practice, current performance results, various issues related to software development, and software update will be presented.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73372-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS73 Code Generation and Transformation in HPC on Heterogeneous Platforms - Part  I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS73</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs>752522,762908</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803867,803868,798614,803869</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73373</EventHandoutURL>
    <EventDescription>Developing large-scale scientific applications to effectively utilize modern heterogeneous hardware is a major challenge in the HPC community.  Moreover, maintaining performance as platforms change becomes increasingly difficult.
This session focuses on tools and concepts to tackle these challenges. We consider code generation and source-to-source transformation as well as domain specific languages (DSLs), which aim to (i) transform between codes for different platforms, (ii) between codes in different programming languages, and (iii) from a DSL to compilable code.  
The presented techniques are applied in the areas of scientific and high-performance computing, which are known to be particularly challenging when it comes to performance portability. The session aims to discuss new developments as well as shed light on experiences with the adaptation of DSLs and tools in scientific communities.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73373-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS74 Parallel-in-Time Integration Techniques - Part II of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS74</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs>795239,780337</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>784885,803757,781852,795239</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73332</EventHandoutURL>
    <EventDescription>The need for new algorithmic concepts is particularly urgent in fields that require simulating the evolution of a system starting from a given initial condition. Since the parallelization potential of an algorithm is critical for its performance on modern HPC architectures, parallelization must be taken into account from the outset. Recent successes have established the potential of parallel-in-time integration as a powerful algorithmic paradigm to unlock the performance of Exascale systems. This minisymposium features talks on both theoretical and computational aspects of parallel-in-time integration. Research topics ranging from algorithm development and mathematical analysis to software implementation are discussed.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73332-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS75 High-Performance Computational Fluid Dynamics - Part II of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS75</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs>789629,803764</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>732534,781066,803759,737639</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73338</EventHandoutURL>
    <EventDescription>This minisymposium concerns the design, implementation, and application of high-performance software in computational fluid dynamics.  
The development of scalable, extensible, and portable simulation code is crucial in many areas of computational fluid dynamics, e.g., aerodynamic design, combustion, climate modelling, or astrophysics.
These problems often include complex geometries, the interaction of multiple scales, or multi-physics modelling, and require careful software design and appropriate algorithms to efficiently exploit modern computing architectures.
We aim to gather experts in the field of numerical methods for flow simulations and scientific computing to discuss software design, parallel processing, and application-specific challenges.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73338-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS76 Co-Design of Data Flow Accelerators for Scientific Simulations and Machine Learning - Part II of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS76</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>801</EventKeywords>
    <EventChairs>790203</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803882,803883,803884,804284</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73326</EventHandoutURL>
    <EventDescription>This minisymposium is focused on emerging data flow accelerators. There are several such accelerators that are being developed in academia and industry. Some examples of such accelerator from industry are Cerebras, Graphcore, NextSilicon, and SambaNova. Example accelerators from academic include Maeri, SIGMA and Eyeriss. The focus of these accelerators vary from specific a specific linear algebra kernel (GEMM), machine learning problems, or scientific computing problems. Irrespective of their differences, all the proposed accelerators are data flow / spatial accelerators.  This mini-symposium will bring together researchers from academia, industry, and national laboratories to share the state-of-the-art in co-designing such accelerators.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73326-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS77 Kinetic and Continuum Theories for Computational Plasma Physics - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS77</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22 10:40:00 AM</EventStartTime>
    <EventEndTime>02/26/22 12:20:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs>796612,790096,719838</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803629,730580,803631,735268</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73283</EventHandoutURL>
    <EventDescription>Computational plasma physics is a venerable area built on decades of work and many high-quality implementations. This minisymposium will bring together researchers focused on both kinetic and continuum theories. We hope to examine the boundary between these descriptions, look at scenarios which require closer interaction between them such as the solar corona and ultra-cold neutral plasmas, and implementations which can combine these theories.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73283-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T10:40:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS78 Parallel-in-Time Integration Techniques - Part III of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS78</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs>795239,780337</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803758,803760,769511,784887</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73333</EventHandoutURL>
    <EventDescription>The need for new algorithmic concepts is particularly urgent in fields that require simulating the evolution of a system starting from a given initial condition. Since the parallelization potential of an algorithm is critical for its performance on modern HPC architectures, parallelization must be taken into account from the outset. Recent successes have established the potential of parallel-in-time integration as a powerful algorithmic paradigm to unlock the performance of Exascale systems. This minisymposium features talks on both theoretical and computational aspects of parallel-in-time integration. Research topics ranging from algorithm development and mathematical analysis to software implementation are discussed.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73333-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS79 Code Generation and Transformation in HPC on Heterogeneous Platforms - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS79</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>800</EventKeywords>
    <EventChairs>752522,762908</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803870,803871,803872,752522</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73374</EventHandoutURL>
    <EventDescription>Developing large-scale scientific applications to effectively utilize modern heterogeneous hardware is a major challenge in the HPC community.  Moreover, maintaining performance as platforms change becomes increasingly difficult.
This session focuses on tools and concepts to tackle these challenges. We consider code generation and source-to-source transformation as well as domain specific languages (DSLs), which aim to (i) transform between codes for different platforms, (ii) between codes in different programming languages, and (iii) from a DSL to compilable code.  
The presented techniques are applied in the areas of scientific and high-performance computing, which are known to be particularly challenging when it comes to performance portability. The session aims to discuss new developments as well as shed light on experiences with the adaptation of DSLs and tools in scientific communities.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73374-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS8 Performance Modeling, Code Generation, and Machine Learning for Autotuning - Part I of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS8</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs>739657</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>773150,803747,780272,735129</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73328</EventHandoutURL>
    <EventDescription>Autotuning has been an established technique for performance engineering of applications with the assistance of performance models. This allows the implementers to overcome the issues of selecting hyperparameters to match the modern and emerging hardware with complex application codes.
This session will review optimization space search and ML techniques
employed in the exploration of performance space and the results of
improvements in runtime. We gather together practitioners of the
autotuning that target a variety of methods, software stacks, and
hardware platforms.

Some of the topics of interest include:

- performance search space exploration and exhaustive search issues

- surrogate models and approximations of performance metrics

- hyperparameter space representation techniques

- code generation methods for efficient HPC code

- strategies for exhaustive search overhead mitigation</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73328-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS80 Advances in Data-Parallel Deep Learning</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS80</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs>803842,789988</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803842,803843,803844,804273</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73367</EventHandoutURL>
    <EventDescription>Deep learning is increasingly popular for scientific computing applications and is now a common workload for supercomputers, using significant resources. The most common way to parallelize deep learning across multiple workers is data parallelism. This is a conceptually simple approach, but the details of the implementation greatly affect the scaling efficiency. There are several competing implementations of data-parallel deep learning that are currently used in production on distributed systems. These frameworks are also under active development, including with recent papers on improvements to the parallelization. Since this field is developing rapidly, there is much we do not know about the scaling properties of the latest releases of these frameworks and how their details differ. Our mini-symposium will begin with an overview of the topic, including a scaling study comparing three of these popular frameworks. Then we will have speakers from each of the three. We will learn about new improvements as well as areas for further research.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73367-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS81 Parallel Algorithms for Tensor Computations and their Applications - Part III of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS81</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>5100</EventKeywords>
    <EventChairs>708009,735519,796902,790183</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>789345,763099,798231,797419</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73354</EventHandoutURL>
    <EventDescription>Tensors, or multidimensional arrays, are a natural way to represent high-dimensional data arising in a multitude of applications. Tensor decompositions, such as the CANDECOMP/PARAFAC, Tucker, and Tensor Train models, help to identify latent structure, achieve data compression, and enable other tools of scientific and data analysis.  This minisymposium explores recent advances in algorithms for computing tensor decompositions, parallel algorithms for computing key tensor decomposition kernels, and applications of these methods to scientific and data analysis use-cases.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73354-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS82 Inference of Efficient Discrete Differential Operators</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS82</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 12:50:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>806</EventKeywords>
    <EventChairs>790027</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803527,734368,780404,804231</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73240</EventHandoutURL>
    <EventDescription>Partial differential equations (PDEs) are commonly used for the mathematical descriptions of various phenomena across all scientific disciplines. They are often solved numerically using finite element, finite difference or similar methods that dicretize the variables under consideration. In these discretized domains, differential operators are represented as matrices with nonzero weights for points in the neighborhood of the position under consideration – the so called stencil.
In order to employ such approaches, the underlying PDEs have to be known beforehand. Since this is not necessarily the case, especially for experimental data, researchers are working on the data-driven inference of either the underlying PDE or the stencil itself, which can be a computationally demanding step in case of large data sets. Once the stencil or PDE is determined, another challenge consists in the efficient, parallel solving of the underlying discrete system. 
Our minisymposium will focus on approaches for the data-driven inference of stencils or the underlying PDEs as well as their efficient realization using automatic stencil generation. Speakers will present deep-learning approaches as well as comprehensive regression techniques for the inference of stencil weights. In addition, robust methods for the identification of PDEs from noisy data will be presented and the automatic generation of optimized code will be discussed.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73240-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS84 High-Performance Computational Fluid Dynamics - Part III of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS84</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs>789629,803764</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803761,795483,803806,714749</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73339</EventHandoutURL>
    <EventDescription>This minisymposium concerns the design, implementation, and application of high-performance software in computational fluid dynamics.  
The development of scalable, extensible, and portable simulation code is crucial in many areas of computational fluid dynamics, e.g., aerodynamic design, combustion, climate modelling, or astrophysics.
These problems often include complex geometries, the interaction of multiple scales, or multi-physics modelling, and require careful software design and appropriate algorithms to efficiently exploit modern computing architectures.
We aim to gather experts in the field of numerical methods for flow simulations and scientific computing to discuss software design, parallel processing, and application-specific challenges.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73339-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS85 Targeting Future Exascale and Extreme Heterogeneity Era - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS85</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>700</EventKeywords>
    <EventChairs>771959,803366</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803354,747195,719886</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73179</EventHandoutURL>
    <EventDescription>While computing technologies have remained relatively stable for nearly two decades, new architectural features, such as heterogeneous cores, deep memory hierarchies, and near-memory processing, have emerged as possible solutions to address the concerns of energy-efficiency, manufacturability, and cost. However, we expect this ‘golden age’ of architectural change to lead to extreme heterogeneity and it will have a major impact on software systems and applications. In this upcoming exascale and extreme heterogeneity era, it will be critical to explore new software approaches that will enable us to effectively exploit this diverse hardware to advance science, the next-generation systems with heterogeneous elements will need to accommodate complex workflows. This is mainly due to the many forms of heterogeneous accelerators (no longer just GPU accelerators) in this heterogeneous era, and the need of mapping different parts of an application onto elements most appropriate for that application component. This minisymposium will address strategies to address these important challenges from the upcoming extreme heterogeneity era.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73179-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS86 Task-Based Programming for Distributed Memory Systems - Part II of II</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS86</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>807</EventKeywords>
    <EventChairs>747120,747244,739678,727320</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803042,735518,803122,803490</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73057</EventHandoutURL>
    <EventDescription>As a consequence of processing units specialization, modern supercomputers are becoming more and more heterogeneous and difficult to program. For this reason, the high performance computing community is increasingly embracing task-based parallelism and runtimes as an alternative to traditional parallel programming paradigms; these provide the HPC practitioner with a unified programming model and interface which ensure high performance portability while relieving him from the burden of dealing with low level architectural details. In this minisymposium we will present the latest advances in the field of task-based parallel programming and runtimes with a special focus on large scale, distributed memory parallelism. The minisymposium is divided into two parts. The first will present recent research on programming models and some of most well known and widely used runtimes. The second will deal with the use of task based parallelism and runtimes for the development of scalable algorithms and applications.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73057-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS88 Co-Design of Data Flow Accelerators for Scientific Simulations and Machine Learning - Part III of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS88</EventNumber>
    <EventDate>Feb 26, 2022</EventDate>
    <EventStartTime>02/26/22  1:50:00 PM</EventStartTime>
    <EventEndTime>02/26/22  3:30:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>801</EventKeywords>
    <EventChairs>790203</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>803877,803878,803742,804331</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73327</EventHandoutURL>
    <EventDescription>This minisymposium is focused on emerging data flow accelerators. There are several such accelerators that are being developed in academia and industry. Some examples of such accelerator from industry are Cerebras, Graphcore, NextSilicon, and SambaNova. Example accelerators from academic include Maeri, SIGMA and Eyeriss. The focus of these accelerators vary from specific a specific linear algebra kernel (GEMM), machine learning problems, or scientific computing problems. Irrespective of their differences, all the proposed accelerators are data flow / spatial accelerators.  This mini-symposium will bring together researchers from academia, industry, and national laboratories to share the state-of-the-art in co-designing such accelerators.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73327-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-26T13:50:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>MS9 Domain Decomposition Methods – Parallel Algorithms, Software, and Machine Learning - Part I of III</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>MS9</EventNumber>
    <EventDate>Feb 23, 2022</EventDate>
    <EventStartTime>02/23/22  1:00:00 PM</EventStartTime>
    <EventEndTime>02/23/22  2:40:00 PM</EventEndTime>
    <EventFilter>PP22|Minisymposium|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords>600</EventKeywords>
    <EventChairs>720131,803849,774576,763792,790005</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>8363,774576,768555,768192</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73346</EventHandoutURL>
    <EventDescription>Domain decomposition methods are parallel scalable, preconditioned iterative methods for the efficient solution of problems arising from the discretization of partial differential equations. In this minisymposium, we focus on several important aspects of domain decomposition methods: Coarse spaces, nonlinear preconditioning, high-performance computing, software development, and scientific machine learning.</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73346-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-23T13:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>PD1 Current and Future Trends Impacting Inclusivity in High-Performance Scientific Computing</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PD1</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  9:25:00 AM</EventStartTime>
    <EventEndTime>02/24/22 10:25:00 PM</EventEndTime>
    <EventFilter>PP22|Panel Discussion|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>721529</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73827</EventHandoutURL>
    <EventDescription>This panel will explore challenges, ongoing efforts, and 
pathways to a more inclusive computational science and 
high-performance computing workforce.  We will begin with 
a look at workforce data and how that sets the stage for 
advances in diversity, equity, and inclusion.  Panelists 
represent a broad spectrum of perspectives, career stages, 
and roles.  Topics include effects of the global pandemic, 
challenges and opportunities in recruitment and retention, 
and the impact of the growing interest in artificial 
intelligence opportunities in industry.


</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73827-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T09:25:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>PD2  AI for HPC and Combining AI and Numerical Approaches</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PD2</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  3:20:00 PM</EventStartTime>
    <EventEndTime>02/24/22  5:00:00 PM</EventEndTime>
    <EventFilter>PP22|Panel Discussion|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>796990</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73833</EventHandoutURL>
    <EventDescription>



</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73833-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T15:20:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>PD3 Build, Integration and Testing for Sustainable Scientific Computing Software</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PD3</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22 11:10:00 AM</EventStartTime>
    <EventEndTime>02/25/22 12:50:00 PM</EventEndTime>
    <EventFilter>PP22|Panel Discussion|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>796990</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73834</EventHandoutURL>
    <EventDescription>
</EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73834-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-25T11:10:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>Poster Session</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>PP</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  6:00:00 PM</EventStartTime>
    <EventEndTime>02/24/22  8:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>796990</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>790176,802691,780246,803845,774784,803909,803917,791858,803807,803727,790179,704793,790070,784882</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73765-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo xsi:nil="true" />
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled xsi:nil="true" />
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T18:00:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>SIAG/SC Business Meeting</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>CB</EventNumber>
    <EventDate>Feb 24, 2022</EventDate>
    <EventStartTime>02/24/22  5:15:00 PM</EventStartTime>
    <EventEndTime>02/24/22  6:00:00 PM</EventEndTime>
    <EventFilter>PP22|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs xsi:nil="true" />
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs xsi:nil="true" />
    <EventSponsor></EventSponsor>
    <EventHandoutName></EventHandoutName>
    <EventHandoutURL></EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73783-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord>1</AutoRecord>
    <EnabledForIAV>1</EnabledForIAV>
    <MeetingIAVMode>Meeting</MeetingIAVMode>
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity>300</Capacity>
    <DirectLinkEnabled>0</DirectLinkEnabled>
    <DirectLinkRecordingEnabled xsi:nil="true" />
    <start>2022-02-24T17:15:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>SP1 SIAG Best Paper Prize: Optimizing Communication-Avoiding Sparse LU Factorization on Multi-GPU Clusters</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>SP1</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  8:30:00 AM</EventStartTime>
    <EventEndTime>02/25/22  9:15:00 AM</EventEndTime>
    <EventFilter>PP22|Prize Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>723527</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>790070</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73820</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73820-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>1</DirectLinkEnabled>
    <DirectLinkRecordingEnabled>1</DirectLinkRecordingEnabled>
    <start>2022-02-25T08:30:00</start>
  </meeting>
  <meeting>
    <ConfCode>PP22</ConfCode>
    <EventName>SP2 SIAG/Supercomputing Career Prize: Scaling in Space and Time</EventName>
    <EventSubName></EventSubName>
    <Poster></Poster>
    <EventNumber>SP2</EventNumber>
    <EventDate>Feb 25, 2022</EventDate>
    <EventStartTime>02/25/22  9:25:00 AM</EventStartTime>
    <EventEndTime>02/25/22  9:55:00 AM</EventEndTime>
    <EventFilter>PP22|Prize Speaker|</EventFilter>
    <EventLocation></EventLocation>
    <EventRoom></EventRoom>
    <FloorPlan>Hyatt Regency Seattle</FloorPlan>
    <EVENT_FEE></EVENT_FEE>
    <EventKeywords></EventKeywords>
    <EventChairs>763168</EventChairs>
    <EventModerators></EventModerators>
    <EventSpeakers></EventSpeakers>
    <EventSpeakerIDs>708034</EventSpeakerIDs>
    <EventSponsor></EventSponsor>
    <EventHandoutName>session</EventHandoutName>
    <EventHandoutURL>http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=73832</EventHandoutURL>
    <EventDescription></EventDescription>
    <EventParentName></EventParentName>
    <EventUniqueID>73832-SESS</EventUniqueID>
    <STATUS>active</STATUS>
    <AutoRecord xsi:nil="true" />
    <EnabledForIAV xsi:nil="true" />
    <MeetingIAVMode xsi:nil="true" />
    <PublishAutoRecordedVideo>1</PublishAutoRecordedVideo>
    <Capacity xsi:nil="true" />
    <DirectLinkEnabled>1</DirectLinkEnabled>
    <DirectLinkRecordingEnabled>1</DirectLinkRecordingEnabled>
    <start>2022-02-25T09:25:00</start>
  </meeting>
</meetings>